{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-dataset\" data-toc-modified-id=\"Importing-the-dataset-1\">Importing the dataset</a></span></li><li><span><a href=\"#Cleaning-and-organizing-our-data\" data-toc-modified-id=\"Cleaning-and-organizing-our-data-2\">Cleaning and organizing our data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Product\" data-toc-modified-id=\"Product-2.1\">Product</a></span></li><li><span><a href=\"#Handling-Null-Values:\" data-toc-modified-id=\"Handling-Null-Values:-2.2\">Handling Null Values:</a></span></li></ul></li><li><span><a href=\"#Sentiments\" data-toc-modified-id=\"Sentiments-3\">Sentiments</a></span></li><li><span><a href=\"#CHECK\" data-toc-modified-id=\"CHECK-4\">CHECK</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Unique-Words\" data-toc-modified-id=\"Unique-Words-4.0.1\">Unique Words</a></span></li></ul></li><li><span><a href=\"#Hashtags\" data-toc-modified-id=\"Hashtags-4.1\">Hashtags</a></span></li><li><span><a href=\"#Text-Pre-Processing\" data-toc-modified-id=\"Text-Pre-Processing-4.2\">Text Pre-Processing</a></span></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-4.3\">Stemming</a></span></li><li><span><a href=\"#Tokenize\" data-toc-modified-id=\"Tokenize-4.4\">Tokenize</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-5\">Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF-Vector\" data-toc-modified-id=\"TF-IDF-Vector-5.1\">TF IDF Vector</a></span></li><li><span><a href=\"#Positive\" data-toc-modified-id=\"Positive-5.2\">Positive</a></span></li></ul></li><li><span><a href=\"#N-grams\" data-toc-modified-id=\"N-grams-6\">N-grams</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "\n",
    "# Import Sklearn libraries to build models \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #  TF-IDF to vectorize words \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Import Libraries to perform computation and do visualization. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "# Import nltk to check english lexicon.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk import pos_tag # for Parts of Speech tagging\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Generate wordcloud for word distribution visualization.\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Generating random numbers.\n",
    "import random \n",
    "\n",
    "\n",
    "# Transforms text to a fixed-length vector of integers.\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "#Efficient functions to search in strings.\n",
    "import re as re \n",
    "\n",
    "# Import images for world cloud.\n",
    "from PIL import Image, ImageDraw, ImageFont \n",
    "\n",
    "\n",
    "# Import Yellowbrick and vector coupon for visualization of frequent words\n",
    " \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "\n",
    "\n",
    "from os import path\n",
    "from os import environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'not' in stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'not' in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not happy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet_text</td>\n",
       "      <td>emotion_in_tweet_is_directed_at</td>\n",
       "      <td>is_there_an_emotion_directed_at_a_brand_or_pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0                                         tweet_text   \n",
       "1  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "2  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "3  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "4  @sxsw I hope this year's festival isn't as cra...   \n",
       "\n",
       "                           Product  \\\n",
       "0  emotion_in_tweet_is_directed_at   \n",
       "1                           iPhone   \n",
       "2               iPad or iPhone App   \n",
       "3                             iPad   \n",
       "4               iPad or iPhone App   \n",
       "\n",
       "                                           Sentiment  \n",
       "0  is_there_an_emotion_directed_at_a_brand_or_pro...  \n",
       "1                                   Negative emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Positive emotion  \n",
       "4                                   Negative emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS=['Tweet','Product','Sentiment']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TA = pd.read_csv('D:\\OneDrive - NITT\\Custom_Download\\TWEETS.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "TA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = TA['Tweet'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge not and next word\n",
    "x = tweet.split()\n",
    "n = x.index('not')\n",
    "x[n+1] = x[n]+'_'+x[n+1]\n",
    "x.pop(n)\n",
    "' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wait'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy\n",
    "happiest\n",
    "happiness\n",
    "\n",
    "# def merge_not(tweet):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# preprocess the tweets\n",
    "# print/plot , not and next words\n",
    "\n",
    "# apply countvectorizer\n",
    "# apply tfidf vectorizer\n",
    "# create the accuracy Table for comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and organizing our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first row by selecting all rows from first row onwards\n",
    "\n",
    "TA = TA.iloc[1: , :]\n",
    "TA.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking length \n",
    "print('length of data is', len(TA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understadning the type of data we are dealing with\n",
    "\n",
    "print(TA.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of columns in the data is:  ', len(TA.columns))\n",
    "print('Count of rows in the data is:  ', len(TA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for Null values.  We use the heatmap code which shows the contrast well.\n",
    "\n",
    "sns.heatmap(TA.isnull(), cbar=False)\n",
    "plt.title(\"NaNs\")\n",
    "plt.xlabel('Culomns')\n",
    "plt.ylabel('Row')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Nulls numerically \n",
    "print(TA.isna().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any dupllicate rows\n",
    "\n",
    "print(TA.duplicated().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate values\n",
    "TA.drop_duplicates(inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular project we will be focusing on the companies themselves and therefore we we consolidated the names. All if apple related products (e.g. iPad, iPhone ) will be under Apple and Google’s products will be consolidated into one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Product_Brand']= TA['Product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.Product_Brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TA['Product'].fillna('Undefined',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TA.isna().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [('#google'),('Google'), 'google', 'Anroid', \n",
    "     'anroid', 'ipad','ipad2','iphone','apple','Apple', 'ipad3','SXSW']\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in L)\n",
    "\n",
    "TA['Product'] = TA['Tweet'].str.extract('('+ pat + ')', \n",
    "                                        expand=False, \n",
    "                                        flags=re.I) # The function takes a an objecg from the Consolidated list we created, and returns the replacement string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TA.isna().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.Product_Brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " TA['Product_Brand'].replace({\"iPhone\":\"Apple\",\n",
    "                              'iPad':\"Apple\",\n",
    "                     \"iPad or iPhone App\":\"Apple\",\n",
    "                     \"iPhone\":\"Apple\", \n",
    "                      \"iPAD2\": \"Apple\" , \n",
    "                      \"iPad2\":\"Apple\",\n",
    "                      \"IPAD\":\"Apple\",\n",
    "                      \"Iphone\":\"Apple\",\n",
    "                      \"Ipad2\":\"Apple\",\n",
    "                      \"IPHONE\":\"Apple\",\n",
    "                      \"iPAD\":\"Apple\",\n",
    "                      \"iPHONE\":\"Apple\",\n",
    "                      \"IPad\":\"Apple\",\n",
    "                      \"APPLE\":\"Apple\",\n",
    "                      \"apple\":\"Apple\",                   \n",
    "                      \"IPad\":\"Apple\",\n",
    "                      \"ipad\":\"Apple\",\n",
    "                      \"iphone\":\"Apple\",\n",
    "                      \"ipad2\":\"Apple\",\n",
    "                      \"Ipad\":\"Apple\",\n",
    "                      \"IPhone\":\"Apple\",         \n",
    "                      \"Android\":\"Google\", \n",
    "                      \"Android App\":\"Google\", \n",
    "                      \"SXSW\":\"Google\",\n",
    "                      \"sxsw\": \"Google\",\n",
    "                      \"SxSw\": \"Google\",     \n",
    "                      \"Sxsw\": \"Google\",\n",
    "                      \"GOOGLE\": \"Google\",\n",
    "                      \"google\": \"Google\",\n",
    "                      \"sXsw\": \"Google\",      \n",
    "                      \"SxSW\": \"Google\",    \n",
    "                      \"Other Google product or service\":\"Google\",\n",
    "                      \"Other Apple product or service\":\"Google\" },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.Product_Brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(TA.isnull(), cbar=False)\n",
    "plt.title(\"NaNs\")\n",
    "plt.xlabel('Culomns')\n",
    "plt.ylabel('Row')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.dropna(subset = ['Product_Brand','Tweet','Sentiment'], inplace = True) #Droping null entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(TA.isnull(), cbar=False)\n",
    "plt.title(\"NaNs\")\n",
    "plt.xlabel('Culomns')\n",
    "plt.ylabel('Row')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TA.isna().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Handling Null Values: \n",
    "\n",
    "Sentiment: there are only 195 null values in the sentiment column therefore the most effective approach is to delete these.\n",
    "Tweets: there are only 27 null values and here as well the most effective approach is to remove these null values. \n",
    "On the other hand, Product shows 5,997 mulls - we will excitant any wards that appear on the tweet that mention the product. After we did that we only were left with 77 null values with we removed. Now we are dealing with 9,026 tweets with their respective product, brand and sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the possible values in the sentiment column\n",
    "TA['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will clean up the data by removing ‘emotion' then replacing ‘I can’t tell’ to \"Neutral\" and \"nan\" to \"Undefined\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Sentiment'].replace({\"Positive emotion\":\"Postive\",\n",
    "                         \"Negative emotion\":\"Negative\", \n",
    "                         \"No emotion toward brand or product\":\"Neutural\",\n",
    "                         \"I can't tell\":\"0\",\n",
    "                         \"nan\":\"4\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Sentiment'].fillna('Undefined',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA[\"Sentiment\"].hist(facecolor='g',alpha=0.5) # Examine the data and check if the data unbalanced \n",
    "\n",
    "#plt.bar(TA[\"Sentiment\"],color=['red','beige','lightpink','black'], height=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK \n",
    "Next, let's try tokenizing our dataset. In order to save ourselves some time, we'll write a function to clean our dataset, and then use Python's built-in map() function to clean every article in the dataset at the same time.\n",
    "\n",
    "In the cell below, complete the process_article() function. This function should:\n",
    "\n",
    "Take in one parameter, article\n",
    "Tokenize the article using the appropriate function from nltk\n",
    "Lowercase every token, remove any stopwords found in stopwords_list from the tokenized article, and return the results\n",
    "\n",
    "\n",
    "\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``']\n",
    "\n",
    "def process_article(article):\n",
    "    tokens = nltk.word_tokenize(article)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    return stopwords_removed    \n",
    "\n",
    "##### Unique Words\n",
    "\n",
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)\n",
    "\n",
    "\n",
    "articles_concat = []\n",
    "for article in processed_data:\n",
    "    articles_concat += article\n",
    "articles_freqdist = FreqDist(articles_concat)\n",
    "articles_freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtags\n",
    "We extracted the hashtags into a new column from the tweets so that we can analyze them closer later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtags(tweet):\n",
    "    #This function extracts hashtags from the tweets.\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "  \n",
    "TA['Hashtags'] = TA['Tweet'].apply(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre-Processing  \n",
    "\n",
    "We we start cleaning the tweets by removing unnecessary words. We extracted words from the main stop word list. I will not remove stop words that have negative or some sentiment related connotation since it can be useful for our analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set().union(stopwords.words('english'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited stop words\n",
    "stopwordlist_limited = ['rt','a', 'about', 'after', 'again',  'all', 'am', 'an', 'and',  'are',  'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',  'd', 'did', 'do', 'does', 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn','has', 'hasn', \"hasn't\", 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is',  'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn',  'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'no', 'nor',  'now', 'o', 'of',  'on', 'once', 'only', 'or', 'ours', 'ourselves', 'out',  'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', \"wasn't\", 'we', 'were',  'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'will', 'with',  'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove our Stopwords\n",
    "def remove_stopwordlist_limited(text): #Function to remove the stopwords we chose that we think won't impact the santiment \n",
    "    return [word for word in word_tokenize(text) if not word in stopwordlist_limited] #Returns tweet without stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column for the clean Tweets but we will keep the old column as well in case we need it later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying function to build new column\n",
    "TA['Clean_Tweets'] = TA['Tweet'].apply(lambda x: remove_stopwordlist_limited(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA[\"Clean_Tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Clean_Tweets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Clean_Tweets'] = TA['Clean_Tweets'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Clean_Tweets'] = TA['Clean_Tweets'].str.lower()\\\n",
    "          .str.replace('(@[a-z0-9]+)\\w+',' ')\\\n",
    "          .str.replace('(http\\S+)', ' ')\\\n",
    "          .str.replace('([^0-9a-z \\t])',' ')\\\n",
    "          .str.replace(' +',' ')\\\n",
    "          .str.replace('#',' ')\\\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Clean_Tweets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove any short words that are less than 2 letters as they often happen to be meaningless and will not help our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short words\n",
    "\n",
    "TA['Clean_Tweets'] = TA['Clean_Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA['Clean_Tweets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "We stem the words to make our text more standardize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tokenized_tweets = TA['Clean_Tweets'].apply(lambda x: stemming(x))\n",
    "TA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "We tokenize the tweets so that we can make a use of our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to tenize the tweets\n",
    "\n",
    "TA['Tokenize_Tweets'] = TA['Clean_Tweets'].apply(word_tokenize)\n",
    "TA['Tokenize_Tweets'] .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF IDF Vector\n",
    "\n",
    "TF-IDF is a way to measure statistically the relevant words in the document. It is effectively the product of frequency in which a word appears in a text and inverse document frequency (I.e. whether the word is rare or common in the text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_Concat = []\n",
    "for tweet in TA['Tokenize_Tweets']:\n",
    "   Tweets_Concat += tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_freqdist = FreqDist(Tweets_Concat)\n",
    "tweets_freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "docs       = vectorizer.fit_transform(Tweets_Concat)\n",
    "features   = vectorizer.get_feature_names()\n",
    "\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(docs)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 are The follwoing: \n",
    "    \n",
    "    1.sxsw\n",
    "    2.mention\n",
    "    3.link\n",
    "    4.ipad\n",
    "    4.apple\n",
    "    5.google\n",
    "    6.iphone\n",
    "    7.quot\n",
    "    8.store\n",
    "    9.app\n",
    "    10.new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUESTION - How can it be helpful for me to know this *** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n",
    "    return tuple(Reds_9.colors[random.randint(6,8)])  # Function to help us generate wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the most common words used in our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualize our data with a word cloud \n",
    "\n",
    "all_words = ' '.join([text for text in TA['Clean_Tweets']])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the most common words in our data for negative and positive tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####                                                                       Positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tweets into one long string with each word separate with a \"space\"\n",
    "TA['Clean_Tweets'] = TA['Clean_Tweets'].apply(str)\n",
    "TA['Sentiment'] = TA['Sentiment'].apply(str)\n",
    "tweets_long_string = TA['Clean_Tweets'].tolist()\n",
    "tweets_long_string = \" \".join(tweets_long_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing sentiments that are positive and combining them\n",
    "\n",
    "Po_Sentiments = TA[TA['Sentiment']=='Positive'] \n",
    "PT= \" \".join([sentence for sentence in TA['Clean_Tweets'][TA['Sentiment']=='Positive']])\n",
    "\n",
    "# Choosing sentiments that are negative and combining them\n",
    "\n",
    "Ne_Sentiments = TA[TA['Sentiment']=='Negative']\n",
    "NT = \" \".join([sentence for sentence in TA['Clean_Tweets'][TA['Sentiment']=='Negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * QUESTION WHY DOESNT WORK\n",
    "    \n",
    "\n",
    "font_path = \"/Users/nataliaedelson/Desktop/OpenSans-CondBold.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a function to generate word cloud\n",
    "def plot_wordcloud(wordcloud):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add thumbs up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image to np.array\n",
    "mask = np.array(Image.open('TD.png'))\n",
    "# Generate wordcloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='white', colormap='Reds', collocations=False, contour_color = 'white',contour_width=1, mask=mask).generate(NT)\n",
    "# Plot\n",
    "plot_wordcloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to see if I can get more color on the word distribution and check what the output will be if we extract adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a function to obtain adjectives from tweets\n",
    "def getAdjectives(tweet):\n",
    "    tweet = word_tokenize(tweet)  # convert string to tokens\n",
    "    tweet = [word for (word, tag) in pos_tag(tweet)\n",
    "             if tag == \"JJ\"]  # pos_tag module in NLTK librarybb\n",
    "    return \" \".join(tweet)  # join words with a space in between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert teh column in to strings\n",
    "TA['Clean_Tweets'] = TA['Clean_Tweets'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply getAdgectives to our tweets\n",
    "TA['Tweets_Adjectives'] = TA['Clean_Tweets'].apply(getAdjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.head() # Check dataframe first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tweets into one long string with each word separate with a \"space\"\n",
    "tweets_long_string = TA['Tweets_Adjectives'].tolist()\n",
    "tweets_long_string = \" \".join(tweets_long_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of words in our data\n",
    "#fdist = FreqDist(TA['Clean_Tweets'])\n",
    "#WordCloud\n",
    "wc = WordCloud(width=900, height=500, max_words=50).generate(tweets_long_string )\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = set()\n",
    "for comment in TA['Clean_Tweets']:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** DOUBLE CHECK # There are 7,921 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of data focusing on sentiments and sentiment’s brand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.countplot(data = TA, y = 'Sentiment', palette=\"Set3\") #Setting p to plot of Emotion\n",
    "p.set(xlabel = 'Count') #Labling X\n",
    "p.set(ylabel = 'Sentiment') #Labling Y\n",
    "p.set(title = \"Count of Tweets per Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four more times positive tweets than negatives. There are barely any neutral tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.countplot(data = TA, y = 'Product_Brand', palette=\"Set3\") #Setting p to plot of Emotion\n",
    "p.set(xlabel = 'Count') #Labling X\n",
    "p.set(ylabel = 'Product_Brand') #Labling Y\n",
    "p.set(title = \"Count of Tweets per Sentiment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apple has a little more than double tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(TA.groupby(['Sentiment'])['Product_Brand'].value_counts()) #Checking tweets rated by brand\n",
    "\n",
    "display(TA.groupby(['Product_Brand'])['Sentiment'].value_counts()) #Checking tweets rated by brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.countplot(data = TA, x = 'Product_Brand', hue = 'Sentiment') #Setting p to plot of Brand and Emotion\n",
    "plot.legend(title = 'Sentiment', bbox_to_anchor = (1, 1), loc = 'upper left') #Creating legend for plot\n",
    "plot.set(xlabel = 'Product_Brand') #Setting x label\n",
    "plot.set(ylabel = 'Count') #Setting y label\n",
    "plot.set(title = 'No. of Tweets per Brand by Sentiment'); #Setting title of plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.countplot(data = TA, x = 'Sentiment', hue = 'Product_Brand',color='silver') #Setting p to plot of Emotion and Brand\n",
    "plot.legend(title = 'Product_Brand', bbox_to_anchor = (1, 1), loc = 'upper left') #Creating legend for plot\n",
    "plot.set(xlabel = 'Sentiment') #Setting x label\n",
    "plot.set(ylabel = 'Count') #Setting y label\n",
    "plot.set(title = 'No. of Tweets per sentiment by Brand'); #Setting title of plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://www.sharpsightlabs.com/blog/seaborn-countplot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to dummy variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiments_dummies = pd.get_dummies(TA[\"Sentiment\"], prefix=\"Sentiment\")\n",
    "TA = pd.concat([TA, sentiments_dummies ], axis = 1)\n",
    "TA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = TA[\"Tokenize_Tweets\"]  \n",
    "y = TA[\"Sentiment\"]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer =TfidfVectorizer(ngram_range=(1,2), max_features =40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print(\"Number of words:\", len(vectorizer.getfeature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    text_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "    text_classifier.fit(X_train, y_train)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
