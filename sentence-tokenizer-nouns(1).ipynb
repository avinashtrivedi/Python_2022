{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web content text tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits titles from web articles into verbs, adjectives, adverbs, superlatives and named entities. Outputs the results to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename  = 'data/source/find-keywords.xlsx'\n",
    "output_filename = 'data/find-keywords-nouns.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the articles and display a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article count:  593\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Url</th>\n",
       "      <th>Title</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ParaDocs provides our clients with the highest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AGNITY Global Inc., (AGNITY) is a global provi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FlashCo has grown to be one of the largest man...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PAC Worldwide is a global leader in the manufa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>DynexÂ® Technologies, Inc. is an original pion...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Auris Surgical Robotics, Inc. is a technology ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sense Corp powers insight-driven organizations...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>We provide quality software and uncompromising...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Driven by the ever-changing needs of our clien...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Americo Manufacturing Company, headquartered i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Url                                              Title  Unnamed: 2  \\\n",
       "0  NaN  ParaDocs provides our clients with the highest...         NaN   \n",
       "1  NaN  AGNITY Global Inc., (AGNITY) is a global provi...         NaN   \n",
       "2  NaN  FlashCo has grown to be one of the largest man...         NaN   \n",
       "3  NaN  PAC Worldwide is a global leader in the manufa...         NaN   \n",
       "4  NaN  DynexÂ® Technologies, Inc. is an original pion...         NaN   \n",
       "5  NaN  Auris Surgical Robotics, Inc. is a technology ...         NaN   \n",
       "6  NaN  Sense Corp powers insight-driven organizations...         NaN   \n",
       "7  NaN  We provide quality software and uncompromising...         NaN   \n",
       "8  NaN  Driven by the ever-changing needs of our clien...         NaN   \n",
       "9  NaN  Americo Manufacturing Company, headquartered i...         NaN   \n",
       "\n",
       "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Abstract  \n",
       "0         NaN         NaN         NaN       NaN  \n",
       "1         NaN         NaN         NaN       NaN  \n",
       "2         NaN         NaN         NaN       NaN  \n",
       "3         NaN         NaN         NaN       NaN  \n",
       "4         NaN         NaN         NaN       NaN  \n",
       "5         NaN         NaN         NaN       NaN  \n",
       "6         NaN         NaN         NaN       NaN  \n",
       "7         NaN         NaN         NaN       NaN  \n",
       "8         NaN         NaN         NaN       NaN  \n",
       "9         NaN         NaN         NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(input_filename)\n",
    "print('Article count: ', len(df))\n",
    "# For testing\n",
    "# df = df.head(10)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spacy, a Natural Language Processing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(texts):\n",
    "    docs = [doc for doc in nlp.pipe(texts, batch_size=500, n_threads=4)]\n",
    "    return docs\n",
    "\n",
    "def to_text(tokens):\n",
    "    return next(map(lambda token: token.orth_, tokens), '')\n",
    "\n",
    "\n",
    "def to_text(tokens):\n",
    "  for token in tokens:\n",
    "    print(token.orth_)\n",
    "  return next(map(lambda token: token.orth_, tokens), '')\n",
    "\n",
    "def filter_first_punct(noun_chunks):\n",
    "    noun_chunks = list(noun_chunks)\n",
    "    if len(noun_chunks) > 0:\n",
    "        print('ROOT', noun_chunks[0].sent[noun_chunks[0].start])\n",
    "    return []\n",
    "\n",
    "def get_nouns(sentences):\n",
    "    return [to_text(docs.noun_chunks) for docs in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the titles and abstracts. Token types available [here](https://spacy.io/docs/usage/pos-tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docs'] = tokenize_text(df['Title'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(article_docs):\n",
    "    # print('Domain:       ', url)\n",
    "    print('Title:     ', article_docs)\n",
    "    print('-------------')\n",
    "    print('Words:     ', list(map(lambda word: word, article_docs)))\n",
    "    print('Lemma:     ', list(map(lambda word: word.lemma_, article_docs)))\n",
    "    print('Types:     ', list(map(lambda word: word.pos_, article_docs)))\n",
    "    print('Tags:      ', list(map(lambda word: word.tag_, article_docs)))\n",
    "    print('>')\n",
    "    print('Nouns:     ', list(filter(lambda word: word.pos_ == 'NOUN' or word.tag_ == 'NNP' or word.tag_ == 'NNPS', article_docs)))\n",
    "    print('Nouns sentences (chunks):     ', get_nouns(article_docs.sents))\n",
    "    print('Noun chunks:', list(article_docs.noun_chunks))\n",
    "    print('Noun chunks +1 words:', list(filter(lambda chunk: len(str(chunk).split(' ')) >= 2, list(article_docs.noun_chunks))))\n",
    "    print('Verbs:     ', list(filter(lambda word: word.pos_ == 'VERB', article_docs)))\n",
    "    print('Verbs Lemma:', list(map(lambda word: word.lemma_, filter(lambda word: word.pos_ == 'VERB', article_docs))))\n",
    "    print('Adjectives:', list(filter(lambda word: word.pos_ == 'ADJ', article_docs)))\n",
    "    print('Adjs Lemma:', list(map(lambda word: word.lemma_, filter(lambda word: word.pos_ == 'ADJ', article_docs))))\n",
    "    print('Adverbs:   ', list(filter(lambda word: word.pos_ == 'ADV', article_docs)))\n",
    "    print('Adverbs Lemma:', list(map(lambda word: word.lemma_, filter(lambda word: word.pos_ == 'ADV', article_docs))))\n",
    "    print('Superlatives:', list(filter(lambda word: word.tag_ == 'JJS' or word.tag_ == 'RBS', article_docs)))\n",
    "    print('Entities:  ', list(map(lambda entity: (entity, entity.label_), article_docs.ents)))\n",
    "    \n",
    "def df_url_docs(id, docs_field = 'docs'):\n",
    "    return df[docs_field][id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually check that the correct data types have been identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:      ParaDocs provides our clients with the highest level of emergency medical standby. Professionalism, discretion, and clinical excellence are our core values. From large public concerts to intimate private dinners we can provide an array of medical services to fit your needs. We provide only the most highly trained EMTâ€™s, paramedics, doctors, or ambulances services.\n",
      "-------------\n",
      "Words:      [ParaDocs, provides, our, clients, with, the, highest, level, of, emergency, medical, standby, ., Professionalism, ,, discretion, ,, and, clinical, excellence, are, our, core, values, ., From, large, public, concerts, to, intimate, private, dinners, we, can, provide, an, array, of, medical, services, to, fit, your, needs, ., We, provide, only, the, most, highly, trained, EMTâ€, ™, s, ,, paramedics, ,, doctors, ,, or, ambulances, services, .]\n",
      "Lemma:      ['paradocs', 'provide', '-PRON-', 'client', 'with', 'the', 'high', 'level', 'of', 'emergency', 'medical', 'standby', '.', 'professionalism', ',', 'discretion', ',', 'and', 'clinical', 'excellence', 'be', '-PRON-', 'core', 'value', '.', 'from', 'large', 'public', 'concert', 'to', 'intimate', 'private', 'dinner', '-PRON-', 'can', 'provide', 'an', 'array', 'of', 'medical', 'service', 'to', 'fit', '-PRON-', 'need', '.', '-PRON-', 'provide', 'only', 'the', 'most', 'highly', 'train', 'emtâ€', '™', 's', ',', 'paramedic', ',', 'doctor', ',', 'or', 'ambulance', 'service', '.']\n",
      "Types:      ['PROPN', 'VERB', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'CCONJ', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PUNCT', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PART', 'VERB', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'ADV', 'DET', 'ADV', 'ADV', 'VERB', 'NOUN', 'PROPN', 'PART', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'CCONJ', 'NOUN', 'NOUN', 'PUNCT']\n",
      "Tags:       ['NNP', 'VBZ', 'PRP$', 'NNS', 'IN', 'DT', 'JJS', 'NN', 'IN', 'NN', 'JJ', 'NN', '.', 'NN', ',', 'NN', ',', 'CC', 'JJ', 'NN', 'VBP', 'PRP$', 'JJ', 'NNS', '.', 'IN', 'JJ', 'JJ', 'NNS', 'IN', 'JJ', 'JJ', 'NNS', 'PRP', 'MD', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'PRP$', 'NNS', '.', 'PRP', 'VBP', 'RB', 'DT', 'RBS', 'RB', 'VBN', 'NN', 'NNP', 'POS', ',', 'NNS', ',', 'NNS', ',', 'CC', 'NNS', 'NNS', '.']\n",
      ">\n",
      "Nouns:      [ParaDocs, clients, level, emergency, standby, Professionalism, discretion, excellence, values, concerts, dinners, array, services, needs, EMTâ€, ™, paramedics, doctors, ambulances, services]\n",
      "Nouns sentences (chunks):      ['ParaDocs', 'Professionalism', 'large public concerts', 'We']\n",
      "Noun chunks: [ParaDocs, our clients, the highest level, emergency medical standby, Professionalism, discretion, clinical excellence, our core values, large public concerts, to intimate private dinners, we, an array, medical services, your needs, We, only the most highly trained EMTâ€, paramedics, doctors, ambulances services]\n",
      "Noun chunks +1 words: [our clients, the highest level, emergency medical standby, clinical excellence, our core values, large public concerts, to intimate private dinners, an array, medical services, your needs, only the most highly trained EMTâ€, ambulances services]\n",
      "Verbs:      [provides, are, can, provide, fit, provide, trained]\n",
      "Verbs Lemma: ['provide', 'be', 'can', 'provide', 'fit', 'provide', 'train']\n",
      "Adjectives: [our, highest, medical, clinical, our, core, large, public, intimate, private, medical, your]\n",
      "Adjs Lemma: ['-PRON-', 'high', 'medical', 'clinical', '-PRON-', 'core', 'large', 'public', 'intimate', 'private', 'medical', '-PRON-']\n",
      "Adverbs:    [only, most, highly]\n",
      "Adverbs Lemma: ['only', 'most', 'highly']\n",
      "Superlatives: [highest, most]\n",
      "Entities:   [(ParaDocs, 'ORG'), (™s, 'PRODUCT')]\n"
     ]
    }
   ],
   "source": [
    "print_tokens(df_url_docs(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through every article and applies f() to it. \n",
    "# Then applies token_extractor() to convert from a Token to a string.\n",
    "# Finally, concatenates the tokens of a single type with commas\n",
    "def filter_bad_excel_strings(tokens_string):\n",
    "    tokens_string = re.sub('[\\000-\\010]|[\\013-\\014]|[\\016-\\037]', '', tokens_string)\n",
    "    if tokens_string.startswith(\"=\"):\n",
    "        return tokens_string[1:]\n",
    "    elif tokens_string.startswith(\"- \"):\n",
    "        return tokens_string[2:]\n",
    "    else:\n",
    "        return tokens_string\n",
    "\n",
    "def map_articles(token_extractor, f, articles):\n",
    "    def map_article(article):\n",
    "        tokens_string = \",\".join(map(token_extractor, f(article)))\n",
    "        # Replace excel invalid chars\n",
    "        tokens_strings = filter_bad_excel_strings(tokens_string)\n",
    "        return tokens_string\n",
    "    return list(map(map_article, articles))\n",
    "\n",
    "def make_excel_df(docs_column_name = 'docs'):\n",
    "    df_excel = pd.DataFrame()\n",
    "    docs = df[docs_column_name]\n",
    "    df_excel['Title'] = df['Title']\n",
    "    df_excel['Nouns'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'NOUN' or word.tag_ == 'NNP' or word.tag_ == 'NNPS', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Noun Chunks (1)'] = map_articles(lambda token: token, \n",
    "                                     lambda doc : get_nouns(doc.sents), \n",
    "                                     docs)\n",
    "    df_excel['Noun Chunks (2)'] = map_articles(\n",
    "                                     lambda chunk: str(chunk),\n",
    "                                     lambda doc : list(doc.noun_chunks), \n",
    "                                     docs)\n",
    "    df_excel['Noun Chunks (3) +1 words'] = map_articles(\n",
    "                                     lambda chunk: str(chunk),\n",
    "                                     lambda doc : list(filter(lambda chunk: len(str(chunk).split(' ')) >= 2, list(doc.noun_chunks))), \n",
    "                                     docs)\n",
    "    #df_excel['Noun Chunks +1 words (3)'] = map_articles(lambda token: token, \n",
    "    #                                 list(filter(lambda chunk: len(str(chunk).split(' ')) >= 2, list(article_docs.noun_chunks))))\n",
    "    df_excel['Verbs'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'VERB', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Verbs Lemma'] = map_articles(lambda token: token.lemma_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'VERB', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adjectives'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADJ', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adjectives Lemma'] = map_articles(lambda token: token.lemma_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADJ', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adverbs'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADV', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adverbs Lemma'] = map_articles(lambda token: token.lemma_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADV', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Superlatives'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.tag_ == 'JJS' or word.tag_ == 'RBS', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Superlatives Lemma'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.tag_ == 'JJS' or word.tag_ == 'RBS', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Entities'] = map_articles(lambda ent: ent.orth_, \n",
    "                                     lambda sentence : sentence.ents, \n",
    "                                     docs)\n",
    "    return df_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Noun Chunks (1)</th>\n",
       "      <th>Noun Chunks (2)</th>\n",
       "      <th>Noun Chunks (3) +1 words</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Verbs Lemma</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Adjectives Lemma</th>\n",
       "      <th>Adverbs</th>\n",
       "      <th>Adverbs Lemma</th>\n",
       "      <th>Superlatives</th>\n",
       "      <th>Superlatives Lemma</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParaDocs provides our clients with the highest...</td>\n",
       "      <td>ParaDocs,clients,level,emergency,standby,Profe...</td>\n",
       "      <td>ParaDocs,Professionalism,large public concerts,We</td>\n",
       "      <td>ParaDocs,our clients,the highest level,emergen...</td>\n",
       "      <td>our clients,the highest level,emergency medica...</td>\n",
       "      <td>provides,are,can,provide,fit,provide,trained</td>\n",
       "      <td>provide,be,can,provide,fit,provide,train</td>\n",
       "      <td>our,highest,medical,clinical,our,core,large,pu...</td>\n",
       "      <td>-PRON-,high,medical,clinical,-PRON-,core,large...</td>\n",
       "      <td>only,most,highly</td>\n",
       "      <td>only,most,highly</td>\n",
       "      <td>highest,most</td>\n",
       "      <td>highest,most</td>\n",
       "      <td>ParaDocs,™s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGNITY Global Inc., (AGNITY) is a global provi...</td>\n",
       "      <td>AGNITY,Global,Inc.,AGNITY,provider,Intelligent...</td>\n",
       "      <td>AGNITY Global Inc.,AGNITYâ€™s products,All AGN...</td>\n",
       "      <td>AGNITY Global Inc.,AGNITY,a global provider,In...</td>\n",
       "      <td>AGNITY Global Inc.,a global provider,Intellige...</td>\n",
       "      <td>is,enable,transform,become,capitalize,are,powe...</td>\n",
       "      <td>be,enable,transform,become,capitalize,be,power...</td>\n",
       "      <td>global,real,their,workplace,competitive,new,it...</td>\n",
       "      <td>global,real,-PRON-,workplace,competitive,new,-...</td>\n",
       "      <td>more</td>\n",
       "      <td>more</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AGNITY Global Inc.,AGNITY,Intelligent Business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FlashCo has grown to be one of the largest man...</td>\n",
       "      <td>FlashCo,manufacturers,flashings,accessories,No...</td>\n",
       "      <td>FlashCo,We,FlashCo,Our designs,FlashCo,its pro...</td>\n",
       "      <td>FlashCo,the largest manufacturers,roof flashin...</td>\n",
       "      <td>the largest manufacturers,roof flashings,North...</td>\n",
       "      <td>has,grown,be,are,providing,designed,lasting,ha...</td>\n",
       "      <td>have,grow,be,be,provide,design,last,have,make,...</td>\n",
       "      <td>largest,roof,committed,longest,roof,available,...</td>\n",
       "      <td>large,roof,committed,long,roof,available,signi...</td>\n",
       "      <td>best,strongly,when</td>\n",
       "      <td>best,strongly,when</td>\n",
       "      <td>largest,best,longest,best,best,most,highest,ne...</td>\n",
       "      <td>largest,best,longest,best,best,most,highest,ne...</td>\n",
       "      <td>FlashCo,North America,FlashCo,FlashCo,FlashCo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAC Worldwide is a global leader in the manufa...</td>\n",
       "      <td>PAC,Worldwide,leader,manufacturing,distributio...</td>\n",
       "      <td>PAC Worldwide,Our products,We</td>\n",
       "      <td>PAC Worldwide,a global leader,the manufacturin...</td>\n",
       "      <td>PAC Worldwide,a global leader,the manufacturin...</td>\n",
       "      <td>is,branded,include,provide,branded,will,receive</td>\n",
       "      <td>be,brand,include,provide,brand,will,receive</td>\n",
       "      <td>global,protective,Our,flat,our,highest,our,you...</td>\n",
       "      <td>global,protective,-PRON-,flat,-PRON-,high,-PRO...</td>\n",
       "      <td>now</td>\n",
       "      <td>now</td>\n",
       "      <td>highest</td>\n",
       "      <td>highest</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DynexÂ® Technologies, Inc. is an original pion...</td>\n",
       "      <td>DynexÂ,®,Technologies,Inc.,pioneer,technology,...</td>\n",
       "      <td>DynexÂ®,Our talented, multidisciplinary staff,...</td>\n",
       "      <td>DynexÂ®,Technologies,Inc.,an original pioneer,...</td>\n",
       "      <td>an original pioneer,microplate technology,Our ...</td>\n",
       "      <td>is,include,deliver,cutting,meet,improve,enhanc...</td>\n",
       "      <td>be,include,deliver,cut,meet,improve,enhance,le...</td>\n",
       "      <td>original,microplate,Our,talented,multidiscipli...</td>\n",
       "      <td>original,microplate,-PRON-,talented,multidisci...</td>\n",
       "      <td>approximately,worldwide,most,ultimately,also,w...</td>\n",
       "      <td>approximately,worldwide,most,ultimately,also,w...</td>\n",
       "      <td>most</td>\n",
       "      <td>most</td>\n",
       "      <td>Technologies, Inc.,approximately 100,DSXÂ,ELIS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  ParaDocs provides our clients with the highest...   \n",
       "1  AGNITY Global Inc., (AGNITY) is a global provi...   \n",
       "2  FlashCo has grown to be one of the largest man...   \n",
       "3  PAC Worldwide is a global leader in the manufa...   \n",
       "4  DynexÂ® Technologies, Inc. is an original pion...   \n",
       "\n",
       "                                               Nouns  \\\n",
       "0  ParaDocs,clients,level,emergency,standby,Profe...   \n",
       "1  AGNITY,Global,Inc.,AGNITY,provider,Intelligent...   \n",
       "2  FlashCo,manufacturers,flashings,accessories,No...   \n",
       "3  PAC,Worldwide,leader,manufacturing,distributio...   \n",
       "4  DynexÂ,®,Technologies,Inc.,pioneer,technology,...   \n",
       "\n",
       "                                     Noun Chunks (1)  \\\n",
       "0  ParaDocs,Professionalism,large public concerts,We   \n",
       "1  AGNITY Global Inc.,AGNITYâ€™s products,All AGN...   \n",
       "2  FlashCo,We,FlashCo,Our designs,FlashCo,its pro...   \n",
       "3                      PAC Worldwide,Our products,We   \n",
       "4  DynexÂ®,Our talented, multidisciplinary staff,...   \n",
       "\n",
       "                                     Noun Chunks (2)  \\\n",
       "0  ParaDocs,our clients,the highest level,emergen...   \n",
       "1  AGNITY Global Inc.,AGNITY,a global provider,In...   \n",
       "2  FlashCo,the largest manufacturers,roof flashin...   \n",
       "3  PAC Worldwide,a global leader,the manufacturin...   \n",
       "4  DynexÂ®,Technologies,Inc.,an original pioneer,...   \n",
       "\n",
       "                            Noun Chunks (3) +1 words  \\\n",
       "0  our clients,the highest level,emergency medica...   \n",
       "1  AGNITY Global Inc.,a global provider,Intellige...   \n",
       "2  the largest manufacturers,roof flashings,North...   \n",
       "3  PAC Worldwide,a global leader,the manufacturin...   \n",
       "4  an original pioneer,microplate technology,Our ...   \n",
       "\n",
       "                                               Verbs  \\\n",
       "0       provides,are,can,provide,fit,provide,trained   \n",
       "1  is,enable,transform,become,capitalize,are,powe...   \n",
       "2  has,grown,be,are,providing,designed,lasting,ha...   \n",
       "3    is,branded,include,provide,branded,will,receive   \n",
       "4  is,include,deliver,cutting,meet,improve,enhanc...   \n",
       "\n",
       "                                         Verbs Lemma  \\\n",
       "0           provide,be,can,provide,fit,provide,train   \n",
       "1  be,enable,transform,become,capitalize,be,power...   \n",
       "2  have,grow,be,be,provide,design,last,have,make,...   \n",
       "3        be,brand,include,provide,brand,will,receive   \n",
       "4  be,include,deliver,cut,meet,improve,enhance,le...   \n",
       "\n",
       "                                          Adjectives  \\\n",
       "0  our,highest,medical,clinical,our,core,large,pu...   \n",
       "1  global,real,their,workplace,competitive,new,it...   \n",
       "2  largest,roof,committed,longest,roof,available,...   \n",
       "3  global,protective,Our,flat,our,highest,our,you...   \n",
       "4  original,microplate,Our,talented,multidiscipli...   \n",
       "\n",
       "                                    Adjectives Lemma  \\\n",
       "0  -PRON-,high,medical,clinical,-PRON-,core,large...   \n",
       "1  global,real,-PRON-,workplace,competitive,new,-...   \n",
       "2  large,roof,committed,long,roof,available,signi...   \n",
       "3  global,protective,-PRON-,flat,-PRON-,high,-PRO...   \n",
       "4  original,microplate,-PRON-,talented,multidisci...   \n",
       "\n",
       "                                             Adverbs  \\\n",
       "0                                   only,most,highly   \n",
       "1                                               more   \n",
       "2                                 best,strongly,when   \n",
       "3                                                now   \n",
       "4  approximately,worldwide,most,ultimately,also,w...   \n",
       "\n",
       "                                       Adverbs Lemma  \\\n",
       "0                                   only,most,highly   \n",
       "1                                               more   \n",
       "2                                 best,strongly,when   \n",
       "3                                                now   \n",
       "4  approximately,worldwide,most,ultimately,also,w...   \n",
       "\n",
       "                                        Superlatives  \\\n",
       "0                                       highest,most   \n",
       "1                                                      \n",
       "2  largest,best,longest,best,best,most,highest,ne...   \n",
       "3                                            highest   \n",
       "4                                               most   \n",
       "\n",
       "                                  Superlatives Lemma  \\\n",
       "0                                       highest,most   \n",
       "1                                                      \n",
       "2  largest,best,longest,best,best,most,highest,ne...   \n",
       "3                                            highest   \n",
       "4                                               most   \n",
       "\n",
       "                                            Entities  \n",
       "0                                        ParaDocs,™s  \n",
       "1  AGNITY Global Inc.,AGNITY,Intelligent Business...  \n",
       "2  FlashCo,North America,FlashCo,FlashCo,FlashCo,...  \n",
       "3                                                     \n",
       "4  Technologies, Inc.,approximately 100,DSXÂ,ELIS...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_excel_titles = make_excel_df()\n",
    "df_excel_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(output_filename)\n",
    "df_excel_titles.to_excel(writer,'Titles')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
