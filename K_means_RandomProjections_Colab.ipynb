{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gBbFDVQHSnu"
   },
   "source": [
    "\n",
    "# Clustering text documents using k-means and dimensionality reduction\n",
    "\n",
    "\n",
    "This is an example showing how dimensionality reduction can mitigate the \"curse of dimensionality\", by denoising data and improving performance of euclidean-based clustering approaches. In this example, we cluster a set of documents, represented as bag-of-worids, using two approaches:\n",
    "1. A standard k-means algorithm\n",
    "2. We apply k-means after reducing the dimensionality of the space via application of random projections\n",
    "\n",
    "We use standard measures of clustering quality to compare results provided by the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6QGGwd-HSnx"
   },
   "source": [
    "## Datasets\n",
    "To test our ideas, we begin with some standard datasets, for which ```sklearn``` provides a class for automatic downloading and preprocessing. \n",
    "As stated in the description, \"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\" Please refer to http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset for more information.\n",
    "\n",
    "To begin with, we import the libraries we will be using in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3hUitYSJHSny"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import homogeneity_score, homogeneity_completeness_v_measure, completeness_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection as srp\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDaxKNtlHSnz"
   },
   "source": [
    "We first select some categories from the 20 newsgroups dataset. These are specified by a list of string descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "813hkDtEHSn0",
    "outputId": "586d45ff-c2dd-4954-e4ab-0a6deabcc538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['talk.religion.misc', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSLutN9iHncH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_na9tcj0HSn1"
   },
   "source": [
    "We next download the corresponding dataset, both training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ozbRkMZnHSn1"
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', categories=categories, \n",
    "                             shuffle=False, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cp800bYuHo93"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITBZK8k9HSn2"
   },
   "source": [
    "Documents are not randomly reordered (```Shuffle=False```) and we remove all metadata, leaving only body text. ```train_set``` is an object describing the dataset. Its attributes ```filenames``` and ```target``` are two arrays, respectively containing the paths to the different documents and the corresponding labels, represented as integers from ```0``` to ```len(categories) - 1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mtIu6WsiHSn3"
   },
   "outputs": [],
   "source": [
    "labels = dataset.target\n",
    "true_k = len(np.unique(labels)) ## This should be 3 in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRYxEu7THulS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjeMnImHHSn4"
   },
   "source": [
    "We first perform lemmatization, which seems to behave better than stemming. The reason might be that the latter is too \"aggressive\" for this collection, consisting of short documents that may contain misspells, abbreviations etc. You would probably experience similar problems with a corpus of Twitter posts. You can try with stemming after commenting the next block of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPmsnQygHSn4",
    "outputId": "2aa01012-ad9b-4f7f-875b-b03ccfd3db9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# First, we download the necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# We next perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(dataset.data)):\n",
    "    word_list = word_tokenize(dataset.data[i])\n",
    "    lemmatized_doc = \"\"\n",
    "    for word in word_list:\n",
    "        lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n",
    "    dataset.data[i] = lemmatized_doc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-QYesIaHSn5"
   },
   "source": [
    "We next convert our corpus into tf-idf vectors in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S2ClmmEVHSn5"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english') ## Corpus is in English\n",
    "X = vectorizer.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYnNUBbBHSn5",
    "outputId": "f4c04de6-b83b-4eca-f383-8043d91cb4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2588, 28070)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnhEICFHSn6"
   },
   "source": [
    "## Classification take 1: using standard k-means\n",
    "We first cluster documents using the standard k-means algorithm (actually, a refined variant called k-means++), without any further data preprocessing. The key parameter of choice when performing k-means is $k$. Alas, there really is no principled way to choose an initial value for $k$. Essentially we have two options:\n",
    "\n",
    "1. We choose a value that reflects our knowledge about the data, as in this case\n",
    "2. We try several value, possibly in increasing order. We proceed this way as long as the quality of the resulting clustering (as measured by one or more quality indices) increases. We stop when it starts decreasing. As you may suspect, this case arises pretty often in practice\n",
    "\n",
    "In this specific case, we set $k = 3$ of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xny97Hj-HSn6",
    "outputId": "a278d7ba-742f-47ea-c056-5ef0564d8c13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.666s\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', n_init=20, max_iter=100)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU_HlonvHSn7"
   },
   "source": [
    "Quality indices for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRC0cn9mHSn7",
    "outputId": "e4ba5baa-d5d2-4cb9-a3d6-d96a05a2593c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.324\n",
      "Completeness: 0.420\n",
      "V-measure: 0.366\n",
      "Adjusted Rand-Index: 0.253\n",
      "Silhouette Coefficient: 0.009\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B07svW7-HSn7"
   },
   "source": [
    "We next identify the $10$ most important terms associated to the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kE3wv9UHSn7",
    "outputId": "921780e2-c904-4a8a-94a3-b3b42bef162e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: file image thanks format program know graphic color ftp bit\n",
      "Cluster 1: wa space like just think ha year time nasa know\n",
      "Cluster 2: god jesus wa christian people bible did say kent koresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b71pJ2GfHSn7"
   },
   "source": [
    "## Classification take 2: random projections first\n",
    "We next use again k-means, but we first use (sparse) random projections to project onto a low-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "S7if43nBHSn7"
   },
   "outputs": [],
   "source": [
    "transformer = srp(n_components=500, dense_output=False)\n",
    "X_proj = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArHRWzWPHSn8"
   },
   "source": [
    "Note that we set ```dense_output=True```. The reason is that otherwise the projected data will be represented as a sparse matrix, which considerably slows $k$-means' computation. Note also that the number of dimensions was chosen by trial and error and is lower than what suggested by Johnson-Lindenstrauss lemma. Actually, the number of dimensions required for $k$-means' cost to be preserved is lower than what we saw (recent results from 2019). We next perform k-means clustering on projected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzdjBVj6HSn9",
    "outputId": "aa10d23a-2ae9-4517-dc14-51d9532e6d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.459s\n",
      "Data shape: (2588, 500)\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', n_init=20, max_iter=100)\n",
    "t0 = time()\n",
    "km.fit(X_proj)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Data shape:\", X_proj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX3TMXDeHSn9"
   },
   "source": [
    "Note that reduction in computational time is definitely consistent with what we said in class: due to the linear dependence of Lloyd's algorithm on the number of dimensions, decreasing their number by a factor $f$ approximately affords a speed-up by the same factor. We next take the usual measures of the quality of the clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQzvPlG_HSn9",
    "outputId": "82fb6aba-6e66-441d-d982-53f7d51b808c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.300\n",
      "Completeness: 0.408\n",
      "V-measure: 0.345\n",
      "Adjusted Rand-Index: 0.236\n",
      "Silhouette Coefficient: 0.009\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baq1QIz9HSn-"
   },
   "source": [
    "Next, we want to find the most representative words for the clusters we obtained. The problem is that now the centroids returned by ```cluster.centers_``` attribute are in projected space and their dimensions have no particular meaning. We need to compute the corresponding centroids in the original space ourselves. This is easily done, since the ```labels_``` attribute of the ```KMeans``` object provides, for every $i$, a label identifying the cluster the $i$-th point was assigned to. We therefore proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xBpeVlw1HSn-"
   },
   "outputs": [],
   "source": [
    "centroids = np.zeros((true_k, X.shape[1])) # Initializing true_k centroid arrays\n",
    "cluster_sizes = [0]*true_k # For each cluster, the number of points it contains, needed for taking average\n",
    "for i in range(X_proj.shape[0]):\n",
    "    index = int(km.labels_[i]) # index is the index of the cluster the i-th point belongs to\n",
    "    centroids[index] += X[i] # Adding component-wise\n",
    "    cluster_sizes[index] += 1\n",
    "\n",
    "for i in range(true_k):\n",
    "    centroids[i] = centroids[i]/cluster_sizes[i] # Computing centroids: take sum and divide by cluster size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkJN7bBPHSn-"
   },
   "source": [
    "Once this is done, we proceed as in the previous case with the identification of representative words for each topic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikaUAKa_HSn-",
    "outputId": "c5fca8c9-98e3-4af8-eab0-a35cc9c27f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: wa god know did think people just like say jesus\n",
      "Cluster 1: space wa like launch just ha nasa program shuttle software\n",
      "Cluster 2: file image format gif program thanks know color convert graphic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "centroids = centroids.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpf3JPcBHSn_"
   },
   "source": [
    "With less than $(1/40)$-th of the original dimensions, quality of the results is almost as good. The speed-up is not as good on Colab, probably because for this dataset instance, pre-processing accounts for most of the computational cost. We instead observe roughly a 10-fold speed-up if the notebook is run as a Jupyter notebook on a Desktop running Ubuntu 20.04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDIzPQoqLtKH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
