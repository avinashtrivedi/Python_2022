{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = 'D:\\OneDrive - NITT\\Custom_Download'\n",
    "data_dir = main_dir #os.path.join(main_dir, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\OneDrive - NITT\\\\Custom_Download'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  cEXT  cNEU  cAGR  cCON  \\\n",
      "0  I just got back from your class, so I decided ...     0     0     1     1   \n",
      "1  It is 9:35 and I am beginning my stream of con...     1     0     1     0   \n",
      "2  Not only was the server down but it has taken ...     1     0     1     1   \n",
      "3  I am not exactly sure how this is supposed to ...     1     0     1     0   \n",
      "4  Well, here I am on Friday, September something...     0     0     1     0   \n",
      "\n",
      "   cOPN  split               id  \n",
      "0     0      2  2000_576170.txt  \n",
      "1     1      3  2000_576862.txt  \n",
      "2     1      3  1998_733941.txt  \n",
      "3     0      0  2000_904579.txt  \n",
      "4     0      9  2002_097387.txt  \n",
      "2467\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "essays = pd.read_csv(os.path.join(data_dir, 'essays.csv'), index_col=[0])\n",
    "essays['id']=essays['#AUTHID']\n",
    "essays=essays.drop(columns='#AUTHID')\n",
    "print(essays.head())\n",
    "print(len(essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>split</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just got back from your class, so I decided ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2000_576170.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is 9:35 and I am beginning my stream of con...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2000_576862.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not only was the server down but it has taken ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1998_733941.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am not exactly sure how this is supposed to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000_904579.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I am on Friday, September something...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2002_097387.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cEXT  cNEU  cAGR  cCON  \\\n",
       "0  I just got back from your class, so I decided ...     0     0     1     1   \n",
       "1  It is 9:35 and I am beginning my stream of con...     1     0     1     0   \n",
       "2  Not only was the server down but it has taken ...     1     0     1     1   \n",
       "3  I am not exactly sure how this is supposed to ...     1     0     1     0   \n",
       "4  Well, here I am on Friday, September something...     0     0     1     0   \n",
       "\n",
       "   cOPN  split               id  \n",
       "0     0      2  2000_576170.txt  \n",
       "1     1      3  2000_576862.txt  \n",
       "2     1      3  1998_733941.txt  \n",
       "3     0      0  2000_904579.txt  \n",
       "4     0      9  2002_097387.txt  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417 25 25\n"
     ]
    }
   ],
   "source": [
    "#split data\n",
    "train, test=train_test_split(essays, test_size=0.02, random_state = 54321)\n",
    "test, val = train_test_split(test, test_size = 0.5, random_state=54321)\n",
    "\n",
    "#reset index\n",
    "train=train.reset_index(drop=True)\n",
    "test=test.reset_index(drop=True)\n",
    "val=val.reset_index(drop=True)\n",
    "\n",
    "print(len(train), len(test), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cEXT :\n",
      "1    1251\n",
      "0    1166\n",
      "Name: cEXT, dtype: int64\n",
      "----------\n",
      "cNEU :\n",
      "1    1213\n",
      "0    1204\n",
      "Name: cNEU, dtype: int64\n",
      "----------\n",
      "cAGR :\n",
      "1    1285\n",
      "0    1132\n",
      "Name: cAGR, dtype: int64\n",
      "----------\n",
      "cCON :\n",
      "1    1229\n",
      "0    1188\n",
      "Name: cCON, dtype: int64\n",
      "----------\n",
      "cOPN :\n",
      "1    1244\n",
      "0    1173\n",
      "Name: cOPN, dtype: int64\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#check train data distribution\n",
    "label_cols =['cEXT', 'cNEU','cAGR',\t'cCON',\t'cOPN']\n",
    "\n",
    "for label in label_cols:\n",
    "    print(label, ':')\n",
    "    print(train[label].value_counts())\n",
    "    print('----------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution looks ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert yes and no to boolean\n",
    "labels=['cEXT','cNEU','cAGR','cCON','cOPN']\n",
    "\n",
    "for i in range(len(train)):\n",
    "    for label in labels:\n",
    "        if train[label].iloc[i]=='y':\n",
    "            train[label].iloc[i]=1\n",
    "        elif train[label].iloc[i]=='n':\n",
    "            train[label].iloc[i]=0\n",
    "\n",
    "for i in range(len(test)):\n",
    "    for label in labels:\n",
    "        if test[label].iloc[i]=='y':\n",
    "            test[label].iloc[i]=1\n",
    "        elif test[label].iloc[i]=='n':\n",
    "            test[label].iloc[i]=0\n",
    "\n",
    "\n",
    "for i in range(len(val)):\n",
    "    for label in labels:\n",
    "        if val[label].iloc[i]=='y':\n",
    "            val[label].iloc[i]=1\n",
    "        elif val[label].iloc[i]=='n':\n",
    "            val[label].iloc[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>split</th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am thinking about an email I just read. It's...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2002_012297.txt</td>\n",
       "      <td>[0, 0, 1, 1, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just completed the pretesting survey. Sigh, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2000_954075.txt</td>\n",
       "      <td>[1, 1, 0, 1, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have to work at 1pm, go back to my dorm, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1997_956061.txt</td>\n",
       "      <td>[0, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thnik that after this I'll do my chemis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2002_708914.txt</td>\n",
       "      <td>[0, 1, 1, 0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stream of Conciousness. hmmm. My mind is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2002_896653.txt</td>\n",
       "      <td>[0, 1, 0, 1, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cEXT  cNEU  cAGR  cCON  \\\n",
       "0  I am thinking about an email I just read. It's...     1     0     0     1   \n",
       "1  I just completed the pretesting survey. Sigh, ...     1     1     1     0   \n",
       "2  I have to work at 1pm, go back to my dorm, and...     1     0     1     1   \n",
       "3       I thnik that after this I'll do my chemis...     0     0     1     1   \n",
       "4       Stream of Conciousness. hmmm. My mind is ...     1     0     1     0   \n",
       "\n",
       "   cOPN  split               id           labels  \n",
       "0     1      9  2002_012297.txt  [0, 0, 1, 1, 9]  \n",
       "1     1      8  2000_954075.txt  [1, 1, 0, 1, 8]  \n",
       "2     0      0  1997_956061.txt  [0, 1, 1, 0, 0]  \n",
       "3     0      5  2002_708914.txt  [0, 1, 1, 0, 5]  \n",
       "4     1      8  2002_896653.txt  [0, 1, 0, 1, 8]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group labels together\n",
    "\n",
    "train['labels'] = train[train.columns[2:7]].values.tolist()\n",
    "test['labels'] = test[test.columns[2:7]].values.tolist()\n",
    "val['labels'] = val[val.columns[2:7]].values.tolist()\n",
    "\n",
    "train.head()\n",
    "test.head()\n",
    "val.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002_012297.txt</td>\n",
       "      <td>I am thinking about an email I just read. It's...</td>\n",
       "      <td>[0, 0, 1, 1, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000_954075.txt</td>\n",
       "      <td>I just completed the pretesting survey. Sigh, ...</td>\n",
       "      <td>[1, 1, 0, 1, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_956061.txt</td>\n",
       "      <td>I have to work at 1pm, go back to my dorm, and...</td>\n",
       "      <td>[0, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002_708914.txt</td>\n",
       "      <td>I thnik that after this I'll do my chemis...</td>\n",
       "      <td>[0, 1, 1, 0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002_896653.txt</td>\n",
       "      <td>Stream of Conciousness. hmmm. My mind is ...</td>\n",
       "      <td>[0, 1, 0, 1, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  \\\n",
       "0  2002_012297.txt  I am thinking about an email I just read. It's...   \n",
       "1  2000_954075.txt  I just completed the pretesting survey. Sigh, ...   \n",
       "2  1997_956061.txt  I have to work at 1pm, go back to my dorm, and...   \n",
       "3  2002_708914.txt       I thnik that after this I'll do my chemis...   \n",
       "4  2002_896653.txt       Stream of Conciousness. hmmm. My mind is ...   \n",
       "\n",
       "            labels  \n",
       "0  [0, 0, 1, 1, 9]  \n",
       "1  [1, 1, 0, 1, 8]  \n",
       "2  [0, 1, 1, 0, 0]  \n",
       "3  [0, 1, 1, 0, 5]  \n",
       "4  [0, 1, 0, 1, 8]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop unecessary columns\n",
    "train = train[['id','text', 'labels']]\n",
    "test = test[['id','text', 'labels']]\n",
    "val = val[['id','text', 'labels']]\n",
    "\n",
    "train.head()\n",
    "test.head()\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save modified data to OS\n",
    "train.to_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test.to_csv(os.path.join(data_dir, 'test.csv'))\n",
    "val.to_csv(os.path.join(data_dir, 'val.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import LongformerTokenizerFast, \\\n",
    "LongformerModel, LongformerConfig, Trainer, TrainingArguments,EvalPrediction, AutoTokenizer\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel, LongformerClassificationHead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a class that will handle the data\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, tokenizer, id_column, text_column, label_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "    \n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column\n",
    "        \n",
    "        # define the id column and transform it to list\n",
    "        self.id_column = id_column.tolist()\n",
    "        \n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text_column[index])\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length= 3048,\n",
    "                                       padding = 'max_length',\n",
    "                                       return_attention_mask = True,\n",
    "                                       truncation = True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        labels_ = torch.tensor(self.label_column[index], dtype=torch.float)\n",
    "        id_ = self.id_column[index]\n",
    "        return {'input_ids':input_ids[0], 'attention_mask':attention_mask[0], \n",
    "                'labels':labels_, 'id_':id_}\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.text_column) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d500541673e8421da7a5faa9a55c8e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0dbd3a6b7540ed939dbdcd7138c314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc694e686dd47ff8545c11ec632a792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d32f104453e4ad7b9750c542869784f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 4\n",
    "# create a class to process the traininga and test data\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', \n",
    "                                                    padding = 'max_length',\n",
    "                                                    truncation=True, \n",
    "                                                    max_length = 3048)\n",
    "training_data = Data_Processing(tokenizer, \n",
    "                                train['id'], \n",
    "                                train['text'], \n",
    "                                train['labels'])\n",
    "\n",
    "val_data =  Data_Processing(tokenizer, \n",
    "                             val['id'], \n",
    "                             val['text'], \n",
    "                             val['labels'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "                    'val': DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                 'val':len(val_data)\n",
    "                }\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=next(iter(dataloaders_dict['val']))\n",
    "a['id_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Longformer for multilabel classification class\n",
    "\n",
    "class LongformerForMultiLabelSequenceClassification(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    We instantiate a class of LongFormer adapted for a multilabel classification task. \n",
    "    This instance takes the pooled output of the LongFormer based model and passes it through a\n",
    "    classification head. We replace the traditional Cross Entropy loss with a BCE loss that generate probabilities\n",
    "    for all the labels that we feed into the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, pos_weight=None):\n",
    "        super(LongformerForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pos_weight = pos_weight\n",
    "        self.longformer = LongformerModel(config)\n",
    "        self.classifier = LongformerClassificationHead(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, \n",
    "                token_type_ids=None, position_ids=None, inputs_embeds=None, \n",
    "                labels=None):\n",
    "        \n",
    "        # create global attention on sequence, and a global attention token on the `s` token\n",
    "        # the equivalent of the CLS token on BERT models\n",
    "        if global_attention_mask is None:\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "        \n",
    "        # pass arguments to longformer model\n",
    "        outputs = self.longformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            global_attention_mask = global_attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids)\n",
    "        \n",
    "        # if specified the model can return a dict where each key corresponds to the output of a\n",
    "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
    "        # which will have the shape (batch_size, sequence_length, hidden_size). \n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "        \n",
    "        \n",
    "        # pass the hidden states through the classifier to obtain thee logits\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "            labels = labels.float()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.view(-1, self.num_labels))\n",
    "            #outputs = (loss,) + outputs\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongformerForMultiLabelSequenceClassification.from_pretrained('allenai/longformer-base-4096',                                                  \n",
    "                                                  gradient_checkpointing=False,\n",
    "                                                  attention_window = 512,\n",
    "                                                  num_labels = 5,\n",
    "                                                  return_dict=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "    \n",
    "def multi_label_metric(\n",
    "    predictions, \n",
    "    references, \n",
    "    ):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_true = references\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    metrics = {'f1':f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metric(\n",
    "        predictions=preds, \n",
    "        references=p.label_ids\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(main_dir, 'Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    num_train_epochs = 4,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 64,    \n",
    "    per_device_eval_batch_size= 16,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps = 1500,\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    fp16 = False,    \n",
    "    dataloader_num_workers = 0\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #data_collator = Data_Processing(),\n",
    "\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ba63c43e742a07d0b8f2dc79ddb9101d86a2cfb857b84681149a3b8cfa5a52e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
