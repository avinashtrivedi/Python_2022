{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:25:55.358606Z",
     "iopub.status.busy": "2022-12-18T03:25:55.358071Z",
     "iopub.status.idle": "2022-12-18T03:26:06.493421Z",
     "shell.execute_reply": "2022-12-18T03:26:06.491559Z",
     "shell.execute_reply.started": "2022-12-18T03:25:55.358565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\avitr\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "!pip install pyspark\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from time import time\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:06.497395Z",
     "iopub.status.busy": "2022-12-18T03:26:06.496894Z",
     "iopub.status.idle": "2022-12-18T03:26:06.507789Z",
     "shell.execute_reply": "2022-12-18T03:26:06.506722Z",
     "shell.execute_reply.started": "2022-12-18T03:26:06.497344Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName('SparkifyProj').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:06.510680Z",
     "iopub.status.busy": "2022-12-18T03:26:06.509414Z",
     "iopub.status.idle": "2022-12-18T03:26:07.397888Z",
     "shell.execute_reply": "2022-12-18T03:26:07.396888Z",
     "shell.execute_reply.started": "2022-12-18T03:26:06.510640Z"
    }
   },
   "outputs": [],
   "source": [
    "# load gthe dataset\n",
    "data = spark.read.json(\"D:\\OneDrive - NITT\\Custom_Download\\mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:07.400268Z",
     "iopub.status.busy": "2022-12-18T03:26:07.399922Z",
     "iopub.status.idle": "2022-12-18T03:26:07.409769Z",
     "shell.execute_reply": "2022-12-18T03:26:07.408462Z",
     "shell.execute_reply.started": "2022-12-18T03:26:07.400233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'userId']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns # list the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:07.412729Z",
     "iopub.status.busy": "2022-12-18T03:26:07.411814Z",
     "iopub.status.idle": "2022-12-18T03:26:07.489403Z",
     "shell.execute_reply": "2022-12-18T03:26:07.488440Z",
     "shell.execute_reply.started": "2022-12-18T03:26:07.412693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|          artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|  Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|           Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|              Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "|    Adam Lambert|Logged In|    Colin|     M|           51| Freeman| 282.8273| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|   Time For Miracles|   200|1538352394000|Mozilla/5.0 (Wind...|    30|\n",
      "|          Enigma|Logged In|    Micah|     M|           80|    Long|262.71302| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|Knocking On Forbi...|   200|1538352416000|\"Mozilla/5.0 (Win...|     9|\n",
      "|       Daft Punk|Logged In|    Colin|     M|           52| Freeman|223.60771| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Harder Better Fas...|   200|1538352676000|Mozilla/5.0 (Wind...|    30|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5) # display top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:07.491147Z",
     "iopub.status.busy": "2022-12-18T03:26:07.490598Z",
     "iopub.status.idle": "2022-12-18T03:26:07.497820Z",
     "shell.execute_reply": "2022-12-18T03:26:07.496565Z",
     "shell.execute_reply.started": "2022-12-18T03:26:07.491115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:07.500736Z",
     "iopub.status.busy": "2022-12-18T03:26:07.499870Z",
     "iopub.status.idle": "2022-12-18T03:26:08.237477Z",
     "shell.execute_reply": "2022-12-18T03:26:08.236546Z",
     "shell.execute_reply.started": "2022-12-18T03:26:07.500688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count() # rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:08.238970Z",
     "iopub.status.busy": "2022-12-18T03:26:08.238556Z",
     "iopub.status.idle": "2022-12-18T03:26:08.248079Z",
     "shell.execute_reply": "2022-12-18T03:26:08.246749Z",
     "shell.execute_reply.started": "2022-12-18T03:26:08.238935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns) # no of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:08.251317Z",
     "iopub.status.busy": "2022-12-18T03:26:08.250155Z",
     "iopub.status.idle": "2022-12-18T03:26:15.264435Z",
     "shell.execute_reply": "2022-12-18T03:26:15.263552Z",
     "shell.execute_reply.started": "2022-12-18T03:26:08.251261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name          Number of NaN       \n",
      "--------------------------------------------\n",
      "artist               58392               \n",
      "auth                 0                   \n",
      "firstName            8346                \n",
      "gender               8346                \n",
      "itemInSession        0                   \n",
      "lastName             8346                \n",
      "length               58392               \n",
      "level                0                   \n",
      "location             8346                \n",
      "method               0                   \n",
      "page                 0                   \n",
      "registration         8346                \n",
      "sessionId            0                   \n",
      "song                 58392               \n",
      "status               0                   \n",
      "ts                   0                   \n",
      "userAgent            8346                \n",
      "userId               8346                \n"
     ]
    }
   ],
   "source": [
    "# column wise NaN Values\n",
    "print(f\"{'Column Name':<20} {'Number of NaN':<20}\")\n",
    "print('--------------------------------------------')\n",
    "for col in data.columns:\n",
    "    n = data[(data[col].isNull()) | (data[col]==\"\")].count()\n",
    "    \n",
    "    print(f\"{col:<20} {n:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:15.272383Z",
     "iopub.status.busy": "2022-12-18T03:26:15.270203Z",
     "iopub.status.idle": "2022-12-18T03:26:15.287893Z",
     "shell.execute_reply": "2022-12-18T03:26:15.286610Z",
     "shell.execute_reply.started": "2022-12-18T03:26:15.272341Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop rows where user id is NaN\n",
    "df_filtered = data.dropna(subset = ['userId'])\n",
    "\n",
    "# filter the data to consider rows where userid is not empty\n",
    "df_filtered = df_filtered.filter(df_filtered['userId'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:15.291012Z",
     "iopub.status.busy": "2022-12-18T03:26:15.290028Z",
     "iopub.status.idle": "2022-12-18T03:26:15.323713Z",
     "shell.execute_reply": "2022-12-18T03:26:15.322579Z",
     "shell.execute_reply.started": "2022-12-18T03:26:15.290965Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform ts and registation time format\n",
    "get_date_fn = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "df_filtered = df_filtered.withColumn('event_time', get_date_fn('ts'))\n",
    "df_filtered = df_filtered.withColumn('registration_time', get_date_fn('registration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:15.325387Z",
     "iopub.status.busy": "2022-12-18T03:26:15.325044Z",
     "iopub.status.idle": "2022-12-18T03:26:15.350013Z",
     "shell.execute_reply": "2022-12-18T03:26:15.349086Z",
     "shell.execute_reply.started": "2022-12-18T03:26:15.325356Z"
    }
   },
   "outputs": [],
   "source": [
    "# get day number\n",
    "get_day_fn = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).day)\n",
    "df_filtered = df_filtered.withColumn('day_of_month', get_day_fn('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:15.352407Z",
     "iopub.status.busy": "2022-12-18T03:26:15.351553Z",
     "iopub.status.idle": "2022-12-18T03:26:15.379608Z",
     "shell.execute_reply": "2022-12-18T03:26:15.378339Z",
     "shell.execute_reply.started": "2022-12-18T03:26:15.352359Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract Operating System\n",
    "dict_map = {'Compatible': 'Windows', 'Ipad': 'iPad', 'Iphone': 'iPhone', 'Macintosh': 'Mac',  \n",
    "       'Windows nt 5.1': 'Windows', 'Windows nt 6.0': 'Windows', 'Windows nt 6.1': 'Windows', \n",
    "       'Windows nt 6.2': 'Windows',  'Windows nt 6.3': 'Windows', 'X11': 'Linux'}\n",
    "\n",
    "get_operating_sys = udf(lambda x: dict_map[re.findall('\\(([^\\)]*)\\)', x)[0].split(';')[0].capitalize()])\n",
    "df_filtered = df_filtered.withColumn('operating_system', get_operating_sys(df_filtered.userAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:15.382038Z",
     "iopub.status.busy": "2022-12-18T03:26:15.381456Z",
     "iopub.status.idle": "2022-12-18T03:26:16.860042Z",
     "shell.execute_reply": "2022-12-18T03:26:16.858751Z",
     "shell.execute_reply.started": "2022-12-18T03:26:15.381989Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o221.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 1 times, most recent failure: Lost task 0.0 in stage 59.0 (TID 180) (DESKTOP-JID9S04 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$Lambda$3906/873611897.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2494/1887070209.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2305/16270310.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4125/834079987.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4123/718461260.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$Lambda$3906/873611897.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2494/1887070209.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2305/16270310.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m get_location_fn \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:\u001b[38;5;28mlen\u001b[39m(x)])\n\u001b[0;32m      3\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df_filtered\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation_state\u001b[39m\u001b[38;5;124m'\u001b[39m, get_location_fn(df_filtered\u001b[38;5;241m.\u001b[39mlocation))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdf_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocation_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o221.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 1 times, most recent failure: Lost task 0.0 in stage 59.0 (TID 180) (DESKTOP-JID9S04 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$Lambda$3906/873611897.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2494/1887070209.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2305/16270310.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4125/834079987.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4123/718461260.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$Lambda$3906/873611897.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2494/1887070209.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2305/16270310.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "# extract state from location\n",
    "get_location_fn = udf(lambda x:x[-2:len(x)])\n",
    "df_filtered = df_filtered.withColumn('location_state', get_location_fn(df_filtered.location))\n",
    "df_filtered.select('location_state').distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:16.861728Z",
     "iopub.status.busy": "2022-12-18T03:26:16.861362Z",
     "iopub.status.idle": "2022-12-18T03:26:16.897971Z",
     "shell.execute_reply": "2022-12-18T03:26:16.897013Z",
     "shell.execute_reply.started": "2022-12-18T03:26:16.861694Z"
    }
   },
   "outputs": [],
   "source": [
    "# create feature \"downgrade\"\n",
    "downgrade_event_fn = udf(lambda x: 0 if x != 'Submit Downgrade' else 1, IntegerType())\n",
    "df_filtered = df_filtered.withColumn('downgrade_event', downgrade_event_fn('page'))\n",
    "df_filtered = df_filtered.withColumn('downgrade', max('downgrade_event').over(Window.partitionBy('userId')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:16.899894Z",
     "iopub.status.busy": "2022-12-18T03:26:16.899108Z",
     "iopub.status.idle": "2022-12-18T03:26:16.946308Z",
     "shell.execute_reply": "2022-12-18T03:26:16.944987Z",
     "shell.execute_reply.started": "2022-12-18T03:26:16.899831Z"
    }
   },
   "outputs": [],
   "source": [
    "# create feature \"churn\"\n",
    "churn_event = udf(lambda x: 0 if x != 'Cancellation Confirmation' else 1, IntegerType())\n",
    "df_filtered = df_filtered.withColumn('churn_event', churn_event('page'))\n",
    "df_filtered = df_filtered.withColumn('churn', max('churn_event').over(Window.partitionBy('userId')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:57:36.226103Z",
     "iopub.status.busy": "2022-12-18T03:57:36.225657Z",
     "iopub.status.idle": "2022-12-18T03:57:46.068130Z",
     "shell.execute_reply": "2022-12-18T03:57:46.066889Z",
     "shell.execute_reply.started": "2022-12-18T03:57:36.226064Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pd = df_filtered.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:59:11.242158Z",
     "iopub.status.busy": "2022-12-18T03:59:11.241732Z",
     "iopub.status.idle": "2022-12-18T03:59:11.416315Z",
     "shell.execute_reply": "2022-12-18T03:59:11.415123Z",
     "shell.execute_reply.started": "2022-12-18T03:59:11.242124Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(df_pd['churn'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T04:17:24.903560Z",
     "iopub.status.busy": "2022-12-18T04:17:24.902626Z",
     "iopub.status.idle": "2022-12-18T04:17:24.931534Z",
     "shell.execute_reply": "2022-12-18T04:17:24.930303Z",
     "shell.execute_reply.started": "2022-12-18T04:17:24.903502Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T04:22:45.914094Z",
     "iopub.status.busy": "2022-12-18T04:22:45.913616Z",
     "iopub.status.idle": "2022-12-18T04:22:45.923505Z",
     "shell.execute_reply": "2022-12-18T04:22:45.922451Z",
     "shell.execute_reply.started": "2022-12-18T04:22:45.914048Z"
    }
   },
   "outputs": [],
   "source": [
    "def statPlot(colname):\n",
    "    \n",
    "    ############# crarte df ################\n",
    "    df1 = df_pd[[colname,'churn']]\n",
    "\n",
    "    df1  = df1.groupby([colname, \n",
    "                              'churn']).size().reset_index().pivot(columns='churn', \n",
    "                                                                        index=colname, \n",
    "                                                                    values=0)\n",
    "#     return df1\n",
    "    df1['percentage_of_churn']= df1[1]*100/(df1[0]+ df1[1])\n",
    "    # df_job['percentage_of_unsubscribers']= df_job['no']/(df_job['no']+ df_job['yes'])\n",
    "    df1.dropna(inplace=True)\n",
    "    df1= df1.sort_values('percentage_of_churn').reset_index()\n",
    "    df1[colname] = df1[colname].astype('str')\n",
    "    \n",
    "    ##### PLOT ####################\n",
    "    plt.figure(figsize=(15,5))\n",
    "    ax = sns.barplot(df1[colname],df1['percentage_of_churn'],\n",
    "                     palette=\"vlag\")\n",
    "    for bars in ax.containers:\n",
    "        ax.bar_label(bars, fmt=\"%.2f%%\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T04:22:46.123468Z",
     "iopub.status.busy": "2022-12-18T04:22:46.122990Z",
     "iopub.status.idle": "2022-12-18T04:22:46.357166Z",
     "shell.execute_reply": "2022-12-18T04:22:46.355502Z",
     "shell.execute_reply.started": "2022-12-18T04:22:46.123428Z"
    }
   },
   "outputs": [],
   "source": [
    "statPlot('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T04:23:36.181447Z",
     "iopub.status.busy": "2022-12-18T04:23:36.180967Z",
     "iopub.status.idle": "2022-12-18T04:23:36.474447Z",
     "shell.execute_reply": "2022-12-18T04:23:36.473167Z",
     "shell.execute_reply.started": "2022-12-18T04:23:36.181412Z"
    }
   },
   "outputs": [],
   "source": [
    "statPlot('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T04:24:09.073534Z",
     "iopub.status.busy": "2022-12-18T04:24:09.072914Z",
     "iopub.status.idle": "2022-12-18T04:24:09.400126Z",
     "shell.execute_reply": "2022-12-18T04:24:09.398998Z",
     "shell.execute_reply.started": "2022-12-18T04:24:09.073455Z"
    }
   },
   "outputs": [],
   "source": [
    "statPlot('operating_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T04:24:40.484108Z",
     "iopub.status.busy": "2022-12-18T04:24:40.483631Z",
     "iopub.status.idle": "2022-12-18T04:24:41.059172Z",
     "shell.execute_reply": "2022-12-18T04:24:41.057755Z",
     "shell.execute_reply.started": "2022-12-18T04:24:40.484068Z"
    }
   },
   "outputs": [],
   "source": [
    "statPlot('location_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:16.971793Z",
     "iopub.status.busy": "2022-12-18T03:26:16.971382Z",
     "iopub.status.idle": "2022-12-18T03:26:16.977605Z",
     "shell.execute_reply": "2022-12-18T03:26:16.976437Z",
     "shell.execute_reply.started": "2022-12-18T03:26:16.971751Z"
    }
   },
   "outputs": [],
   "source": [
    "model_features = [] # store model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:16.979786Z",
     "iopub.status.busy": "2022-12-18T03:26:16.979420Z",
     "iopub.status.idle": "2022-12-18T03:26:18.146711Z",
     "shell.execute_reply": "2022-12-18T03:26:18.145719Z",
     "shell.execute_reply.started": "2022-12-18T03:26:16.979755Z"
    }
   },
   "outputs": [],
   "source": [
    "# gender of the user\n",
    "temp = df_filtered.select(['userId', 'gender']).dropDuplicates(['userId'])\n",
    "temp = temp.replace(['F', 'M'], ['0', '1'], 'gender')\n",
    "df_gender = temp.withColumn('gender', temp.gender.cast('int'))\n",
    "model_features.append(df_gender)\n",
    "df_gender.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:18.148510Z",
     "iopub.status.busy": "2022-12-18T03:26:18.148144Z",
     "iopub.status.idle": "2022-12-18T03:26:19.373774Z",
     "shell.execute_reply": "2022-12-18T03:26:19.372661Z",
     "shell.execute_reply.started": "2022-12-18T03:26:18.148454Z"
    }
   },
   "outputs": [],
   "source": [
    "# level of the user\n",
    "temp = df_filtered.select(['userId', 'level']).dropDuplicates(['userId'])\n",
    "temp = temp.replace(['paid', 'free'], ['0', '1'], 'level')\n",
    "df_payment = temp.withColumn('level', temp.level.cast('int'))\n",
    "model_features.append(df_payment)\n",
    "df_payment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:19.375395Z",
     "iopub.status.busy": "2022-12-18T03:26:19.375042Z",
     "iopub.status.idle": "2022-12-18T03:26:21.112623Z",
     "shell.execute_reply": "2022-12-18T03:26:21.111282Z",
     "shell.execute_reply.started": "2022-12-18T03:26:19.375361Z"
    }
   },
   "outputs": [],
   "source": [
    "# did the user downgrade\n",
    "temp = df_filtered.select(['userId','downgrade']).dropDuplicates(['userId']) \n",
    "df_downgrade = temp.withColumn('downgrade', temp.downgrade.cast('int'))\n",
    "model_features.append(df_downgrade)\n",
    "df_downgrade.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:21.114455Z",
     "iopub.status.busy": "2022-12-18T03:26:21.113979Z",
     "iopub.status.idle": "2022-12-18T03:26:22.830823Z",
     "shell.execute_reply": "2022-12-18T03:26:22.829820Z",
     "shell.execute_reply.started": "2022-12-18T03:26:21.114404Z"
    }
   },
   "outputs": [],
   "source": [
    "# did the user churn\n",
    "temp = df_filtered.select(['userId','churn']).dropDuplicates(['userId'])\n",
    "df_churn = temp.withColumn('churn', temp.churn.cast('int'))\n",
    "df_churn.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:22.832924Z",
     "iopub.status.busy": "2022-12-18T03:26:22.832052Z",
     "iopub.status.idle": "2022-12-18T03:26:22.856468Z",
     "shell.execute_reply": "2022-12-18T03:26:22.855397Z",
     "shell.execute_reply.started": "2022-12-18T03:26:22.832878Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of songs the user listened to in total\n",
    "num_songs = df_filtered.select('userID','song').groupBy('userID').count()\n",
    "model_features.append(num_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:22.859045Z",
     "iopub.status.busy": "2022-12-18T03:26:22.858198Z",
     "iopub.status.idle": "2022-12-18T03:26:23.840900Z",
     "shell.execute_reply": "2022-12-18T03:26:23.839691Z",
     "shell.execute_reply.started": "2022-12-18T03:26:22.859008Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of Thumbs-Up/Down\n",
    "num_thumbs_up = df_filtered.select('userID','page').where(df_filtered.page == 'Thumbs Up').groupBy('userID').count().withColumnRenamed('count', 'num_thumbs_up') \n",
    "print(num_thumbs_up.show(5))\n",
    "model_features.append(num_thumbs_up)\n",
    "num_thumbs_down = df_filtered.select('userID','page').where(df_filtered.page == 'Thumbs Down').groupBy('userID').count().withColumnRenamed('count', 'num_thumbs_down') \n",
    "model_features.append(num_thumbs_down)\n",
    "print(num_thumbs_down.show(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:23.842249Z",
     "iopub.status.busy": "2022-12-18T03:26:23.841910Z",
     "iopub.status.idle": "2022-12-18T03:26:24.286978Z",
     "shell.execute_reply": "2022-12-18T03:26:24.285700Z",
     "shell.execute_reply.started": "2022-12-18T03:26:23.842217Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of songs added to playlist\n",
    "num_playlist = df_filtered.select('userID','page').where(df_filtered.page == 'Add to Playlist').groupBy('userID').count().withColumnRenamed('count', 'num_playlist')\n",
    "model_features.append(num_playlist)\n",
    "num_playlist.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:24.288649Z",
     "iopub.status.busy": "2022-12-18T03:26:24.288213Z",
     "iopub.status.idle": "2022-12-18T03:26:24.730449Z",
     "shell.execute_reply": "2022-12-18T03:26:24.729465Z",
     "shell.execute_reply.started": "2022-12-18T03:26:24.288613Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of friends added\n",
    "num_friends = df_filtered.select('userID','page').where(df_filtered.page == 'Add Friend').groupBy('userID').count().withColumnRenamed('count', 'num_friend')\n",
    "model_features.append(num_friends)\n",
    "num_friends.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:24.737325Z",
     "iopub.status.busy": "2022-12-18T03:26:24.736388Z",
     "iopub.status.idle": "2022-12-18T03:26:25.752517Z",
     "shell.execute_reply": "2022-12-18T03:26:25.751624Z",
     "shell.execute_reply.started": "2022-12-18T03:26:24.737282Z"
    }
   },
   "outputs": [],
   "source": [
    "# total length of listening\n",
    "sum_listened = df_filtered.select('userID','length').groupBy('userID').sum().withColumnRenamed('sum(length)', 'sum_listened')\n",
    "model_features.append(sum_listened)\n",
    "sum_listened.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:25.753862Z",
     "iopub.status.busy": "2022-12-18T03:26:25.753465Z",
     "iopub.status.idle": "2022-12-18T03:26:26.978984Z",
     "shell.execute_reply": "2022-12-18T03:26:26.977893Z",
     "shell.execute_reply.started": "2022-12-18T03:26:25.753828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of songs listened per session\n",
    "av_song_session = df_filtered.where('page == \"NextSong\"').groupby(['userId', 'sessionId']).count().groupby(['userId']).agg({'count':'avg'}).withColumnRenamed('avg(count)', 'av_song_session')\n",
    "model_features.append(av_song_session)\n",
    "av_song_session.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:26.980298Z",
     "iopub.status.busy": "2022-12-18T03:26:26.979961Z",
     "iopub.status.idle": "2022-12-18T03:26:28.451643Z",
     "shell.execute_reply": "2022-12-18T03:26:28.450273Z",
     "shell.execute_reply.started": "2022-12-18T03:26:26.980268Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of artists listened to\n",
    "num_artists = df_filtered.filter(df_filtered.page==\"NextSong\").select(['userId', 'artist']).dropDuplicates().groupby('userId').count().withColumnRenamed('count', 'num_artists') \n",
    "model_features.append(num_artists)\n",
    "num_artists.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:28.454605Z",
     "iopub.status.busy": "2022-12-18T03:26:28.453087Z",
     "iopub.status.idle": "2022-12-18T03:26:29.606135Z",
     "shell.execute_reply": "2022-12-18T03:26:29.605225Z",
     "shell.execute_reply.started": "2022-12-18T03:26:28.454555Z"
    }
   },
   "outputs": [],
   "source": [
    "# time since registration in days\n",
    "days_member = df_filtered.select('userId','ts','registration').withColumn(\n",
    "    'days_member',((df_filtered.ts - df_filtered.registration)/1000/3600/24)).groupBy('userId').agg(\n",
    "    {'days_member':'max'}).withColumnRenamed('max(days_member)','days_member') \n",
    "\n",
    "model_features.append(days_member)\n",
    "days_member.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:29.607456Z",
     "iopub.status.busy": "2022-12-18T03:26:29.607126Z",
     "iopub.status.idle": "2022-12-18T03:26:30.731409Z",
     "shell.execute_reply": "2022-12-18T03:26:30.730540Z",
     "shell.execute_reply.started": "2022-12-18T03:26:29.607426Z"
    }
   },
   "outputs": [],
   "source": [
    "# session count per user\n",
    "num_session = df_filtered.select('userId', 'sessionId').dropDuplicates().groupby('userId').count().withColumnRenamed('count', 'num_sessions') \n",
    "model_features.append(num_session)\n",
    "num_session.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:30.732807Z",
     "iopub.status.busy": "2022-12-18T03:26:30.732473Z",
     "iopub.status.idle": "2022-12-18T03:26:33.125361Z",
     "shell.execute_reply": "2022-12-18T03:26:33.124515Z",
     "shell.execute_reply.started": "2022-12-18T03:26:30.732776Z"
    }
   },
   "outputs": [],
   "source": [
    "# duration of the session\n",
    "sessionStart = df_filtered.groupBy('userId', 'sessionId').min('ts').withColumnRenamed('min(ts)', 'start')\n",
    "sessionEnd = df_filtered.groupBy('userId', 'sessionId').max('ts').withColumnRenamed('max(ts)', 'end')\n",
    "durSession = sessionStart.join(sessionEnd, ['userId', 'sessionId'])\n",
    "durSession = durSession.select('userId', 'sessionId', ((durSession.end-durSession.start)/(1000*60*60)).alias('dur_session'))\n",
    "model_features.append(durSession)\n",
    "durSession.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:33.126710Z",
     "iopub.status.busy": "2022-12-18T03:26:33.126330Z",
     "iopub.status.idle": "2022-12-18T03:26:33.599972Z",
     "shell.execute_reply": "2022-12-18T03:26:33.598913Z",
     "shell.execute_reply.started": "2022-12-18T03:26:33.126665Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, feature_to_join in enumerate(model_features):\n",
    "    df_churn = df_churn.join(feature_to_join,'userID','outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:33.601284Z",
     "iopub.status.busy": "2022-12-18T03:26:33.600963Z",
     "iopub.status.idle": "2022-12-18T03:26:33.649174Z",
     "shell.execute_reply": "2022-12-18T03:26:33.647888Z",
     "shell.execute_reply.started": "2022-12-18T03:26:33.601256Z"
    }
   },
   "outputs": [],
   "source": [
    "df_churn = df_churn.drop('userID') \n",
    "df_final = df_churn.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:33.651626Z",
     "iopub.status.busy": "2022-12-18T03:26:33.650498Z",
     "iopub.status.idle": "2022-12-18T03:26:40.191041Z",
     "shell.execute_reply": "2022-12-18T03:26:40.190063Z",
     "shell.execute_reply.started": "2022-12-18T03:26:33.651588Z"
    }
   },
   "outputs": [],
   "source": [
    "print((df_final.count(), len(df_final.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:40.192405Z",
     "iopub.status.busy": "2022-12-18T03:26:40.192054Z",
     "iopub.status.idle": "2022-12-18T03:26:40.223010Z",
     "shell.execute_reply": "2022-12-18T03:26:40.222072Z",
     "shell.execute_reply.started": "2022-12-18T03:26:40.192374Z"
    }
   },
   "outputs": [],
   "source": [
    "# rename\n",
    "df_final = df_final.withColumnRenamed(\"churn\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:40.224389Z",
     "iopub.status.busy": "2022-12-18T03:26:40.224033Z",
     "iopub.status.idle": "2022-12-18T03:26:40.326952Z",
     "shell.execute_reply": "2022-12-18T03:26:40.325987Z",
     "shell.execute_reply.started": "2022-12-18T03:26:40.224356Z"
    }
   },
   "outputs": [],
   "source": [
    "# vector assembler\n",
    "assembler = VectorAssembler(inputCols=df_final.columns[1:], outputCol=\"features\")\n",
    "data = assembler.transform(df_final)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:40.329089Z",
     "iopub.status.busy": "2022-12-18T03:26:40.328449Z",
     "iopub.status.idle": "2022-12-18T03:26:56.858836Z",
     "shell.execute_reply": "2022-12-18T03:26:56.857545Z",
     "shell.execute_reply.started": "2022-12-18T03:26:40.329056Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True)\n",
    "scalerModel = scaler.fit(data)\n",
    "data = scalerModel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:56.860638Z",
     "iopub.status.busy": "2022-12-18T03:26:56.860250Z",
     "iopub.status.idle": "2022-12-18T03:26:56.881799Z",
     "shell.execute_reply": "2022-12-18T03:26:56.880549Z",
     "shell.execute_reply.started": "2022-12-18T03:26:56.860603Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop column \"features\" - as no longer needed\n",
    "data = data.drop(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:56.885748Z",
     "iopub.status.busy": "2022-12-18T03:26:56.884923Z",
     "iopub.status.idle": "2022-12-18T03:26:56.925330Z",
     "shell.execute_reply": "2022-12-18T03:26:56.924373Z",
     "shell.execute_reply.started": "2022-12-18T03:26:56.885699Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the data into train and test \n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:56.926755Z",
     "iopub.status.busy": "2022-12-18T03:26:56.926338Z",
     "iopub.status.idle": "2022-12-18T03:26:56.934023Z",
     "shell.execute_reply": "2022-12-18T03:26:56.932792Z",
     "shell.execute_reply.started": "2022-12-18T03:26:56.926712Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "def get_report(model):\n",
    "    p_df = model.toPandas()\n",
    "    y_true = p_df['label']\n",
    "    y_pred = p_df['prediction']\n",
    "    print(classification_report(y_true,y_pred))\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:26:56.936509Z",
     "iopub.status.busy": "2022-12-18T03:26:56.936029Z",
     "iopub.status.idle": "2022-12-18T03:28:33.257203Z",
     "shell.execute_reply": "2022-12-18T03:28:33.255909Z",
     "shell.execute_reply.started": "2022-12-18T03:26:56.936440Z"
    }
   },
   "outputs": [],
   "source": [
    "lr =  LogisticRegression(featuresCol='scaled_features', labelCol='label')\n",
    "lr_clf = lr.fit(train)\n",
    "lr_acc = get_report(lr_clf.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:28:33.259217Z",
     "iopub.status.busy": "2022-12-18T03:28:33.258829Z",
     "iopub.status.idle": "2022-12-18T03:29:45.648931Z",
     "shell.execute_reply": "2022-12-18T03:29:45.646556Z",
     "shell.execute_reply.started": "2022-12-18T03:28:33.259181Z"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"scaled_features\")\n",
    "dt_clf = dt.fit(train)\n",
    "dt_acc = get_report(dt_clf.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:29:45.651300Z",
     "iopub.status.busy": "2022-12-18T03:29:45.650844Z",
     "iopub.status.idle": "2022-12-18T03:31:36.536649Z",
     "shell.execute_reply": "2022-12-18T03:31:36.535235Z",
     "shell.execute_reply.started": "2022-12-18T03:29:45.651252Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"scaled_features\")\n",
    "rf_clf = rf.fit(train)\n",
    "rf_acc = get_report(rf_clf.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:31:36.538676Z",
     "iopub.status.busy": "2022-12-18T03:31:36.538295Z",
     "iopub.status.idle": "2022-12-18T03:32:40.258047Z",
     "shell.execute_reply": "2022-12-18T03:32:40.256643Z",
     "shell.execute_reply.started": "2022-12-18T03:31:36.538639Z"
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"scaled_features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "results_gbt = gbt_model.transform(test)\n",
    "\n",
    "gbt_acc = get_report(results_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T03:32:40.261938Z",
     "iopub.status.busy": "2022-12-18T03:32:40.260117Z",
     "iopub.status.idle": "2022-12-18T03:32:40.278267Z",
     "shell.execute_reply": "2022-12-18T03:32:40.277153Z",
     "shell.execute_reply.started": "2022-12-18T03:32:40.261889Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression','Decision Tree','Random Forest', 'Gradient Boost Tree'],\n",
    "    'Score': [lr_acc,\n",
    "                dt_acc,\n",
    "                rf_acc,\n",
    "                gbt_acc]})\n",
    "\n",
    "result_df = results.sort_values(by='Score', ascending=False)\n",
    "result_df = result_df.set_index('Model')\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
