{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX03ez5ckq-7"
   },
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. \n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure. \n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Number of training data samples-----> 150\n",
      "Number of training features --------> 4\n",
      "Shape of the target value ----------> (150, 1)\n",
      "====================================================================================================\n",
      "Printing the tree :).....\n",
      "  Is 2>=3.0?\n",
      " ----- True branch :)\n",
      "    Is 3>=1.8?\n",
      "   ----- True branch :)\n",
      "      Is 2>=4.9?\n",
      "     ----- True branch :)\n",
      "        The predicted value --> 2.0\n",
      "     ----- False branch :)\n",
      "        Is 0>=6.0?\n",
      "       ----- True branch :)\n",
      "          The predicted value --> 2.0\n",
      "       ----- False branch :)\n",
      "          The predicted value --> 1.0\n",
      "   ----- False branch :)\n",
      "      Is 2>=5.0?\n",
      "     ----- True branch :)\n",
      "        Is 3>=1.6?\n",
      "       ----- True branch :)\n",
      "          Is 0>=7.2?\n",
      "         ----- True branch :)\n",
      "            The predicted value --> 2.0\n",
      "         ----- False branch :)\n",
      "            The predicted value --> 1.0\n",
      "       ----- False branch :)\n",
      "          The predicted value --> 2.0\n",
      "     ----- False branch :)\n",
      "        Is 3>=1.7?\n",
      "       ----- True branch :)\n",
      "          The predicted value --> 2.0\n",
      "       ----- False branch :)\n",
      "          The predicted value --> 1.0\n",
      " ----- False branch :)\n",
      "    The predicted value --> 0.0\n",
      "====================================================================================================\n",
      "Accuracy of the prediction is 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class DecisionNode:\n",
    "    \"\"\"\n",
    "    Class for a parent/leaf node in the decision tree.\n",
    "    A Node with node information about it's left and right nodes if any. it has the impurity info also.\n",
    "    \"\"\"\n",
    "    def __init__(self, impurity=None, question=None, feature_index=None, threshold=None,\n",
    "                 true_subtree=None, false_subtree=None):\n",
    "        \"\"\"\n",
    "        :param\n",
    "        \"\"\"\n",
    "        self.impurity = impurity\n",
    "        # Which question to ask , to split the dataset.\n",
    "        self.question = question \n",
    "        # Index of the feature which make the best fit for this node.\n",
    "        self.feature_index = feature_index\n",
    "        # The threshold value for that feature to make the split.\n",
    "        self.threshold = threshold\n",
    "        # DecisionNode Object of the left subtree.\n",
    "        self.true_left_subtree = true_subtree\n",
    "        # DecisionNode Object of the right subtree.\n",
    "        self.false_right_subtree = false_subtree\n",
    "\n",
    "class LeafNode:\n",
    "    \"\"\" Leaf Node of the decision tree.\"\"\"\n",
    "    def __init__(self, value):\n",
    "        self.prediction_value = value\n",
    "        \n",
    "        \n",
    "class DecisionTree:\n",
    "    \"\"\"Common class for making decision tree for classification and regression tasks.\"\"\"\n",
    "    def __init__(self, min_sample_split=3, min_impurity=1e-7, max_depth=float('inf'),\n",
    "                 impurity_function=None, leaf_node_calculation=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.impurity_function = impurity_function\n",
    "        self.leaf_node_calculation = leaf_node_calculation\n",
    "\n",
    "    def _partition_dataset(self, Xy, feature_index, threshold):\n",
    "        \"\"\"Split the dataset based on the given feature and threshold.\n",
    "        \n",
    "        \"\"\"\n",
    "        split_func = None\n",
    "        if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "            split_func = lambda sample: sample[feature_index] >= threshold\n",
    "        else:\n",
    "            split_func = lambda sample: sample[feature_index] == threshold\n",
    "\n",
    "        X_1 = np.array([sample for sample in Xy if split_func(sample)])\n",
    "        X_2 = np.array([sample for sample in Xy if not split_func(sample)])\n",
    "\n",
    "        return X_1, X_2\n",
    "\n",
    "    def _find_best_split(self, Xy):\n",
    "        \"\"\" Find the best question/best feature threshold which splits the data well.\n",
    "        \n",
    "        \"\"\"\n",
    "        best_question = tuple() # this will containe the feature and its value which make the best split(higest gain).\n",
    "        best_datasplit = {} # best data split.\n",
    "        largest_impurity = 0\n",
    "        n_features = (Xy.shape[1] - 1)\n",
    "        # iterate over all the features.\n",
    "        for feature_index in range(n_features):\n",
    "            # find the unique values in that feature.\n",
    "            unique_value = set(s for s in Xy[:,feature_index])\n",
    "            # iterate over all the unique values to find the impurity.\n",
    "            for threshold in unique_value:\n",
    "                # split the dataset based on the feature value.\n",
    "                true_xy, false_xy = self._partition_dataset(Xy, feature_index, threshold)\n",
    "                # skip the node which has any on type 0. because this means it is already pure.\n",
    "                if len(true_xy) > 0 and len(false_xy) > 0:\n",
    "                    \n",
    "\n",
    "                    # find the y values.\n",
    "                    y = Xy[:, -1]\n",
    "                    true_y = true_xy[:, -1]\n",
    "                    false_y = false_xy[:, -1]\n",
    "\n",
    "                    # calculate the impurity function.\n",
    "                    impurity = self.impurity_function(y, true_y, false_y)\n",
    "\n",
    "                    # if the calculated impurity is larger than save this value for comaparition.\n",
    "                    if impurity > largest_impurity:\n",
    "                        largest_impurity = impurity\n",
    "                        best_question = (feature_index, threshold)\n",
    "                        best_datasplit = {\n",
    "                                    \"leftX\": true_xy[:, :n_features],   # X of left subtree\n",
    "                                    \"lefty\": true_xy[:, n_features:],   # y of left subtree\n",
    "                                    \"rightX\": false_xy[:, :n_features],  # X of right subtree\n",
    "                                    \"righty\": false_xy[:, n_features:]   # y of right subtree\n",
    "                        }\n",
    "                    \n",
    "        return largest_impurity, best_question, best_datasplit\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\"\n",
    "        This is a recursive method to build the decision tree.\n",
    "        \"\"\"\n",
    "        n_samples , n_features = X.shape\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "        # find the Information gain on each feature each values and return the question which splits the data very well\n",
    "        # based on the impurity function. (classfication - Information gain, regression - variance reduction).\n",
    "        if (n_samples >= self.min_sample_split) and (current_depth <= self.max_depth):\n",
    "            # find the best split/ which question split the data well.\n",
    "            impurity, quesion, best_datasplit = self._find_best_split(Xy)\n",
    "            if impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branch.\n",
    "                true_branch = self._build_tree(best_datasplit[\"leftX\"], best_datasplit[\"lefty\"], current_depth + 1)\n",
    "                false_branch = self._build_tree(best_datasplit[\"rightX\"], best_datasplit[\"righty\"], current_depth + 1)\n",
    "                return DecisionNode( impurity=impurity, question=quesion, feature_index=quesion[0], threshold=quesion[1],\n",
    "                                    true_subtree=true_branch, false_subtree=false_branch)\n",
    "\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "        return LeafNode(value=leaf_value)\n",
    "\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the decision tree.\n",
    "\n",
    "        :param X: Train features/dependant values.\n",
    "        :param y: train target/independant value.\n",
    "        \"\"\"\n",
    "        self.root = self._build_tree(X, y, current_depth=0)\n",
    "\n",
    "    def predict_sample(self, x, tree=None):\n",
    "        \"\"\"move form the top to bottom of the tree make a prediction of the sample by the\n",
    "            value in the leaf node \"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        # if it a leaf node the return the prediction.\n",
    "        if isinstance(tree , LeafNode):\n",
    "\n",
    "            return tree.prediction_value\n",
    "        feature_value = x[tree.feature_index]\n",
    "\n",
    "        branch = tree.false_right_subtree\n",
    "\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            \n",
    "            if feature_value >= tree.threshold:\n",
    "\n",
    "                branch = tree.true_left_subtree\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_left_subtree\n",
    "\n",
    "        return self.predict_sample(x, branch)\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        \"\"\" predict the unknow feature.\"\"\"\n",
    "        x = np.array(test_X)\n",
    "        y_pred = [self.predict_sample(sample) for sample in x]\n",
    "        # y_pred = np.array(y_pred)\n",
    "        # y_pred = np.expand_dims(y_pred, axis = 1)\n",
    "        return y_pred\n",
    "    \n",
    "    def draw_tree(self, tree = None, indentation = \" \"):\n",
    "        \"\"\"print the whole decitions of the tree from top to bottom.\"\"\"\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        def print_question(question, indention):\n",
    "            \"\"\"\n",
    "            :param question: tuple of feature_index and threshold.\n",
    "            \"\"\"\n",
    "            feature_index = question[0]\n",
    "            threshold = question[1]\n",
    "\n",
    "            condition = \"==\"\n",
    "            if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "                condition = \">=\"\n",
    "            print(indention,\"Is {col}{condition}{value}?\".format(col=feature_index, condition=condition, value=threshold))\n",
    "\n",
    "        if isinstance(tree , LeafNode):\n",
    "            print(indentation,\"The predicted value -->\", tree.prediction_value)\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            # print the question.\n",
    "            print_question(tree.question,indentation)\n",
    "            if tree.true_left_subtree is not None:\n",
    "                # travers to the true left branch.\n",
    "                print (indentation + '----- True branch :)')\n",
    "                self.draw_tree(tree.true_left_subtree, indentation + \"  \")\n",
    "            if tree.false_right_subtree is not None:\n",
    "                # travers to the false right-side branch.\n",
    "                print (indentation + '----- False branch :)')\n",
    "                self.draw_tree(tree.false_right_subtree, indentation + \"  \")\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier(DecisionTree):\n",
    "    \"\"\" Decision Tree for the classification problem.\"\"\"\n",
    "    def __init__(self, min_sample_split=3, min_impurity=1e-7, max_depth=float('inf'),\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param min_sample_split: min value a leaf node must have.\n",
    "        :param min_impurity: minimum impurity.\n",
    "        :param max_depth: maximum depth of the tree.\n",
    "        \"\"\"\n",
    "        self._impurity_function = self._claculate_information_gain\n",
    "        self._leaf_value_calculation = self._calculate_majarity_class\n",
    "        super(DecisionTreeClassifier, self).__init__(min_sample_split=min_sample_split, min_impurity=min_impurity, max_depth=max_depth,\n",
    "                         impurity_function=self._impurity_function, leaf_node_calculation=self._leaf_value_calculation)\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\" Find the entropy for the given data\"\"\"\n",
    "        entropy = 0\n",
    "        unique_value = np.unique(y)\n",
    "        for val in unique_value:\n",
    "            # probability of that class.\n",
    "            p = len(y[y==val]) / len(y)\n",
    "            entropy += -p * (math.log(p) / math.log(2))\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def _claculate_information_gain(self, y, y1, y2):\n",
    "        \"\"\"\n",
    "        Calculate the information gain.\n",
    "\n",
    "        :param y: target value.\n",
    "        :param y1: target value for dataset in the true split/right branch.\n",
    "        :param y2: target value for dataset in the false split/left branch.\n",
    "        \"\"\"\n",
    "        # propobility of true values.\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = self._entropy(y)\n",
    "        info_gain = entropy - p * self._entropy(y1) - (1 - p) * self._entropy(y2)\n",
    "        return info_gain       \n",
    "\n",
    "    def _calculate_majarity_class(self, y):\n",
    "        \"\"\"\n",
    "        calculate the prediction value for that leaf node.\n",
    "        \n",
    "        :param y: leaf node target array.\n",
    "        \"\"\"\n",
    "        most_frequent_label = None\n",
    "        max_count = 0\n",
    "        unique_labels = np.unique(y)\n",
    "        # iterate over all the unique values and find their frequentcy count.\n",
    "        for label in unique_labels:\n",
    "            count = len( y[y == label])\n",
    "            if count > max_count:\n",
    "                most_frequent_label = label\n",
    "                max_count = count\n",
    "        return most_frequent_label\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree.\n",
    "\n",
    "        :param X: Feature array/depentant values.\n",
    "        :parma y: target array/indepentant values.\n",
    "        \"\"\"\n",
    "        # train the model.\n",
    "        super(DecisionTreeClassifier, self).train(X, y)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        \"\"\" predict the unknow feature.\"\"\"\n",
    "        y_pred = super(DecisionTreeClassifier, self).predict(test_X)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_pred = np.expand_dims(y_pred, axis = 1)\n",
    "        return y_pred\n",
    "    \n",
    "data = 'data (3).csv'\n",
    "    \n",
    "df = pd.read_csv(data)\n",
    "X = df.drop(['class'], axis=1)\n",
    "X = X.values\n",
    "y = df[['class']].values\n",
    "    \n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Number of training data samples-----> {}\".format(X.shape[0]))\n",
    "print(\"Number of training features --------> {}\".format(X.shape[1]))\n",
    "print(\"Shape of the target value ----------> {}\".format(y.shape))   \n",
    "\n",
    "#define the parameters\n",
    "sys.setrecursionlimit(2000)\n",
    "param = {\n",
    "    \"n_neibours\" : 5\n",
    "}\n",
    "print(\"=\"*100)\n",
    "decirion_tree_cla = DecisionTreeClassifier(min_sample_split=2, max_depth=45)\n",
    "\n",
    "# Train the model.\n",
    "decirion_tree_cla.train(X, y) \n",
    "# print the decision tree.\n",
    "print(\"Printing the tree :).....\")\n",
    "decirion_tree_cla.draw_tree()\n",
    "# Predict the values.\n",
    "y_pred = decirion_tree_cla.predict(X)\n",
    "\n",
    "#calculate accuracy.\n",
    "acc = np.sum(y==y_pred)/X.shape[0]\n",
    "print(\"=\"*100)\n",
    "print(\"Accuracy of the prediction is {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with criterion entropy: 0.3289\n",
      "Training-set accuracy score: 1.0000\n",
      "Training set score: 1.0000\n",
      "Test set score: 0.3289\n"
     ]
    }
   ],
   "source": [
    "# Importing other dependencies and various libraries we need to make sure this code works.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/CS6140/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Defining the data.\n",
    "data = 'data (3).csv'\n",
    "# Reading the data.\n",
    "df = pd.read_csv(data)\n",
    "# Shaping the data.\n",
    "df.shape\n",
    "# Defining the column names of our data, as we have to make sure everything is properly represented.\n",
    "col_names = ['feature1', 'feature2', 'feature3', 'feature4', 'class']\n",
    "df.columns = col_names\n",
    "\n",
    "# Dropping class for the X_data interpretation process.\n",
    "X = df.drop(['class'], axis=1)\n",
    "# Including class for 'class' for our y.\n",
    "y = df['class']\n",
    "# Training, testing and splitting! :)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.99, random_state = 42)\n",
    "# Shaping our data.\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Encoding our variables.\n",
    "encoder = ce.OrdinalEncoder(cols=['feature1', 'feature2', 'feature3', 'feature4'])\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n",
    "clf_en.fit(X_train, y_train)\n",
    "y_pred_en = clf_en.predict(X_test)\n",
    "y_pred_train_en = clf_en.predict(X_train)\n",
    "y_pred_train_en\n",
    "print('Model accuracy score with criterion entropy: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)))\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))\n",
    "print('Training set score: {:.4f}'.format(clf_en.score(X_train, y_train)))\n",
    "print('Test set score: {:.4f}'.format(clf_en.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "        \n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        None\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(input_var)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "    \n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x,y in zip(input_var, output_var):\n",
    "        cost[i] = compute_cost(input_var, output_var, params)\n",
    "        params_store[:, i] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "        \n",
    "        # Apply stochastic gradient descent\n",
    "        None\n",
    "            \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $θ_{0}$ +$θ_{1}$x1 +$θ_{2}$x2 for unknown constants $θ_{0}$,$θ_{1}$,$θ_{2}$. Use least squares linear regression to find an estimate of $θ_{0}$,$θ_{1}$,$θ_{2}$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
