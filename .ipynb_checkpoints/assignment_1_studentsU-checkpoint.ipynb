{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, train_test_split\n",
    "import matplotlib.pyplot as plot\n",
    "# we can use the LabelEncoder to encode the gender feature\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# oversample the minority class using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data loading and exploratory analysis (18/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n",
      "Mean age of individuals not infected 47.266666666666666\n",
      "               Age       ALB       ALP       ALT       AST       BIL  \\\n",
      "Age       1.000000 -0.197498  0.173340 -0.006021  0.088666  0.032492   \n",
      "ALB      -0.197498  1.000000 -0.141584  0.001606 -0.193450 -0.221651   \n",
      "ALP       0.173340 -0.141584  1.000000  0.214480  0.063948  0.056078   \n",
      "ALT      -0.006021  0.001606  0.214480  1.000000  0.273326 -0.038469   \n",
      "AST       0.088666 -0.193450  0.063948  0.273326  1.000000  0.312231   \n",
      "BIL       0.032492 -0.221651  0.056078 -0.038469  0.312231  1.000000   \n",
      "CHE      -0.075093  0.375878  0.033753  0.147000 -0.208536 -0.333172   \n",
      "CHOL      0.125641  0.208248  0.125429  0.068947 -0.209970 -0.180370   \n",
      "CREA     -0.022296 -0.001573  0.149832 -0.043025 -0.021387  0.031224   \n",
      "GGT       0.153087 -0.155749  0.454630  0.248114  0.491263  0.217024   \n",
      "PROT     -0.153668  0.557197 -0.055109  0.094730  0.040071 -0.047638   \n",
      "category  0.037781 -0.180923 -0.069342  0.089251  0.621724  0.398451   \n",
      "\n",
      "               CHE      CHOL      CREA       GGT      PROT  category  \n",
      "Age      -0.075093  0.125641 -0.022296  0.153087 -0.153668  0.037781  \n",
      "ALB       0.375878  0.208248 -0.001573 -0.155749  0.557197 -0.180923  \n",
      "ALP       0.033753  0.125429  0.149832  0.454630 -0.055109 -0.069342  \n",
      "ALT       0.147000  0.068947 -0.043025  0.248114  0.094730  0.089251  \n",
      "AST      -0.208536 -0.209970 -0.021387  0.491263  0.040071  0.621724  \n",
      "BIL      -0.333172 -0.180370  0.031224  0.217024 -0.047638  0.398451  \n",
      "CHE       1.000000  0.425456 -0.011157 -0.110345  0.295427 -0.230785  \n",
      "CHOL      0.425456  1.000000 -0.047744 -0.006895  0.207071 -0.270496  \n",
      "CREA     -0.011157 -0.047744  1.000000  0.121003 -0.031704  0.136772  \n",
      "GGT      -0.110345 -0.006895  0.121003  1.000000 -0.011767  0.437680  \n",
      "PROT      0.295427  0.207071 -0.031704 -0.011767  1.000000  0.084453  \n",
      "category -0.230785 -0.270496  0.136772  0.437680  0.084453  1.000000  \n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "#Variance is a measure of how data points differ from the mean\n",
    "# load the dataset (1)\n",
    "df = pd.read_csv('hcv_data_split.csv')\n",
    "# print the dimensionality of the dataframe (1)\n",
    "print(df.columns)\n",
    "\n",
    "# print the different data types that can be identified from the entire dataset (1)\n",
    "print(df.info())\n",
    "\n",
    "# print the gender distribution in the complete dataset(i.e., the number of male and female individuals) (1)\n",
    "print(df.groupby('Sex').count().astype(int))\n",
    "\n",
    "# print the class distribution of the entire dataset (1)\n",
    "\n",
    "\n",
    "# print the median age of patients in the dataset having the hepatitis C infection (1.5)\n",
    "index = df.index\n",
    "ageList=[]\n",
    "for i  in index: \n",
    "    if  df['category'][i] == 1:\n",
    "       ageList.append( int(df['Age'][i]) )\n",
    "#print(ageList)\n",
    "print(np.median( ageList))\n",
    "\n",
    "\n",
    "\n",
    "# print the mean age of individuals in the dataset who does not have hepatitis C infection(i.e., the control group) (1.5)\n",
    "not_infecAge=[]\n",
    "for i  in index: \n",
    "    if  df['category'][i] == 0:\n",
    "       not_infecAge.append( int(df['Age'][i]) )\n",
    "print(\"Mean age of individuals not infected\" , np.mean(not_infecAge))\n",
    "\n",
    "# split the dataset into train and test based on the field \"split\" (0.5 + 0.5)\n",
    "\n",
    "test, train =  train_test_split( df[df['split'] == 'train'])\n",
    "print(train)\n",
    "\n",
    "# print the dimensionality of the test dataset (0.5)\n",
    "print(test.ndim)\n",
    "\n",
    "# print the dimensionality of the training dataset (0.5)\n",
    "print(train.ndim)\n",
    "\n",
    "# print the proportional distribution of the classes to identify whether or not the classes are equally(or closer) distributed between the train and test datasets (1 + 1)\n",
    "\n",
    "\n",
    "\n",
    "# analyze the distribution of the individual features(i.e., by using the complete dataset) and plot a feature that has a rough approximation of a Gaussian distribution (2)\n",
    "\n",
    "\n",
    "# identify features that represent a notable correlation (i.e., either positive or negative correlation below or above -0.5 and 0.5) (3)\n",
    "corr_matrix= df.corr()\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model development (64/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# separate the features and the labels to be used in model development (2)\n",
    "\n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "\n",
    "\n",
    "# check for missing values in the training dataset and print how many rows can be identified with the missing values (1)\n",
    "\n",
    "\n",
    "# *data imputation: replacing missing data with substitued values \n",
    "# given the task in predicting individuals with hepatitis C infection, select two of the most appropriate imputation strategies to fill the missing values and briefly explain why you have selected the particular strategies in a markdown cell below the current cell (3)\n",
    "imputer_simple = SimpleImputer(strategy='median')\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "imputer_iter = IterativeImputer(max_iter=10)\n",
    "\n",
    "# print the rows before and after being imputed with the two selected strategies (5)\n",
    "\n",
    "\n",
    "# indicate the encoding strategy that is more appropriate given the categorical feature 'Sex' and briefly explain why you selected one strategy over the other (i.e., either OrdinalEncoder or OneHotEncoder) in the markdown cell mentioned below (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputations explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data encoding strategy explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3\n",
    "# select one of the scaling strategies and briefly explain why it is essential to scale your features in the markdown cell mentioned below (3)\n",
    "\n",
    "# create the necessary pipelines and combine the features to be used as the training data for the given algorithm (8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "# create the following four different models with their default hyperparameter values to be trained using the preprocessed data (0.5 * 4)\n",
    "# Support Vector Machine https://www.youtube.com/watch?v=8A7L0GsBiLQ expl:https://www.youtube.com/watch?v=efR1C6CvhmE leg:https://www.youtube.com/watch?v=FB5EdxAGxQg\n",
    "# https://www.youtube.com/watch?v=8A7L0GsBiLQ\n",
    "# Decision Trees\n",
    "# Random Forests \n",
    "# Naive Bayes\n",
    "#  Complete: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.b\n",
    "# use sklearn GridSearchCV to train your selected model with hyperparameter tuning\n",
    "# state briefly the advantage of using cross-validation in the markdown cell below (2)\n",
    "\n",
    "# finetune 2 or more of the hyperparameters mentioned below and use at least 2 different values for each hyperparameter except for the Naive Bayes algorithm(use param_grid={}) (8)\n",
    "# parameters for SVC:\n",
    "    # C -> e.g., 10, 100\n",
    "    # gamma ->  e.g., 0.001, 0.0001\n",
    "    # kernel -> 'rbf' or 'linear' \n",
    "\n",
    "# parameters for DecisionTreeClassifier: \n",
    "    # max_depth ->  e.g., 3, 4\n",
    "    # min_samples_split -> 5, 10\n",
    "    # min_samples_leaf -> 10, 20\n",
    "\n",
    "# parameters for RandomForestClassifier: \n",
    "    # n_estimators -> 100, 200\n",
    "    # max_depth -> 3, 5\n",
    "    # bootstrap -> True, False\n",
    "\n",
    "\n",
    "\n",
    "# initialize gridsearch with the required parameters, including the following scoring methods and refit='bal_accuracy' (2)\n",
    "scoring = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "# fit the training data (0.5)\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should you use cross-validation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the grid search cross-validation results listing the above mentioned evaluation methods (3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a dummy classifier to identify a simple baseline (i.e., a majority class baseline) so that you can compare your prediction results (3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the test data to be predicted (2)\n",
    "\n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "\n",
    "\n",
    "# transform test data for prediction (2)\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a table format, report the train and test results you have obtained for all 4 models. Your table must include the following columns: (6)\n",
    "- model\n",
    "- best parameters (validation)\n",
    "- best accuracy (validation)\n",
    "- best f1_macro (validation)\n",
    "- best accuracy (test)\n",
    "- best f1_macro (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling data imbalance (18/100)\n",
    "Given the dataset that can be considered as having an imbalance, we can use different data augmentation strategies based on the minority class.\n",
    "In this section, you will be given the task of oversampling the dataset using the Imbalanced-Learn Library. \n",
    "\n",
    "Please install the imbalanced-learn library using the following command:\n",
    "* conda install -c conda-forge imbalanced-learnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the oversampling object\n",
    "oversample = SMOTE()\n",
    "# oversample the minority class\n",
    "# input_x will be the transformed training data using the combined pipelines, and the labels represent the training labels\n",
    "input_x_over, y_over = oversample.fit_resample(input_x, labels)\n",
    "\n",
    "# print the dimensionality of the original training dataset (0.5)\n",
    "\n",
    "# print the dimensionality of the original training dataset (0.5)\n",
    "\n",
    "# print the new class distribution using the Counter (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the same models as before with their default hyperparameters (1)\n",
    "\n",
    "\n",
    "# initialize gridsearch with the required parameters as used before (2)\n",
    "\n",
    "\n",
    "# fit the oversampled training data (0.5)\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain predictions on test data using the best model from GridSearchCV above (i.e., .best_estimator_) (2)\n",
    "\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a table format, report the train and test results you have obtained for all 4 models. Your table must include the following columns: (6)\n",
    "- model\n",
    "- best parameters (validation)\n",
    "- best accuracy (validation)\n",
    "- best f1_macro (validation)\n",
    "- best accuracy (test)\n",
    "- best f1_macro (test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
