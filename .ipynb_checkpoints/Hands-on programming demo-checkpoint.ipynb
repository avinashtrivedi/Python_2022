{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5MB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.3)\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Collecting pytz>=2017.2 (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/2e/dec1cc18c51b8df33c7c4d0a321b084cf38e1733b98f9d15018880fb4970/pytz-2022.1-py2.py3-none-any.whl (503kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 68.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/eb/d0e658465c029feb7083139d9ead51000742e88b1fb7f1504e19e1b4ce6e/scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3MB 33.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.11.0)\n",
      "Collecting joblib>=0.11 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/d5/0163eb0cfa0b673aa4fe1cd3ea9d8a81ea0f32e50807b0c295871e4aab2e/joblib-1.1.0-py2.py3-none-any.whl (306kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 57.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/codio/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: pytz, pandas, joblib, threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 pandas-1.1.5 pytz-2022.1 scikit-learn-0.24.2 sklearn-0.0 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas numpy sklearn --user\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Ensembles\n",
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "- **Bagging**: Building multiple models (typically of the same type) from different subsamples \n",
    "of the training dataset.\n",
    "- **Boosting**: Building multiple models (typically of the same type) each of which learns to fix \n",
    "the prediction errors of a prior model in the sequence of models.\n",
    "- **Voting**: Building multiple models (typically of differing types) and simple statistics (like \n",
    "calculating the mean) are used to combine predictions.\n",
    "\n",
    "This assumes you are generally familiar with machine learning algorithms and ensemble methods \n",
    "and will not go into the details of how the algorithms work or their parameters. The Pima Indians \n",
    "onset of Diabetes dataset is used to demonstrate each algorithm. Each ensemble algorithm is \n",
    "demonstrated using 10-fold cross-validation and the classification accuracy performance metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Algorithms\n",
    "\n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset (with replacement) and training a model for each sample. The final output prediction is averaged across the predictions of all of the sub-models. The two bagging models covered in this section are as follows:\n",
    "\n",
    "- Bagged Decision Trees. \n",
    "- Random Forest.\n",
    "\n",
    "More information on Bagging, can be found in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">API Documentation</a> Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning. \n",
    "\n",
    "In the example below we demonstrate the use of the ```BaggingClassifier``` with the Classification and Regression Trees algorithm (```DecisionTreeClassifier```), with a total of 100 trees created. You can read about the Decision Tree class in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">API Documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7578263841421736\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "\n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed) \n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above code block, we can very easily modify it to build a Random Forest Classifier with 100 trees and 3 max features, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7656527682843473\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features) \n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about using the ```RandomForestClassifier``` in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">API Documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Algorithms\n",
    "\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay less attention to them in the construction of subsequent models. You can construct an AdaBoost model for classification using the ```AdaBoostClassifier``` class. The example below \n",
    "demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm. See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">API Documentation</a> to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552802460697198\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed=7\n",
    "num_trees = 30\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble\n",
    "\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data. The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from sub-models, but this is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n",
    "\n",
    "You can create a voting ensemble model for classification using the ```VotingClassifier``` class. The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem. See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\">API Documentation</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7721633629528367\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Shortlisting Algorithms\n",
    "\n",
    "You cannot know which algorithm will work best on your dataset beforehand. You must use trial and error to discover a shortlist of algorithms that do well on your problem that you can then double down on and tune further. I call this process spot-checking. \n",
    "\n",
    "The question is not: What algorithm should I use on my dataset? Instead it is: What algorithms should I spot-check on my dataset? You can guess at what algorithms might do well on your dataset, and this can be a good starting point. I recommend trying a mixture of algorithms and see what is good at picking out the structure in your data. Below are some suggestions when spot-checking algorithms on your dataset: \n",
    "\n",
    "- Try a mixture of algorithm representations (e.g. instances and trees).\n",
    "- Try a mixture of learning algorithms (e.g. different algorithms for learning the same type of representation).\n",
    "- Try a mixture of modelling types (e.g. linear and nonlinear functions or parametric and non-parametric).\n",
    "\n",
    "We are going to take a look at four classification algorithms that you can spot-check on your \n",
    "dataset. \n",
    "\n",
    "- Logistic Regression\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees (CART). \n",
    "- Support Vector Machines.\n",
    "\n",
    "This time, we will not go into the API or parameterisation of each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression. \n",
    "\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems. You can construct a logistic regression model using the ```LogisticRegression``` class. See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">API Documentation</a> for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708646616541353\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "num_folds = 10\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed,shuffle=True)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors (KNN). \n",
    "\n",
    "KNN uses a distance metric to find the k most similar instances in \n",
    "the training data for a new instance and takes the mean outcome of the neighbors as the prediction. \n",
    "You can construct a KNN model using the ```KNeighborsClassifier``` class. See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">API documentation</a> for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7109876965140123\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "SVM seek a line that best separates two classes. Those data \n",
    "instances that are closest to the line that best separates the classes are called support vectors and \n",
    "influence where the line is placed. SVM has been extended to support multiple classes. Of \n",
    "particular importance is the use of different kernel functions via the kernel parameter. A powerful \n",
    "Radial Basis Function is used by default. \n",
    "\n",
    "You can construct an SVM model using the ```SVC``` class.\n",
    "See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">API Documentation</a> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760457963089542\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed,shuffle=True)\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression Trees (CART or just decision trees) \n",
    "\n",
    "CART constructs a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index). You can construct a CART model using the ```DecisionTreeClassifier``` class. \n",
    "\n",
    "See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">API Documentation</a> for more details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.80      0.79        46\n",
      "         1.0       0.69      0.65      0.67        31\n",
      "\n",
      "    accuracy                           0.74        77\n",
      "   macro avg       0.73      0.72      0.73        77\n",
      "weighted avg       0.74      0.74      0.74        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CART Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "test_size = 0.1\n",
    "\n",
    "# split into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# fit a model\n",
    "model = DecisionTreeClassifier(criterion = \"gini\", random_state=seed)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, predicted)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision-tree learners can create over-complex trees that do not generalise the data well. This is \n",
    "called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum \n",
    "number of samples required at a leaf node or setting the maximum depth of the tree are necessary to \n",
    "avoid this problem. \n",
    "\n",
    "Pruning decision trees, is very easy to implement, we just need to tweak some of the parameters of the model, such as the ```max_leaf_nodes```, ```min_smaples```, and ```max_depth``` to achieve this, as the below code block demonstrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.93      0.82        46\n",
      "         1.0       0.83      0.48      0.61        31\n",
      "\n",
      "    accuracy                           0.75        77\n",
      "   macro avg       0.78      0.71      0.72        77\n",
      "weighted avg       0.77      0.75      0.74        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will rebuild a new tree by using above data and see how it works by tweeking the parameteres\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "test_size = 0.1\n",
    "seed = 7\n",
    "\n",
    "# split into train/test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# fit a model\n",
    "model = DecisionTreeClassifier(criterion = \"gini\", splitter = 'random', max_leaf_nodes = 10, min_samples_leaf = 5, max_depth= 5, random_state=seed)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, predicted)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparing ML Algorithms\n",
    "\n",
    "When you work on a machine learning project, you often end up with multiple good models to choose from. Each model will have different performance characteristics. Using resampling methods like cross-validation, you can get an estimate for how accurate each model may be on unseen data. You need to be able to use these estimates to choose one or two best models from the suite of models that you have created.\n",
    "\n",
    "When you have a new dataset, it is a good idea to visualise the data using different techniques in order to look at the data from different perspectives. The same idea applies to model selection. You should use a number of different ways of looking at the estimated accuracy of your machine learning algorithms in order to choose the one or two algorithm to finalise. A way to do this is to use visualisation methods to show the average accuracy, variance and other properties of the distribution of model accuracies. In this section you will discover exactly how you can do that in Python with scikit-learn.\n",
    "\n",
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is evaluated in the same way on the same data. You can achieve this by forcing each algorithm to be evaluated on a consistent test harness. In the example below six different classification algorithms are compared on a single dataset:\n",
    "\n",
    "- Logistic Regression.\n",
    "- Linear Discriminant Analysis.\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees. \n",
    "- Naive Bayes.\n",
    "- Support Vector Machines.\n",
    "\n",
    "The dataset is, as usual, the Pima Indians onset of diabetes problem. The problem has two classes and eight \n",
    "numeric input variables of varying scales. \n",
    "\n",
    "The 10-fold cross-validation procedure is used to evaluate each algorithm, importantly configured with the same random seed to ensure that the same splits to the training data are performed and that each algorithm is evaluated in precisely the same way. \n",
    "\n",
    "Each algorithm is given a short name, useful for summarising results afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.770865 (0.050905)\n",
      "LDA: 0.766969 (0.047966)\n",
      "KNN: 0.710988 (0.050792)\n",
      "CART: 0.688944 (0.052674)\n",
      "NB: 0.759142 (0.038960)\n",
      "SVM: 0.760458 (0.034712)\n"
     ]
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# load dataset\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "seed = 7 \n",
    "\n",
    "for name, model in models:\n",
    "   kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "   cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "   results.append(cv_results)\n",
    "   names.append(name)\n",
    "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "   print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pipelining\n",
    "\n",
    "There are standard workflows in applied machine learning. Standard because they overcome \n",
    "common problems like data leakage in your test harness. Python scikit-learn provides a ```Pipeline``` \n",
    "utility to help automate machine learning workflows. Pipelines work by allowing for a linear \n",
    "sequence of data transforms to be chained together culminating in a modelling process that can be \n",
    "evaluated.\n",
    "\n",
    "The goal is to ensure that all of the steps in the pipeline are constrained to the data available for the \n",
    "evaluation, such as the training dataset or each fold of the cross-validation procedure. You can learn \n",
    "more about pipelines in scikit-learn by reading the Pipeline section of the <a href=\"https://scikit-learn.org/stable/modules/compose.html\">API guide</a>. \n",
    "\n",
    "You can also review the <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline\">API documentation</a> for the ```Pipeline``` and ```FeatureUnion``` classes and the \n",
    "pipeline module. \n",
    "\n",
    "## The problem\n",
    "An easy trap to fall into in applied machine learning is leaking data from your training dataset to \n",
    "your test dataset. To avoid this trap you need a robust test harness with strong separation of training \n",
    "and testing. This includes data preparation. Data preparation is one easy way to leak knowledge of \n",
    "the whole training dataset to the algorithm. For example, preparing your data using normalisation or \n",
    "standardisation on the entire training dataset before learning would not be a valid test because the \n",
    "training dataset would have been influenced by the scale of the data in the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines help you prevent data leakage in your test harness by ensuring that data preparation like \n",
    "standardisation is constrained to each fold of your cross-validation procedure. The example below \n",
    "demonstrates this important data preparation and model evaluation workflow on the Pima Indians \n",
    "onset of diabetes dataset. The pipeline is defined with two steps:\n",
    "\n",
    "1. Standardise the data.\n",
    "2. Learn a Linear Discriminant Analysis model.\n",
    "\n",
    "The pipeline is then evaluated using 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669685577580315\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that standardises the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# load data\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction is another procedure that is susceptible to data leakage. Like data preparation, \n",
    "feature extraction procedures must be restricted to the data in your training dataset. The pipeline \n",
    "provides a handy tool called the ```FeatureUnion``` which allows the results of multiple feature \n",
    "selection and extraction procedures to be combined into a larger dataset on which a model can be \n",
    "trained. Importantly, all the feature extraction and the feature union occurs within each fold of the \n",
    "cross-validation procedure. The example below demonstrates the pipeline defined with four steps:\n",
    "\n",
    "    1. Feature Extraction with Principal Component Analysis (3 features). \n",
    "    2. Feature Extraction with Statistical Selection (6 features).\n",
    "    3. Feature Union.\n",
    "    4. Learn a Logistic Regression Model.\n",
    "    \n",
    "The pipeline is then evaluated using 10-fold cross-validation. Analyse the code block to see how the ```FeatureUnion``` is its own Pipeline that in turn is a single step in the final Pipeline used to feed Logistic Regression. This might get you thinking about how you can start embedding pipelines within pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# load data\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union)) \n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tuning the model\n",
    "\n",
    "Algorithm tuning is a final step in the process of applied machine learning before finalising your model. It is sometimes called hyperparameter optimisation where the algorithm parameters are referred to as hyperparameters, whereas the coefficients found by the machine learning algorithm itself are referred to as parameters. \n",
    "\n",
    "Optimisation suggests the search-nature of the problem. Phrased as a search problem, you can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem. Python scikit-learn provides two simple methods for algorithm parameter tuning:\n",
    "\n",
    "    1. Grid Search Parameter Tuning.\n",
    "    2. Random Search Parameter Tuning.\n",
    "    \n",
    "**Grid search** is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. You can perform a grid search using the ```GridSearchCV``` class. See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">API Documentation</a> to learn more about how to apply it. \n",
    "\n",
    "The example below evaluates different alpha values for the Ridge Regression algorithm on the standard diabetes dataset. This is a one-dimensional grid search: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.27610844129292433\n",
      "Recommended alpha value 1.0\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# The varying parameters to test and choose the best from\n",
    "alphas = numpy.array([1, 0.1, 0.01, 0.001, 0.0001, 0])\n",
    "\n",
    "param_grid = dict(alpha=alphas)\n",
    "model = Ridge()\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Recommended alpha value\", grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random search** is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen. You can perform a random search for algorithm parameters using the ```RandomizedSearchCV``` class. See the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\">API Documentation</a> for more information.\n",
    "\n",
    "The example below evaluates different random alpha values between 0 and 1 for the Ridge Regression algorithm on the standard diabetes dataset. \n",
    "\n",
    "A total of 100 iterations are performed with uniformly random alpha values selected in the range between 0 and 1 (the range that alpha values can take)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.27610755734028525\n",
      "Recommended alpha value 0.9779895119966027\n"
     ]
    }
   ],
   "source": [
    "# Randomized for Algorithm Tuning\n",
    "from pandas import read_csv\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Need to set the random seed for reproducability\n",
    "seed = 7\n",
    "param_grid = {'alpha': uniform()}\n",
    "\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=seed)\n",
    "rsearch.fit(X, Y)\n",
    "\n",
    "print(\"Best score\", rsearch.best_score_)\n",
    "print(\"Recommended alpha value\", rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Saving and Loading the Model\n",
    "Pickle is the standard way of serialising objects in Python. You can use the ```pickle``` operation to serialise your machine learning algorithms and save the serialised format to a file. See the <a href=\"https://docs.python.org/2/library/pickle.html\">API Documentation</a> to learn more. \n",
    "\n",
    "Later you can load this file to deserialise your model and use it to make new predictions. The example below demonstrates how you can train a logistic regression model on the Pima Indians onset of diabetes dataset, save the model to file and load it to make predictions on the unseen test \n",
    "set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tuning approaches returned differing recommended alpha values, but both are very close and have narrowed down the search leaving you to experiment and judge from the measures of accuracy you get from the model you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ./data/finalized_model.sav\n",
      "Loading ./data/finalized_model.sav\n",
      "0.7874015748031497\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using Pickle\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "filename = './data/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "# Fit the model on 33%\n",
    "model = LogisticRegression() \n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './data/finalized_model.sav' \n",
    "\n",
    "print(\"Saving\", filename)\n",
    "dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "# some time later...\n",
    "\n",
    "\n",
    "# load the model from disk\n",
    "print(\"Loading\", filename)\n",
    "loaded_model = load(open(filename, 'rb')) \n",
    "result = loaded_model.score(X_test, Y_test) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we have time for now, in the next hands-on demo we will start to explore neural networks and we implement some of the basic components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
