{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70da0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3287f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yumoxu/stocknet-code/blob/master/src/Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a536e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open('model.py', 'r') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e02b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line.strip() for line in text if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c35bac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#!/usr/local/bin/python',\n",
       " 'from __future__ import print_function',\n",
       " 'import os',\n",
       " 'import tensorflow as tf',\n",
       " 'import numpy as np',\n",
       " 'import neural as neural',\n",
       " 'import tensorflow.contrib.distributions as ds',\n",
       " 'from tensorflow.contrib.layers import batch_norm',\n",
       " 'from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser',\n",
       " 'class Model:',\n",
       " 'def __init__(self):',\n",
       " \"logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size))\",\n",
       " '# model config',\n",
       " \"self.mode = config_model['mode']\",\n",
       " \"self.opt = config_model['opt']\",\n",
       " \"self.lr = config_model['lr']\",\n",
       " \"self.decay_step = config_model['decay_step']\",\n",
       " \"self.decay_rate = config_model['decay_rate']\",\n",
       " \"self.momentum = config_model['momentum']\",\n",
       " \"self.kl_lambda_anneal_rate = config_model['kl_lambda_anneal_rate']\",\n",
       " \"self.kl_lambda_start_step = config_model['kl_lambda_start_step']\",\n",
       " \"self.use_constant_kl_lambda = config_model['use_constant_kl_lambda']\",\n",
       " \"self.constant_kl_lambda = config_model['constant_kl_lambda']\",\n",
       " \"self.daily_att = config_model['daily_att']\",\n",
       " \"self.alpha = config_model['alpha']\",\n",
       " \"self.clip = config_model['clip']\",\n",
       " \"self.n_epochs = config_model['n_epochs']\",\n",
       " \"self.batch_size_for_name = config_model['batch_size']\",\n",
       " \"self.max_n_days = config_model['max_n_days']\",\n",
       " \"self.max_n_msgs = config_model['max_n_msgs']\",\n",
       " \"self.max_n_words = config_model['max_n_words']\",\n",
       " \"self.weight_init = config_model['weight_init']\",\n",
       " \"uniform = True if self.weight_init == 'xavier-uniform' else False\",\n",
       " 'self.initializer = tf.contrib.layers.xavier_initializer(uniform=uniform)',\n",
       " 'self.bias_initializer = tf.constant_initializer(0.0, dtype=tf.float32)',\n",
       " \"self.word_embed_type = config_model['word_embed_type']\",\n",
       " \"self.y_size = config_model['y_size']\",\n",
       " \"self.word_embed_size = config_model['word_embed_size']\",\n",
       " \"self.stock_embed_size = config_model['stock_embed_size']\",\n",
       " \"self.price_embed_size = config_model['word_embed_size']\",\n",
       " \"self.mel_cell_type = config_model['mel_cell_type']\",\n",
       " \"self.variant_type = config_model['variant_type']\",\n",
       " \"self.vmd_cell_type = config_model['vmd_cell_type']\",\n",
       " \"self.vmd_rec = config_model['vmd_rec']\",\n",
       " \"self.mel_h_size = config_model['mel_h_size']\",\n",
       " \"self.msg_embed_size = config_model['mel_h_size']\",\n",
       " \"self.corpus_embed_size = config_model['mel_h_size']\",\n",
       " \"self.h_size = config_model['h_size']\",\n",
       " \"self.z_size = config_model['h_size']\",\n",
       " \"self.g_size = config_model['g_size']\",\n",
       " \"self.use_in_bn= config_model['use_in_bn']\",\n",
       " \"self.use_o_bn = config_model['use_o_bn']\",\n",
       " \"self.use_g_bn = config_model['use_g_bn']\",\n",
       " \"self.dropout_train_mel_in = config_model['dropout_mel_in']\",\n",
       " \"self.dropout_train_mel = config_model['dropout_mel']\",\n",
       " \"self.dropout_train_ce = config_model['dropout_ce']\",\n",
       " \"self.dropout_train_vmd_in = config_model['dropout_vmd_in']\",\n",
       " \"self.dropout_train_vmd = config_model['dropout_vmd']\",\n",
       " \"self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\",\n",
       " '# model name',\n",
       " \"name_pattern_max_n = 'days-{0}.msgs-{1}-words-{2}'\",\n",
       " 'name_max_n = name_pattern_max_n.format(self.max_n_days, self.max_n_msgs, self.max_n_words)',\n",
       " \"name_pattern_input_type = 'word_embed-{0}.vmd_in-{1}'\",\n",
       " 'name_input_type = name_pattern_input_type.format(self.word_embed_type, self.variant_type)',\n",
       " \"name_pattern_key = 'alpha-{0}.anneal-{1}.rec-{2}'\",\n",
       " 'name_key = name_pattern_key.format(self.alpha, self.kl_lambda_anneal_rate, self.vmd_rec)',\n",
       " \"name_pattern_train = 'batch-{0}.opt-{1}.lr-{2}-drop-{3}-cell-{4}'\",\n",
       " 'name_train = name_pattern_train.format(self.batch_size_for_name, self.opt, self.lr, self.dropout_train_mel_in, self.mel_cell_type)',\n",
       " 'name_tuple = (self.mode, name_max_n, name_input_type, name_key, name_train)',\n",
       " \"self.model_name = '_'.join(name_tuple)\",\n",
       " '# paths',\n",
       " 'self.tf_graph_path = os.path.join(path_parser.graphs, self.model_name)  # summary',\n",
       " 'self.tf_checkpoints_path = os.path.join(path_parser.checkpoints, self.model_name)  # checkpoints',\n",
       " \"self.tf_checkpoint_file_path = os.path.join(self.tf_checkpoints_path, 'checkpoint')  # for restore\",\n",
       " \"self.tf_saver_path = os.path.join(self.tf_checkpoints_path, 'sess')  # for save\",\n",
       " '# verification',\n",
       " \"assert self.opt in ('sgd', 'adam')\",\n",
       " \"assert self.mel_cell_type in ('ln-lstm', 'gru', 'basic')\",\n",
       " \"assert self.vmd_cell_type in ('ln-lstm', 'gru')\",\n",
       " \"assert self.variant_type in ('hedge', 'fund', 'tech', 'discriminative')\",\n",
       " \"assert self.vmd_rec in ('zh', 'h')\",\n",
       " \"assert self.weight_init in ('xavier-uniform', 'xavier-normal')\",\n",
       " 'def _build_placeholders(self):',\n",
       " \"with tf.name_scope('placeholder'):\",\n",
       " 'self.is_training_phase = tf.placeholder(dtype=tf.bool, shape=())',\n",
       " 'self.batch_size = tf.placeholder(dtype=tf.int32, shape=())',\n",
       " '# init',\n",
       " 'self.word_table_init = tf.placeholder(dtype=tf.float32, shape=[vocab_size, self.word_embed_size])',\n",
       " '# model',\n",
       " 'self.stock_ph = tf.placeholder(dtype=tf.int32, shape=[None])',\n",
       " 'self.T_ph = tf.placeholder(dtype=tf.int32, shape=[None, ])',\n",
       " 'self.n_words_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days, self.max_n_msgs])',\n",
       " 'self.n_msgs_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days])',\n",
       " 'self.y_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.max_n_days, self.y_size])  # 2-d vectorised movement',\n",
       " 'self.mv_percent_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days])  # movement percent',\n",
       " 'self.price_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.max_n_days, 3])  # high, low, close',\n",
       " 'self.word_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days, self.max_n_msgs, self.max_n_words])',\n",
       " 'self.ss_index_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days, self.max_n_msgs])',\n",
       " '# dropout',\n",
       " 'self.dropout_mel_in = tf.placeholder_with_default(self.dropout_train_mel_in, shape=())',\n",
       " 'self.dropout_mel = tf.placeholder_with_default(self.dropout_train_mel, shape=())',\n",
       " 'self.dropout_ce = tf.placeholder_with_default(self.dropout_train_ce, shape=())',\n",
       " 'self.dropout_vmd_in = tf.placeholder_with_default(self.dropout_train_vmd_in, shape=())',\n",
       " 'self.dropout_vmd = tf.placeholder_with_default(self.dropout_train_vmd, shape=())',\n",
       " 'def _build_embeds(self):',\n",
       " \"with tf.name_scope('embeds'):\",\n",
       " \"with tf.variable_scope('embeds'):\",\n",
       " \"word_table = tf.get_variable('word_table', initializer=self.word_table_init, trainable=False)\",\n",
       " \"self.word_embed = tf.nn.embedding_lookup(word_table, self.word_ph, name='word_embed')\",\n",
       " 'def _create_msg_embed_layer_in(self):',\n",
       " '\"\"\"',\n",
       " 'acquire the inputs for MEL.',\n",
       " 'Input:',\n",
       " 'word_embed: batch_size * max_n_days * max_n_msgs * max_n_words * word_embed_size',\n",
       " 'Output:',\n",
       " 'mel_in: same as word_embed',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('mel_in'):\",\n",
       " \"with tf.variable_scope('mel_in'):\",\n",
       " 'mel_in = self.word_embed',\n",
       " 'if self.use_in_bn:',\n",
       " \"mel_in = neural.bn(mel_in, self.is_training_phase, bn_scope='bn-mel_inputs')\",\n",
       " 'self.mel_in = tf.nn.dropout(mel_in, keep_prob=1-self.dropout_mel_in)',\n",
       " 'def _create_msg_embed_layer(self):',\n",
       " '\"\"\"',\n",
       " 'Input:',\n",
       " 'mel_in: same as word_embed',\n",
       " 'Output:',\n",
       " 'msg_embed: batch_size * max_n_days * max_n_msgs * msg_embed_size',\n",
       " '\"\"\"',\n",
       " 'def _for_one_trading_day(daily_in, daily_ss_index_vec, daily_mask):',\n",
       " '\"\"\"',\n",
       " 'daily_in: max_n_msgs * max_n_words * word_embed_size',\n",
       " '\"\"\"',\n",
       " 'out, _ = tf.nn.bidirectional_dynamic_rnn(mel_cell_f, mel_cell_b, daily_in, daily_mask,',\n",
       " 'mel_init_f, mel_init_b, dtype=tf.float32)',\n",
       " 'out_f, out_b = out',\n",
       " 'ss_indices = tf.reshape(daily_ss_index_vec, [-1, 1])',\n",
       " 'msg_ids = tf.constant(range(0, self.max_n_msgs), dtype=tf.int32, shape=[self.max_n_msgs, 1])  # [0, 1, 2, ...]',\n",
       " 'out_id = tf.concat([msg_ids, ss_indices], axis=1)',\n",
       " '# fw, bw and average',\n",
       " 'mel_h_f, mel_h_b = tf.gather_nd(out_f, out_id), tf.gather_nd(out_b, out_id)',\n",
       " 'msg_embed = (mel_h_f + mel_h_b) / 2',\n",
       " 'return msg_embed',\n",
       " 'def _for_one_sample(sample, sample_ss_index, sample_mask):',\n",
       " 'return neural.iter(size=self.max_n_days, func=_for_one_trading_day,',\n",
       " 'iter_arg=sample, iter_arg2=sample_ss_index, iter_arg3=sample_mask)',\n",
       " 'def _for_one_batch():',\n",
       " 'return neural.iter(size=self.batch_size, func=_for_one_sample,',\n",
       " 'iter_arg=self.mel_in, iter_arg2=self.ss_index_ph, iter_arg3=self.n_words_ph)',\n",
       " \"with tf.name_scope('mel'):\",\n",
       " \"with tf.variable_scope('mel_iter', reuse=tf.AUTO_REUSE):\",\n",
       " \"if self.mel_cell_type == 'ln-lstm':\",\n",
       " 'mel_cell_f = tf.contrib.rnn.LayerNormBasicLSTMCell(self.mel_h_size)',\n",
       " 'mel_cell_b = tf.contrib.rnn.LayerNormBasicLSTMCell(self.mel_h_size)',\n",
       " \"elif self.mel_cell_type == 'gru':\",\n",
       " 'mel_cell_f = tf.contrib.rnn.GRUCell(self.mel_h_size)',\n",
       " 'mel_cell_b = tf.contrib.rnn.GRUCell(self.mel_h_size)',\n",
       " 'else:',\n",
       " 'mel_cell_f = tf.contrib.rnn.BasicRNNCell(self.mel_h_size)',\n",
       " 'mel_cell_b = tf.contrib.rnn.BasicRNNCell(self.mel_h_size)',\n",
       " 'mel_cell_f = tf.contrib.rnn.DropoutWrapper(mel_cell_f, output_keep_prob=1.0-self.dropout_mel)',\n",
       " 'mel_cell_b = tf.contrib.rnn.DropoutWrapper(mel_cell_b, output_keep_prob=1.0-self.dropout_mel)',\n",
       " 'mel_init_f = mel_cell_f.zero_state([self.max_n_msgs], tf.float32)',\n",
       " 'mel_init_b = mel_cell_f.zero_state([self.max_n_msgs], tf.float32)',\n",
       " 'msg_embed_shape = (self.batch_size, self.max_n_days, self.max_n_msgs, self.msg_embed_size)',\n",
       " 'msg_embed = tf.reshape(_for_one_batch(), shape=msg_embed_shape)',\n",
       " \"self.msg_embed = tf.nn.dropout(msg_embed, keep_prob=1-self.dropout_mel, name='msg_embed')\",\n",
       " 'def _create_corpus_embed(self):',\n",
       " '\"\"\"',\n",
       " 'msg_embed: batch_size * max_n_days * max_n_msgs * msg_embed_size',\n",
       " '=> corpus_embed: batch_size * max_n_days * corpus_embed_size',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('corpus_embed'):\",\n",
       " \"with tf.variable_scope('u_t'):\",\n",
       " \"proj_u = self._linear(self.msg_embed, self.msg_embed_size, 'tanh', use_bias=False)\",\n",
       " \"w_u = tf.get_variable('w_u', shape=(self.msg_embed_size, 1), initializer=self.initializer)\",\n",
       " 'u = tf.reduce_mean(tf.tensordot(proj_u, w_u, axes=1), axis=-1)  # batch_size * max_n_days * max_n_msgs',\n",
       " \"mask_msgs = tf.sequence_mask(self.n_msgs_ph, maxlen=self.max_n_msgs, dtype=tf.bool, name='mask_msgs')\",\n",
       " 'ninf = tf.fill(tf.shape(mask_msgs), np.NINF)',\n",
       " 'masked_score = tf.where(mask_msgs, u, ninf)',\n",
       " 'u = neural.softmax(masked_score)  # batch_size * max_n_days * max_n_msgs',\n",
       " 'u = tf.where(tf.is_nan(u), tf.zeros_like(u), u)  # replace nan with 0.0',\n",
       " 'u = tf.expand_dims(u, axis=-2)  # batch_size * max_n_days * 1 * max_n_msgs',\n",
       " 'corpus_embed = tf.matmul(u, self.msg_embed)  # batch_size * max_n_days * 1 * msg_embed_size',\n",
       " 'corpus_embed = tf.reduce_mean(corpus_embed, axis=-2)  # batch_size * max_n_days * msg_embed_size',\n",
       " \"self.corpus_embed = tf.nn.dropout(corpus_embed, keep_prob=1-self.dropout_ce, name='corpus_embed')\",\n",
       " 'def _build_mie(self):',\n",
       " '\"\"\"',\n",
       " 'Create market information encoder.',\n",
       " 'corpus_embed: batch_size * max_n_days * corpus_embed_size',\n",
       " 'price: batch_size * max_n_days * 3',\n",
       " '=> x: batch_size * max_n_days * x_size',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('mie'):\",\n",
       " 'self.price = self.price_ph',\n",
       " 'self.price_size = 3',\n",
       " \"if self.variant_type == 'tech':\",\n",
       " 'self.x = self.price',\n",
       " 'self.x_size = self.price_size',\n",
       " 'else:',\n",
       " 'self._create_msg_embed_layer_in()',\n",
       " 'self._create_msg_embed_layer()',\n",
       " 'self._create_corpus_embed()',\n",
       " \"if self.variant_type == 'fund':\",\n",
       " 'self.x = self.corpus_embed',\n",
       " 'self.x_size = self.corpus_embed_size',\n",
       " 'else:',\n",
       " 'self.x = tf.concat([self.corpus_embed, self.price], axis=2)',\n",
       " 'self.x_size = self.corpus_embed_size + self.price_size',\n",
       " 'def _create_vmd_with_h_rec(self):',\n",
       " \"with tf.name_scope('vmd'):\",\n",
       " \"with tf.variable_scope('vmd_h_rec'):\",\n",
       " 'x = tf.nn.dropout(self.x, keep_prob=1-self.dropout_vmd_in)',\n",
       " 'x = tf.transpose(x, [1, 0, 2])  # max_n_days * batch_size * x_size',\n",
       " 'y_ = tf.transpose(self.y_ph, [1, 0, 2])  # max_n_days * batch_size * y_size',\n",
       " 'self.mask_aux_trading_days = tf.sequence_mask(self.T_ph - 1, self.max_n_days, dtype=tf.bool,',\n",
       " \"name='mask_aux_trading_days')\",\n",
       " 'def _loop_body(t, ta_h_s, ta_z_prior, ta_z_post, ta_kl):',\n",
       " \"with tf.variable_scope('iter_body', reuse=tf.AUTO_REUSE):\",\n",
       " 'def _init():',\n",
       " 'h_s_init = tf.nn.tanh(tf.random_normal(shape=[self.batch_size, self.h_size]))',\n",
       " 'h_z_init = tf.nn.tanh(tf.random_normal(shape=[self.batch_size, self.z_size]))',\n",
       " 'z_init, _ = self._z(arg=h_z_init, is_prior=False)',\n",
       " 'return h_s_init, z_init',\n",
       " 'def _subsequent():',\n",
       " 'h_s_t_1 = tf.reshape(ta_h_s.read(t-1), [self.batch_size, self.h_size])',\n",
       " 'z_t_1 = tf.reshape(ta_z_post.read(t-1), [self.batch_size, self.z_size])',\n",
       " 'return h_s_t_1, z_t_1',\n",
       " 'h_s_t_1, z_t_1 = tf.cond(t >= 1, _subsequent, _init)',\n",
       " 'gate_args = [x[t], h_s_t_1, z_t_1]',\n",
       " \"with tf.variable_scope('gru_r'):\",\n",
       " \"r = self._linear(gate_args, self.h_size, 'sigmoid')\",\n",
       " \"with tf.variable_scope('gru_u'):\",\n",
       " \"u = self._linear(gate_args, self.h_size, 'sigmoid')\",\n",
       " 'h_args = [x[t], tf.multiply(r, h_s_t_1), z_t_1]',\n",
       " \"with tf.variable_scope('gru_h'):\",\n",
       " \"h_tilde = self._linear(h_args, self.h_size, 'tanh')\",\n",
       " 'h_s_t = tf.multiply(1 - u, h_s_t_1) + tf.multiply(u, h_tilde)',\n",
       " \"with tf.variable_scope('h_z_prior'):\",\n",
       " \"h_z_prior_t = self._linear([x[t], h_s_t], self.z_size, 'tanh')\",\n",
       " \"with tf.variable_scope('z_prior'):\",\n",
       " 'z_prior_t, z_prior_t_pdf = self._z(h_z_prior_t, is_prior=True)',\n",
       " \"with tf.variable_scope('h_z_post'):\",\n",
       " \"h_z_post_t = self._linear([x[t], h_s_t, y_[t]], self.z_size, 'tanh')\",\n",
       " \"with tf.variable_scope('z_post'):\",\n",
       " 'z_post_t, z_post_t_pdf = self._z(h_z_post_t, is_prior=False)',\n",
       " 'kl_t = ds.kl_divergence(z_post_t_pdf, z_prior_t_pdf)',\n",
       " '# write',\n",
       " 'ta_h_s = ta_h_s.write(t, h_s_t)',\n",
       " 'ta_z_prior = ta_z_prior.write(t, z_prior_t)  # write: batch_size * z_size',\n",
       " 'ta_z_post = ta_z_post.write(t, z_post_t)  # write: batch_size * z_size',\n",
       " 'ta_kl = ta_kl.write(t, kl_t)  # write: batch_size * 1',\n",
       " 'return t + 1, ta_h_s, ta_z_prior, ta_z_post, ta_kl',\n",
       " 'ta_h_s_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False)',\n",
       " 'ta_z_prior_init = tf.TensorArray(tf.float32, size=self.max_n_days)',\n",
       " 'ta_z_post_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False)',\n",
       " 'ta_kl_init = tf.TensorArray(tf.float32, size=self.max_n_days)',\n",
       " 'loop_init = (0, ta_h_s_init, ta_z_prior_init, ta_z_post_init, ta_kl_init)',\n",
       " 'loop_cond = lambda t, *args: t < self.max_n_days',\n",
       " '_, ta_h_s, ta_z_prior, ta_z_post, ta_kl = tf.while_loop(loop_cond, _loop_body, loop_init)',\n",
       " 'h_s = tf.reshape(ta_h_s.stack(), shape=(self.max_n_days, self.batch_size, self.h_size))',\n",
       " 'z_shape = (self.max_n_days, self.batch_size, self.z_size)',\n",
       " 'z_prior = tf.reshape(ta_z_prior.stack(), shape=z_shape)',\n",
       " 'z_post = tf.reshape(ta_z_post.stack(), shape=z_shape)',\n",
       " 'kl = tf.reshape(ta_kl.stack(), shape=z_shape)',\n",
       " 'x = tf.transpose(x, [1, 0, 2])  # batch_size * max_n_days * x_size',\n",
       " 'h_s = tf.transpose(h_s, [1, 0, 2])  # batch_size * max_n_days * vmd_h_size',\n",
       " 'z_prior = tf.transpose(z_prior, [1, 0, 2])  # batch_size * max_n_days * z_size',\n",
       " 'z_post = tf.transpose(z_post, [1, 0, 2])  # batch_size * max_n_days * z_size',\n",
       " 'self.kl = tf.reduce_sum(tf.transpose(kl, [1, 0, 2]), axis=2)  # batch_size * max_n_days',\n",
       " \"with tf.variable_scope('g'):\",\n",
       " \"self.g = self._linear([x, h_s, z_post], self.g_size, 'tanh', use_bn=False)\",\n",
       " \"with tf.variable_scope('y'):\",\n",
       " \"self.y = self._linear(self.g, self.y_size, 'softmax')\",\n",
       " \"sample_index = tf.reshape(tf.range(self.batch_size), (self.batch_size, 1), name='sample_index')\",\n",
       " 'self.indexed_T = tf.concat([sample_index, tf.reshape(self.T_ph-1, (self.batch_size, 1))], axis=1)',\n",
       " 'def _infer_func():',\n",
       " 'g_T = tf.gather_nd(params=self.g, indices=self.indexed_T)  # batch_size * g_size',\n",
       " 'if not self.daily_att:',\n",
       " 'y_T = tf.gather_nd(params=self.y, indices=self.indexed_T)  # batch_size * y_size',\n",
       " 'return g_T, y_T',\n",
       " 'return g_T',\n",
       " 'def _gen_func():',\n",
       " '# use prior for g',\n",
       " 'z_prior_T = tf.gather_nd(params=z_prior, indices=self.indexed_T)  # batch_size * z_size',\n",
       " 'h_s_T = tf.gather_nd(params=h_s, indices=self.indexed_T)',\n",
       " 'x_T = tf.gather_nd(params=x, indices=self.indexed_T)',\n",
       " \"with tf.variable_scope('g', reuse=tf.AUTO_REUSE):\",\n",
       " \"g_T = self._linear([x_T, h_s_T, z_prior_T], self.g_size, 'tanh', use_bn=False)\",\n",
       " 'if not self.daily_att:',\n",
       " \"with tf.variable_scope('y', reuse=tf.AUTO_REUSE):\",\n",
       " \"y_T = self._linear(g_T, self.y_size, 'softmax')\",\n",
       " 'return g_T, y_T',\n",
       " 'return g_T',\n",
       " 'if not self.daily_att:',\n",
       " 'self.g_T, self.y_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func)',\n",
       " 'else:',\n",
       " 'self.g_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func)',\n",
       " 'def _create_vmd_with_zh_rec(self):',\n",
       " '\"\"\"',\n",
       " 'Create a variational movement decoder.',\n",
       " 'x: batch_size * max_n_days * vmd_in_size',\n",
       " '=> vmd_h: batch_size * max_n_days * vmd_h_size',\n",
       " '=> z: batch_size * max_n_days * vmd_z_size',\n",
       " '=> y: batch_size * max_n_days * 2',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('vmd'):\",\n",
       " \"with tf.variable_scope('vmd_zh_rec', reuse=tf.AUTO_REUSE):\",\n",
       " 'x = tf.nn.dropout(self.x, keep_prob=1-self.dropout_vmd_in)',\n",
       " 'self.mask_aux_trading_days = tf.sequence_mask(self.T_ph - 1, self.max_n_days, dtype=tf.bool,',\n",
       " \"name='mask_aux_trading_days')\",\n",
       " \"if self.vmd_cell_type == 'ln-lstm':\",\n",
       " 'cell = tf.contrib.rnn.LayerNormBasicLSTMCell(self.h_size)',\n",
       " 'else:',\n",
       " 'cell = tf.contrib.rnn.GRUCell(self.h_size)',\n",
       " 'cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-self.dropout_vmd)',\n",
       " 'init_state = None',\n",
       " '# calculate vmd_h, batch_size * max_n_days * vmd_h_size',\n",
       " 'h_s, _ = tf.nn.dynamic_rnn(cell, x, sequence_length=self.T_ph, initial_state=init_state, dtype=tf.float32)',\n",
       " '# forward max_n_days',\n",
       " 'x = tf.transpose(x, [1, 0, 2])  # max_n_days * batch_size * x_size',\n",
       " 'h_s = tf.transpose(h_s, [1, 0, 2])  # max_n_days * batch_size * vmd_h_size',\n",
       " 'y_ = tf.transpose(self.y_ph, [1, 0, 2])  # max_n_days * batch_size * y_size',\n",
       " 'def _loop_body(t, ta_z_prior, ta_z_post, ta_kl):',\n",
       " '\"\"\"',\n",
       " 'iter body. iter over trading days.',\n",
       " '\"\"\"',\n",
       " \"with tf.variable_scope('iter_body', reuse=tf.AUTO_REUSE):\",\n",
       " \"init = lambda: tf.random_normal(shape=[self.batch_size, self.z_size], name='z_post_t_1')\",\n",
       " 'subsequent = lambda: tf.reshape(ta_z_post.read(t-1), [self.batch_size, self.z_size])',\n",
       " 'z_post_t_1 = tf.cond(t >= 1, subsequent, init)',\n",
       " \"with tf.variable_scope('h_z_prior'):\",\n",
       " \"h_z_prior_t = self._linear([x[t], h_s[t], z_post_t_1], self.z_size, 'tanh')\",\n",
       " \"with tf.variable_scope('z_prior'):\",\n",
       " 'z_prior_t, z_prior_t_pdf = self._z(h_z_prior_t, is_prior=True)',\n",
       " \"with tf.variable_scope('h_z_post'):\",\n",
       " \"h_z_post_t = self._linear([x[t], h_s[t], y_[t], z_post_t_1], self.z_size, 'tanh')\",\n",
       " \"with tf.variable_scope('z_post'):\",\n",
       " 'z_post_t, z_post_t_pdf = self._z(h_z_post_t, is_prior=False)',\n",
       " 'kl_t = ds.kl_divergence(z_post_t_pdf, z_prior_t_pdf)  # batch_size * z_size',\n",
       " 'ta_z_prior = ta_z_prior.write(t, z_prior_t)  # write: batch_size * z_size',\n",
       " 'ta_z_post = ta_z_post.write(t, z_post_t)  # write: batch_size * z_size',\n",
       " 'ta_kl = ta_kl.write(t, kl_t)  # write: batch_size * 1',\n",
       " 'return t + 1, ta_z_prior, ta_z_post, ta_kl',\n",
       " '# loop_init',\n",
       " 'ta_z_prior_init = tf.TensorArray(tf.float32, size=self.max_n_days)',\n",
       " 'ta_z_post_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False)',\n",
       " 'ta_kl_init = tf.TensorArray(tf.float32, size=self.max_n_days)',\n",
       " 'loop_init = (0, ta_z_prior_init, ta_z_post_init, ta_kl_init)',\n",
       " 'cond = lambda t, *args: t < self.max_n_days',\n",
       " '_, ta_z_prior, ta_z_post, ta_kl = tf.while_loop(cond, _loop_body, loop_init)',\n",
       " 'z_shape = (self.max_n_days, self.batch_size, self.z_size)',\n",
       " 'z_prior = tf.reshape(ta_z_prior.stack(), shape=z_shape)',\n",
       " 'z_post = tf.reshape(ta_z_post.stack(), shape=z_shape)',\n",
       " 'kl = tf.reshape(ta_kl.stack(), shape=z_shape)',\n",
       " 'h_s = tf.transpose(h_s, [1, 0, 2])  # batch_size * max_n_days * vmd_h_size',\n",
       " 'z_prior = tf.transpose(z_prior, [1, 0, 2])  # batch_size * max_n_days * z_size',\n",
       " 'z_post = tf.transpose(z_post, [1, 0, 2])  # batch_size * max_n_days * z_size',\n",
       " 'self.kl = tf.reduce_sum(tf.transpose(kl, [1, 0, 2]), axis=2)  # batch_size * max_n_days',\n",
       " \"with tf.variable_scope('g'):\",\n",
       " \"self.g = self._linear([h_s, z_post], self.g_size, 'tanh')  # batch_size * max_n_days * g_size\",\n",
       " \"with tf.variable_scope('y'):\",\n",
       " \"self.y = self._linear(self.g, self.y_size, 'softmax')  # batch_size * max_n_days * y_size\",\n",
       " \"sample_index = tf.reshape(tf.range(self.batch_size), (self.batch_size, 1), name='sample_index')\",\n",
       " 'self.indexed_T = tf.concat([sample_index, tf.reshape(self.T_ph-1, (self.batch_size, 1))], axis=1)',\n",
       " 'def _infer_func():',\n",
       " 'g_T = tf.gather_nd(params=self.g, indices=self.indexed_T)  # batch_size * g_size',\n",
       " 'if not self.daily_att:',\n",
       " 'y_T = tf.gather_nd(params=self.y, indices=self.indexed_T)  # batch_size * y_size',\n",
       " 'return g_T, y_T',\n",
       " 'return g_T',\n",
       " 'def _gen_func():',\n",
       " '# use prior for g & y',\n",
       " 'z_prior_T = tf.gather_nd(params=z_prior, indices=self.indexed_T)  # batch_size * z_size',\n",
       " 'h_s_T = tf.gather_nd(params=h_s, indices=self.indexed_T)',\n",
       " \"with tf.variable_scope('g', reuse=tf.AUTO_REUSE):\",\n",
       " \"g_T = self._linear([h_s_T, z_prior_T], self.g_size, 'tanh', use_bn=False)\",\n",
       " 'if not self.daily_att:',\n",
       " \"with tf.variable_scope('y', reuse=tf.AUTO_REUSE):\",\n",
       " \"y_T = self._linear(g_T, self.y_size, 'softmax')\",\n",
       " 'return g_T, y_T',\n",
       " 'return g_T',\n",
       " 'if not self.daily_att:',\n",
       " 'self.g_T, self.y_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func)',\n",
       " 'else:',\n",
       " 'self.g_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func)',\n",
       " 'def _create_discriminative_vmd(self):',\n",
       " '\"\"\"',\n",
       " 'Create a discriminative movement decoder.',\n",
       " 'x: batch_size * max_n_days * vmd_in_size',\n",
       " '=> vmd_h: batch_size * max_n_days * vmd_h_size',\n",
       " '=> z: batch_size * max_n_days * vmd_z_size',\n",
       " '=> y: batch_size * max_n_days * 2',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('vmd'):\",\n",
       " \"with tf.variable_scope('vmd_zh_rec', reuse=tf.AUTO_REUSE):\",\n",
       " 'x = tf.nn.dropout(self.x, keep_prob=1-self.dropout_vmd_in)',\n",
       " 'self.mask_aux_trading_days = tf.sequence_mask(self.T_ph - 1, self.max_n_days, dtype=tf.bool,',\n",
       " \"name='mask_aux_trading_days')\",\n",
       " \"if self.vmd_cell_type == 'ln-lstm':\",\n",
       " 'cell = tf.contrib.rnn.LayerNormBasicLSTMCell(self.h_size)',\n",
       " 'else:',\n",
       " 'cell = tf.contrib.rnn.GRUCell(self.h_size)',\n",
       " 'cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-self.dropout_vmd)',\n",
       " 'init_state = None',\n",
       " 'h_s, _ = tf.nn.dynamic_rnn(cell, x, sequence_length=self.T_ph, initial_state=init_state, dtype=tf.float32)',\n",
       " '# forward max_n_days',\n",
       " 'x = tf.transpose(x, [1, 0, 2])  # max_n_days * batch_size * x_size',\n",
       " 'h_s = tf.transpose(h_s, [1, 0, 2])  # max_n_days * batch_size * vmd_h_size',\n",
       " 'def _loop_body(t, ta_z):',\n",
       " '\"\"\"',\n",
       " 'iter body. iter over trading days.',\n",
       " '\"\"\"',\n",
       " \"with tf.variable_scope('iter_body', reuse=tf.AUTO_REUSE):\",\n",
       " \"init = lambda: tf.random_normal(shape=[self.batch_size, self.z_size], name='z_post_t_1')\",\n",
       " 'subsequent = lambda: tf.reshape(ta_z.read(t-1), [self.batch_size, self.z_size])',\n",
       " 'z_t_1 = tf.cond(t >= 1, subsequent, init)',\n",
       " \"with tf.variable_scope('h_z'):\",\n",
       " \"h_z_t = self._linear([x[t], h_s[t], z_t_1], self.z_size, 'tanh')\",\n",
       " \"with tf.variable_scope('z'):\",\n",
       " \"z_t = self._linear(h_z_t, self.z_size, 'tanh')\",\n",
       " 'ta_z = ta_z.write(t, z_t)  # write: batch_size * z_size',\n",
       " 'return t + 1, ta_z',\n",
       " '# loop_init',\n",
       " 'ta_z_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False)',\n",
       " 'loop_init = (0, ta_z_init)',\n",
       " 'cond = lambda t, *args: t < self.max_n_days',\n",
       " '_, ta_z_init = tf.while_loop(cond, _loop_body, loop_init)',\n",
       " 'z_shape = (self.max_n_days, self.batch_size, self.z_size)',\n",
       " 'z = tf.reshape(ta_z_init.stack(), shape=z_shape)',\n",
       " 'h_s = tf.transpose(h_s, [1, 0, 2])  # batch_size * max_n_days * vmd_h_size',\n",
       " 'z = tf.transpose(z, [1, 0, 2])  # batch_size * max_n_days * z_size',\n",
       " \"with tf.variable_scope('g'):\",\n",
       " \"self.g = self._linear([h_s, z], self.g_size, 'tanh')  # batch_size * max_n_days * g_size\",\n",
       " \"with tf.variable_scope('y'):\",\n",
       " \"self.y = self._linear(self.g, self.y_size, 'softmax')  # batch_size * max_n_days * y_size\",\n",
       " '# get g_T',\n",
       " \"sample_index = tf.reshape(tf.range(self.batch_size), (self.batch_size, 1), name='sample_index')\",\n",
       " 'self.indexed_T = tf.concat([sample_index, tf.reshape(self.T_ph-1, (self.batch_size, 1))], axis=1)',\n",
       " 'self.g_T = tf.gather_nd(params=self.g, indices=self.indexed_T)',\n",
       " 'def _build_vmd(self):',\n",
       " \"if self.variant_type == 'discriminative':\",\n",
       " 'self._create_discriminative_vmd()',\n",
       " 'else:',\n",
       " \"if self.vmd_rec == 'h':\",\n",
       " 'self._create_vmd_with_h_rec()',\n",
       " 'else:',\n",
       " 'self._create_vmd_with_zh_rec()',\n",
       " 'def _build_temporal_att(self):',\n",
       " '\"\"\"',\n",
       " 'g: batch_size * max_n_days * g_size',\n",
       " 'g_T: batch_size * g_size',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('tda'):\",\n",
       " \"with tf.variable_scope('tda'):\",\n",
       " \"with tf.variable_scope('v_i'):\",\n",
       " \"proj_i = self._linear([self.g], self.g_size, 'tanh', use_bias=False)\",\n",
       " \"w_i = tf.get_variable('w_i', shape=(self.g_size, 1), initializer=self.initializer)\",\n",
       " 'v_i = tf.reduce_sum(tf.tensordot(proj_i, w_i, axes=1), axis=-1)  # batch_size * max_n_days',\n",
       " \"with tf.variable_scope('v_d'):\",\n",
       " \"proj_d = self._linear([self.g], self.g_size, 'tanh', use_bias=False)\",\n",
       " 'g_T = tf.expand_dims(self.g_T, axis=-1)  # batch_size * g_size * 1',\n",
       " 'v_d = tf.reduce_sum(tf.matmul(proj_d, g_T), axis=-1)  # batch_size * max_n_days',\n",
       " \"aux_score = tf.multiply(v_i, v_d, name='v_stared')\",\n",
       " 'ninf = tf.fill(tf.shape(aux_score), np.NINF)',\n",
       " 'masked_aux_score = tf.where(self.mask_aux_trading_days, aux_score, ninf)',\n",
       " 'v_stared = tf.nn.softmax(masked_aux_score)',\n",
       " '# v_stared: batch_size * max_n_days',\n",
       " 'self.v_stared = tf.where(tf.is_nan(v_stared), tf.zeros_like(v_stared), v_stared)',\n",
       " \"if self.daily_att == 'y':\",\n",
       " 'context = tf.transpose(self.y, [0, 2, 1])  # batch_size * y_size * max_n_days',\n",
       " 'else:',\n",
       " 'context = tf.transpose(self.g, [0, 2, 1])  # batch_size * g_size * max_n_days',\n",
       " 'v_stared = tf.expand_dims(self.v_stared, -1)  # batch_size * max_n_days * 1',\n",
       " 'att_c = tf.reduce_sum(tf.matmul(context, v_stared), axis=-1)  # batch_size * g_size / y_size',\n",
       " \"with tf.variable_scope('y_T'):\",\n",
       " \"self.y_T = self._linear([att_c, self.g_T], self.y_size, 'softmax')\",\n",
       " 'def _create_generative_ata(self):',\n",
       " '\"\"\"',\n",
       " 'calculate loss.',\n",
       " 'g: batch_size * max_n_days * g_size',\n",
       " 'y: batch_size * max_n_days * y_size',\n",
       " 'kl_loss: batch_size * max_n_days',\n",
       " '=> loss: batch_size',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('ata'):\",\n",
       " \"with tf.variable_scope('ata'):\",\n",
       " 'v_aux = self.alpha * self.v_stared  # batch_size * max_n_days',\n",
       " 'minor = 0.0  # 0.0, 1e-7',\n",
       " 'likelihood_aux = tf.reduce_sum(tf.multiply(self.y_ph, tf.log(self.y + minor)), axis=2)  # batch_size * max_n_days',\n",
       " 'kl_lambda = self._kl_lambda()',\n",
       " 'obj_aux = likelihood_aux - kl_lambda * self.kl  # batch_size * max_n_days',\n",
       " '# deal with T specially, likelihood_T: batch_size, 1',\n",
       " 'self.y_T_ = tf.gather_nd(params=self.y_ph, indices=self.indexed_T)  # batch_size * y_size',\n",
       " 'likelihood_T = tf.reduce_sum(tf.multiply(self.y_T_, tf.log(self.y_T + minor)), axis=1, keep_dims=True)',\n",
       " 'kl_T = tf.reshape(tf.gather_nd(params=self.kl, indices=self.indexed_T), shape=[self.batch_size, 1])',\n",
       " 'obj_T = likelihood_T - kl_lambda * kl_T',\n",
       " 'obj = obj_T + tf.reduce_sum(tf.multiply(obj_aux, v_aux), axis=1, keep_dims=True)  # batch_size * 1',\n",
       " 'self.loss = tf.reduce_mean(-obj, axis=[0, 1])',\n",
       " 'def _create_discriminative_ata(self):',\n",
       " '\"\"\"',\n",
       " 'calculate discriminative loss.',\n",
       " 'g: batch_size * max_n_days * g_size',\n",
       " 'y: batch_size * max_n_days * y_size',\n",
       " '=> loss: batch_size',\n",
       " '\"\"\"',\n",
       " \"with tf.name_scope('ata'):\",\n",
       " \"with tf.variable_scope('ata'):\",\n",
       " 'v_aux = self.alpha * self.v_stared  # batch_size * max_n_days',\n",
       " 'minor = 0.0  # 0.0, 1e-7',\n",
       " 'likelihood_aux = tf.reduce_sum(tf.multiply(self.y_ph, tf.log(self.y + minor)), axis=2)  # batch_size * max_n_days',\n",
       " '# deal with T specially, likelihood_T: batch_size, 1',\n",
       " 'self.y_T_ = tf.gather_nd(params=self.y_ph, indices=self.indexed_T)  # batch_size * y_size',\n",
       " 'likelihood_T = tf.reduce_sum(tf.multiply(self.y_T_, tf.log(self.y_T + minor)), axis=1, keep_dims=True)',\n",
       " 'obj = likelihood_T + tf.reduce_sum(tf.multiply(likelihood_aux, v_aux), axis=1, keep_dims=True)  # batch_size * 1',\n",
       " 'self.loss = tf.reduce_mean(-obj, axis=[0, 1])',\n",
       " 'def _build_ata(self):',\n",
       " \"if self.variant_type == 'discriminative':\",\n",
       " 'self._create_discriminative_ata()',\n",
       " 'else:',\n",
       " 'self._create_generative_ata()',\n",
       " 'def _create_optimizer(self):',\n",
       " \"with tf.name_scope('optimizer'):\",\n",
       " \"if self.opt == 'sgd':\",\n",
       " 'decayed_lr = tf.train.exponential_decay(learning_rate=self.lr, global_step=self.global_step,',\n",
       " 'decay_steps=self.decay_step, decay_rate=self.decay_rate)',\n",
       " 'optimizer = tf.train.MomentumOptimizer(learning_rate=decayed_lr, momentum=self.momentum)',\n",
       " 'else:',\n",
       " 'optimizer = tf.train.AdamOptimizer(self.lr)',\n",
       " 'gradients, variables = zip(*optimizer.compute_gradients(self.loss))',\n",
       " 'gradients, _ = tf.clip_by_global_norm(gradients, self.clip)',\n",
       " 'self.optimize = optimizer.apply_gradients(zip(gradients, variables))',\n",
       " 'self.global_step = tf.assign_add(self.global_step, 1)',\n",
       " 'def assemble_graph(self):',\n",
       " \"logger.info('Start graph assembling...')\",\n",
       " \"with tf.device('/device:GPU:0'):\",\n",
       " 'self._build_placeholders()',\n",
       " 'self._build_embeds()',\n",
       " 'self._build_mie()',\n",
       " 'self._build_vmd()',\n",
       " 'self._build_temporal_att()',\n",
       " 'self._build_ata()',\n",
       " 'self._create_optimizer()',\n",
       " 'def _kl_lambda(self):',\n",
       " 'def _nonzero_kl_lambda():',\n",
       " 'if self.use_constant_kl_lambda:',\n",
       " 'return self.constant_kl_lambda',\n",
       " 'else:',\n",
       " 'return tf.minimum(self.kl_lambda_anneal_rate * global_step, 1.0)',\n",
       " 'global_step = tf.cast(self.global_step, tf.float32)',\n",
       " 'return tf.cond(global_step < self.kl_lambda_start_step, lambda: 0.0, _nonzero_kl_lambda)',\n",
       " 'def _linear(self, args, output_size, activation=None, use_bias=True, use_bn=False):',\n",
       " 'if type(args) not in (list, tuple):',\n",
       " 'args = [args]',\n",
       " 'shape = [a if a else -1 for a in args[0].get_shape().as_list()[:-1]]',\n",
       " 'shape.append(output_size)',\n",
       " 'sizes = [a.get_shape()[-1].value for a in args]',\n",
       " 'total_arg_size = sum(sizes)',\n",
       " 'scope = tf.get_variable_scope()',\n",
       " 'x = args[0] if len(args) == 1 else tf.concat(args, -1)',\n",
       " 'with tf.variable_scope(scope):',\n",
       " \"weight = tf.get_variable('weight', [total_arg_size, output_size], dtype=tf.float32, initializer=self.initializer)\",\n",
       " 'res = tf.tensordot(x, weight, axes=1)',\n",
       " 'if use_bias:',\n",
       " \"bias = tf.get_variable('bias', [output_size], dtype=tf.float32, initializer=self.bias_initializer)\",\n",
       " 'res = tf.nn.bias_add(res, bias)',\n",
       " 'res = tf.reshape(res, shape)',\n",
       " 'if use_bn:',\n",
       " 'res = batch_norm(res, center=True, scale=True, decay=0.99, updates_collections=None,',\n",
       " 'is_training=self.is_training_phase, scope=scope)',\n",
       " \"if activation == 'tanh':\",\n",
       " 'res = tf.nn.tanh(res)',\n",
       " \"elif activation == 'sigmoid':\",\n",
       " 'res = tf.nn.sigmoid(res)',\n",
       " \"elif activation == 'relu':\",\n",
       " 'res = tf.nn.relu(res)',\n",
       " \"elif activation == 'softmax':\",\n",
       " 'res = tf.nn.softmax(res)',\n",
       " 'return res',\n",
       " 'def _z(self, arg, is_prior):',\n",
       " 'mean = self._linear(arg, self.z_size)',\n",
       " 'stddev = self._linear(arg, self.z_size)',\n",
       " 'stddev = tf.sqrt(tf.exp(stddev))',\n",
       " 'epsilon = tf.random_normal(shape=[self.batch_size, self.z_size])',\n",
       " 'z = mean if is_prior else mean + tf.multiply(stddev, epsilon)',\n",
       " 'pdf_z = ds.Normal(loc=mean, scale=stddev)',\n",
       " 'return z, pdf_z']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1530a79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 588\n"
     ]
    }
   ],
   "source": [
    "print(\"Total lines:\",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98254e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!/usr/local/bin/pyt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \".join(data)\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db94549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!/usr/local/bin/python from __future__ import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info(\\'INIT: #stock: {0}, #vocab+1: {1}\\'.format(ss_size, vocab_size)) # model config self.mode = config_model[\\'mode\\'] self.opt = config_model[\\'opt\\'] self.lr = config_model[\\'lr\\'] self.decay_step = config_model[\\'decay_step\\'] self.decay_rate = config_model[\\'decay_rate\\'] self.momentum = config_model[\\'momentum\\'] self.kl_lambda_anneal_rate = config_model[\\'kl_lambda_anneal_rate\\'] self.kl_lambda_start_step = config_model[\\'kl_lambda_start_step\\'] self.use_constant_kl_lambda = config_model[\\'use_constant_kl_lambda\\'] self.constant_kl_lambda = config_model[\\'constant_kl_lambda\\'] self.daily_att = config_model[\\'daily_att\\'] self.alpha = config_model[\\'alpha\\'] self.clip = config_model[\\'clip\\'] self.n_epochs = config_model[\\'n_epochs\\'] self.batch_size_for_name = config_model[\\'batch_size\\'] self.max_n_days = config_model[\\'max_n_days\\'] self.max_n_msgs = config_model[\\'max_n_msgs\\'] self.max_n_words = config_model[\\'max_n_words\\'] self.weight_init = config_model[\\'weight_init\\'] uniform = True if self.weight_init == \\'xavier-uniform\\' else False self.initializer = tf.contrib.layers.xavier_initializer(uniform=uniform) self.bias_initializer = tf.constant_initializer(0.0, dtype=tf.float32) self.word_embed_type = config_model[\\'word_embed_type\\'] self.y_size = config_model[\\'y_size\\'] self.word_embed_size = config_model[\\'word_embed_size\\'] self.stock_embed_size = config_model[\\'stock_embed_size\\'] self.price_embed_size = config_model[\\'word_embed_size\\'] self.mel_cell_type = config_model[\\'mel_cell_type\\'] self.variant_type = config_model[\\'variant_type\\'] self.vmd_cell_type = config_model[\\'vmd_cell_type\\'] self.vmd_rec = config_model[\\'vmd_rec\\'] self.mel_h_size = config_model[\\'mel_h_size\\'] self.msg_embed_size = config_model[\\'mel_h_size\\'] self.corpus_embed_size = config_model[\\'mel_h_size\\'] self.h_size = config_model[\\'h_size\\'] self.z_size = config_model[\\'h_size\\'] self.g_size = config_model[\\'g_size\\'] self.use_in_bn= config_model[\\'use_in_bn\\'] self.use_o_bn = config_model[\\'use_o_bn\\'] self.use_g_bn = config_model[\\'use_g_bn\\'] self.dropout_train_mel_in = config_model[\\'dropout_mel_in\\'] self.dropout_train_mel = config_model[\\'dropout_mel\\'] self.dropout_train_ce = config_model[\\'dropout_ce\\'] self.dropout_train_vmd_in = config_model[\\'dropout_vmd_in\\'] self.dropout_train_vmd = config_model[\\'dropout_vmd\\'] self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\\'global_step\\') # model name name_pattern_max_n = \\'days-{0}.msgs-{1}-words-{2}\\' name_max_n = name_pattern_max_n.format(self.max_n_days, self.max_n_msgs, self.max_n_words) name_pattern_input_type = \\'word_embed-{0}.vmd_in-{1}\\' name_input_type = name_pattern_input_type.format(self.word_embed_type, self.variant_type) name_pattern_key = \\'alpha-{0}.anneal-{1}.rec-{2}\\' name_key = name_pattern_key.format(self.alpha, self.kl_lambda_anneal_rate, self.vmd_rec) name_pattern_train = \\'batch-{0}.opt-{1}.lr-{2}-drop-{3}-cell-{4}\\' name_train = name_pattern_train.format(self.batch_size_for_name, self.opt, self.lr, self.dropout_train_mel_in, self.mel_cell_type) name_tuple = (self.mode, name_max_n, name_input_type, name_key, name_train) self.model_name = \\'_\\'.join(name_tuple) # paths self.tf_graph_path = os.path.join(path_parser.graphs, self.model_name)  # summary self.tf_checkpoints_path = os.path.join(path_parser.checkpoints, self.model_name)  # checkpoints self.tf_checkpoint_file_path = os.path.join(self.tf_checkpoints_path, \\'checkpoint\\')  # for restore self.tf_saver_path = os.path.join(self.tf_checkpoints_path, \\'sess\\')  # for save # verification assert self.opt in (\\'sgd\\', \\'adam\\') assert self.mel_cell_type in (\\'ln-lstm\\', \\'gru\\', \\'basic\\') assert self.vmd_cell_type in (\\'ln-lstm\\', \\'gru\\') assert self.variant_type in (\\'hedge\\', \\'fund\\', \\'tech\\', \\'discriminative\\') assert self.vmd_rec in (\\'zh\\', \\'h\\') assert self.weight_init in (\\'xavier-uniform\\', \\'xavier-normal\\') def _build_placeholders(self): with tf.name_scope(\\'placeholder\\'): self.is_training_phase = tf.placeholder(dtype=tf.bool, shape=()) self.batch_size = tf.placeholder(dtype=tf.int32, shape=()) # init self.word_table_init = tf.placeholder(dtype=tf.float32, shape=[vocab_size, self.word_embed_size]) # model self.stock_ph = tf.placeholder(dtype=tf.int32, shape=[None]) self.T_ph = tf.placeholder(dtype=tf.int32, shape=[None, ]) self.n_words_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days, self.max_n_msgs]) self.n_msgs_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days]) self.y_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.max_n_days, self.y_size])  # 2-d vectorised movement self.mv_percent_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days])  # movement percent self.price_ph = tf.placeholder(dtype=tf.float32, shape=[None, self.max_n_days, 3])  # high, low, close self.word_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days, self.max_n_msgs, self.max_n_words]) self.ss_index_ph = tf.placeholder(dtype=tf.int32, shape=[None, self.max_n_days, self.max_n_msgs]) # dropout self.dropout_mel_in = tf.placeholder_with_default(self.dropout_train_mel_in, shape=()) self.dropout_mel = tf.placeholder_with_default(self.dropout_train_mel, shape=()) self.dropout_ce = tf.placeholder_with_default(self.dropout_train_ce, shape=()) self.dropout_vmd_in = tf.placeholder_with_default(self.dropout_train_vmd_in, shape=()) self.dropout_vmd = tf.placeholder_with_default(self.dropout_train_vmd, shape=()) def _build_embeds(self): with tf.name_scope(\\'embeds\\'): with tf.variable_scope(\\'embeds\\'): word_table = tf.get_variable(\\'word_table\\', initializer=self.word_table_init, trainable=False) self.word_embed = tf.nn.embedding_lookup(word_table, self.word_ph, name=\\'word_embed\\') def _create_msg_embed_layer_in(self): \"\"\" acquire the inputs for MEL. Input: word_embed: batch_size * max_n_days * max_n_msgs * max_n_words * word_embed_size Output: mel_in: same as word_embed \"\"\" with tf.name_scope(\\'mel_in\\'): with tf.variable_scope(\\'mel_in\\'): mel_in = self.word_embed if self.use_in_bn: mel_in = neural.bn(mel_in, self.is_training_phase, bn_scope=\\'bn-mel_inputs\\') self.mel_in = tf.nn.dropout(mel_in, keep_prob=1-self.dropout_mel_in) def _create_msg_embed_layer(self): \"\"\" Input: mel_in: same as word_embed Output: msg_embed: batch_size * max_n_days * max_n_msgs * msg_embed_size \"\"\" def _for_one_trading_day(daily_in, daily_ss_index_vec, daily_mask): \"\"\" daily_in: max_n_msgs * max_n_words * word_embed_size \"\"\" out, _ = tf.nn.bidirectional_dynamic_rnn(mel_cell_f, mel_cell_b, daily_in, daily_mask, mel_init_f, mel_init_b, dtype=tf.float32) out_f, out_b = out ss_indices = tf.reshape(daily_ss_index_vec, [-1, 1]) msg_ids = tf.constant(range(0, self.max_n_msgs), dtype=tf.int32, shape=[self.max_n_msgs, 1])  # [0, 1, 2, ...] out_id = tf.concat([msg_ids, ss_indices], axis=1) # fw, bw and average mel_h_f, mel_h_b = tf.gather_nd(out_f, out_id), tf.gather_nd(out_b, out_id) msg_embed = (mel_h_f + mel_h_b) / 2 return msg_embed def _for_one_sample(sample, sample_ss_index, sample_mask): return neural.iter(size=self.max_n_days, func=_for_one_trading_day, iter_arg=sample, iter_arg2=sample_ss_index, iter_arg3=sample_mask) def _for_one_batch(): return neural.iter(size=self.batch_size, func=_for_one_sample, iter_arg=self.mel_in, iter_arg2=self.ss_index_ph, iter_arg3=self.n_words_ph) with tf.name_scope(\\'mel\\'): with tf.variable_scope(\\'mel_iter\\', reuse=tf.AUTO_REUSE): if self.mel_cell_type == \\'ln-lstm\\': mel_cell_f = tf.contrib.rnn.LayerNormBasicLSTMCell(self.mel_h_size) mel_cell_b = tf.contrib.rnn.LayerNormBasicLSTMCell(self.mel_h_size) elif self.mel_cell_type == \\'gru\\': mel_cell_f = tf.contrib.rnn.GRUCell(self.mel_h_size) mel_cell_b = tf.contrib.rnn.GRUCell(self.mel_h_size) else: mel_cell_f = tf.contrib.rnn.BasicRNNCell(self.mel_h_size) mel_cell_b = tf.contrib.rnn.BasicRNNCell(self.mel_h_size) mel_cell_f = tf.contrib.rnn.DropoutWrapper(mel_cell_f, output_keep_prob=1.0-self.dropout_mel) mel_cell_b = tf.contrib.rnn.DropoutWrapper(mel_cell_b, output_keep_prob=1.0-self.dropout_mel) mel_init_f = mel_cell_f.zero_state([self.max_n_msgs], tf.float32) mel_init_b = mel_cell_f.zero_state([self.max_n_msgs], tf.float32) msg_embed_shape = (self.batch_size, self.max_n_days, self.max_n_msgs, self.msg_embed_size) msg_embed = tf.reshape(_for_one_batch(), shape=msg_embed_shape) self.msg_embed = tf.nn.dropout(msg_embed, keep_prob=1-self.dropout_mel, name=\\'msg_embed\\') def _create_corpus_embed(self): \"\"\" msg_embed: batch_size * max_n_days * max_n_msgs * msg_embed_size => corpus_embed: batch_size * max_n_days * corpus_embed_size \"\"\" with tf.name_scope(\\'corpus_embed\\'): with tf.variable_scope(\\'u_t\\'): proj_u = self._linear(self.msg_embed, self.msg_embed_size, \\'tanh\\', use_bias=False) w_u = tf.get_variable(\\'w_u\\', shape=(self.msg_embed_size, 1), initializer=self.initializer) u = tf.reduce_mean(tf.tensordot(proj_u, w_u, axes=1), axis=-1)  # batch_size * max_n_days * max_n_msgs mask_msgs = tf.sequence_mask(self.n_msgs_ph, maxlen=self.max_n_msgs, dtype=tf.bool, name=\\'mask_msgs\\') ninf = tf.fill(tf.shape(mask_msgs), np.NINF) masked_score = tf.where(mask_msgs, u, ninf) u = neural.softmax(masked_score)  # batch_size * max_n_days * max_n_msgs u = tf.where(tf.is_nan(u), tf.zeros_like(u), u)  # replace nan with 0.0 u = tf.expand_dims(u, axis=-2)  # batch_size * max_n_days * 1 * max_n_msgs corpus_embed = tf.matmul(u, self.msg_embed)  # batch_size * max_n_days * 1 * msg_embed_size corpus_embed = tf.reduce_mean(corpus_embed, axis=-2)  # batch_size * max_n_days * msg_embed_size self.corpus_embed = tf.nn.dropout(corpus_embed, keep_prob=1-self.dropout_ce, name=\\'corpus_embed\\') def _build_mie(self): \"\"\" Create market information encoder. corpus_embed: batch_size * max_n_days * corpus_embed_size price: batch_size * max_n_days * 3 => x: batch_size * max_n_days * x_size \"\"\" with tf.name_scope(\\'mie\\'): self.price = self.price_ph self.price_size = 3 if self.variant_type == \\'tech\\': self.x = self.price self.x_size = self.price_size else: self._create_msg_embed_layer_in() self._create_msg_embed_layer() self._create_corpus_embed() if self.variant_type == \\'fund\\': self.x = self.corpus_embed self.x_size = self.corpus_embed_size else: self.x = tf.concat([self.corpus_embed, self.price], axis=2) self.x_size = self.corpus_embed_size + self.price_size def _create_vmd_with_h_rec(self): with tf.name_scope(\\'vmd\\'): with tf.variable_scope(\\'vmd_h_rec\\'): x = tf.nn.dropout(self.x, keep_prob=1-self.dropout_vmd_in) x = tf.transpose(x, [1, 0, 2])  # max_n_days * batch_size * x_size y_ = tf.transpose(self.y_ph, [1, 0, 2])  # max_n_days * batch_size * y_size self.mask_aux_trading_days = tf.sequence_mask(self.T_ph - 1, self.max_n_days, dtype=tf.bool, name=\\'mask_aux_trading_days\\') def _loop_body(t, ta_h_s, ta_z_prior, ta_z_post, ta_kl): with tf.variable_scope(\\'iter_body\\', reuse=tf.AUTO_REUSE): def _init(): h_s_init = tf.nn.tanh(tf.random_normal(shape=[self.batch_size, self.h_size])) h_z_init = tf.nn.tanh(tf.random_normal(shape=[self.batch_size, self.z_size])) z_init, _ = self._z(arg=h_z_init, is_prior=False) return h_s_init, z_init def _subsequent(): h_s_t_1 = tf.reshape(ta_h_s.read(t-1), [self.batch_size, self.h_size]) z_t_1 = tf.reshape(ta_z_post.read(t-1), [self.batch_size, self.z_size]) return h_s_t_1, z_t_1 h_s_t_1, z_t_1 = tf.cond(t >= 1, _subsequent, _init) gate_args = [x[t], h_s_t_1, z_t_1] with tf.variable_scope(\\'gru_r\\'): r = self._linear(gate_args, self.h_size, \\'sigmoid\\') with tf.variable_scope(\\'gru_u\\'): u = self._linear(gate_args, self.h_size, \\'sigmoid\\') h_args = [x[t], tf.multiply(r, h_s_t_1), z_t_1] with tf.variable_scope(\\'gru_h\\'): h_tilde = self._linear(h_args, self.h_size, \\'tanh\\') h_s_t = tf.multiply(1 - u, h_s_t_1) + tf.multiply(u, h_tilde) with tf.variable_scope(\\'h_z_prior\\'): h_z_prior_t = self._linear([x[t], h_s_t], self.z_size, \\'tanh\\') with tf.variable_scope(\\'z_prior\\'): z_prior_t, z_prior_t_pdf = self._z(h_z_prior_t, is_prior=True) with tf.variable_scope(\\'h_z_post\\'): h_z_post_t = self._linear([x[t], h_s_t, y_[t]], self.z_size, \\'tanh\\') with tf.variable_scope(\\'z_post\\'): z_post_t, z_post_t_pdf = self._z(h_z_post_t, is_prior=False) kl_t = ds.kl_divergence(z_post_t_pdf, z_prior_t_pdf) # write ta_h_s = ta_h_s.write(t, h_s_t) ta_z_prior = ta_z_prior.write(t, z_prior_t)  # write: batch_size * z_size ta_z_post = ta_z_post.write(t, z_post_t)  # write: batch_size * z_size ta_kl = ta_kl.write(t, kl_t)  # write: batch_size * 1 return t + 1, ta_h_s, ta_z_prior, ta_z_post, ta_kl ta_h_s_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False) ta_z_prior_init = tf.TensorArray(tf.float32, size=self.max_n_days) ta_z_post_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False) ta_kl_init = tf.TensorArray(tf.float32, size=self.max_n_days) loop_init = (0, ta_h_s_init, ta_z_prior_init, ta_z_post_init, ta_kl_init) loop_cond = lambda t, *args: t < self.max_n_days _, ta_h_s, ta_z_prior, ta_z_post, ta_kl = tf.while_loop(loop_cond, _loop_body, loop_init) h_s = tf.reshape(ta_h_s.stack(), shape=(self.max_n_days, self.batch_size, self.h_size)) z_shape = (self.max_n_days, self.batch_size, self.z_size) z_prior = tf.reshape(ta_z_prior.stack(), shape=z_shape) z_post = tf.reshape(ta_z_post.stack(), shape=z_shape) kl = tf.reshape(ta_kl.stack(), shape=z_shape) x = tf.transpose(x, [1, 0, 2])  # batch_size * max_n_days * x_size h_s = tf.transpose(h_s, [1, 0, 2])  # batch_size * max_n_days * vmd_h_size z_prior = tf.transpose(z_prior, [1, 0, 2])  # batch_size * max_n_days * z_size z_post = tf.transpose(z_post, [1, 0, 2])  # batch_size * max_n_days * z_size self.kl = tf.reduce_sum(tf.transpose(kl, [1, 0, 2]), axis=2)  # batch_size * max_n_days with tf.variable_scope(\\'g\\'): self.g = self._linear([x, h_s, z_post], self.g_size, \\'tanh\\', use_bn=False) with tf.variable_scope(\\'y\\'): self.y = self._linear(self.g, self.y_size, \\'softmax\\') sample_index = tf.reshape(tf.range(self.batch_size), (self.batch_size, 1), name=\\'sample_index\\') self.indexed_T = tf.concat([sample_index, tf.reshape(self.T_ph-1, (self.batch_size, 1))], axis=1) def _infer_func(): g_T = tf.gather_nd(params=self.g, indices=self.indexed_T)  # batch_size * g_size if not self.daily_att: y_T = tf.gather_nd(params=self.y, indices=self.indexed_T)  # batch_size * y_size return g_T, y_T return g_T def _gen_func(): # use prior for g z_prior_T = tf.gather_nd(params=z_prior, indices=self.indexed_T)  # batch_size * z_size h_s_T = tf.gather_nd(params=h_s, indices=self.indexed_T) x_T = tf.gather_nd(params=x, indices=self.indexed_T) with tf.variable_scope(\\'g\\', reuse=tf.AUTO_REUSE): g_T = self._linear([x_T, h_s_T, z_prior_T], self.g_size, \\'tanh\\', use_bn=False) if not self.daily_att: with tf.variable_scope(\\'y\\', reuse=tf.AUTO_REUSE): y_T = self._linear(g_T, self.y_size, \\'softmax\\') return g_T, y_T return g_T if not self.daily_att: self.g_T, self.y_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func) else: self.g_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func) def _create_vmd_with_zh_rec(self): \"\"\" Create a variational movement decoder. x: batch_size * max_n_days * vmd_in_size => vmd_h: batch_size * max_n_days * vmd_h_size => z: batch_size * max_n_days * vmd_z_size => y: batch_size * max_n_days * 2 \"\"\" with tf.name_scope(\\'vmd\\'): with tf.variable_scope(\\'vmd_zh_rec\\', reuse=tf.AUTO_REUSE): x = tf.nn.dropout(self.x, keep_prob=1-self.dropout_vmd_in) self.mask_aux_trading_days = tf.sequence_mask(self.T_ph - 1, self.max_n_days, dtype=tf.bool, name=\\'mask_aux_trading_days\\') if self.vmd_cell_type == \\'ln-lstm\\': cell = tf.contrib.rnn.LayerNormBasicLSTMCell(self.h_size) else: cell = tf.contrib.rnn.GRUCell(self.h_size) cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-self.dropout_vmd) init_state = None # calculate vmd_h, batch_size * max_n_days * vmd_h_size h_s, _ = tf.nn.dynamic_rnn(cell, x, sequence_length=self.T_ph, initial_state=init_state, dtype=tf.float32) # forward max_n_days x = tf.transpose(x, [1, 0, 2])  # max_n_days * batch_size * x_size h_s = tf.transpose(h_s, [1, 0, 2])  # max_n_days * batch_size * vmd_h_size y_ = tf.transpose(self.y_ph, [1, 0, 2])  # max_n_days * batch_size * y_size def _loop_body(t, ta_z_prior, ta_z_post, ta_kl): \"\"\" iter body. iter over trading days. \"\"\" with tf.variable_scope(\\'iter_body\\', reuse=tf.AUTO_REUSE): init = lambda: tf.random_normal(shape=[self.batch_size, self.z_size], name=\\'z_post_t_1\\') subsequent = lambda: tf.reshape(ta_z_post.read(t-1), [self.batch_size, self.z_size]) z_post_t_1 = tf.cond(t >= 1, subsequent, init) with tf.variable_scope(\\'h_z_prior\\'): h_z_prior_t = self._linear([x[t], h_s[t], z_post_t_1], self.z_size, \\'tanh\\') with tf.variable_scope(\\'z_prior\\'): z_prior_t, z_prior_t_pdf = self._z(h_z_prior_t, is_prior=True) with tf.variable_scope(\\'h_z_post\\'): h_z_post_t = self._linear([x[t], h_s[t], y_[t], z_post_t_1], self.z_size, \\'tanh\\') with tf.variable_scope(\\'z_post\\'): z_post_t, z_post_t_pdf = self._z(h_z_post_t, is_prior=False) kl_t = ds.kl_divergence(z_post_t_pdf, z_prior_t_pdf)  # batch_size * z_size ta_z_prior = ta_z_prior.write(t, z_prior_t)  # write: batch_size * z_size ta_z_post = ta_z_post.write(t, z_post_t)  # write: batch_size * z_size ta_kl = ta_kl.write(t, kl_t)  # write: batch_size * 1 return t + 1, ta_z_prior, ta_z_post, ta_kl # loop_init ta_z_prior_init = tf.TensorArray(tf.float32, size=self.max_n_days) ta_z_post_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False) ta_kl_init = tf.TensorArray(tf.float32, size=self.max_n_days) loop_init = (0, ta_z_prior_init, ta_z_post_init, ta_kl_init) cond = lambda t, *args: t < self.max_n_days _, ta_z_prior, ta_z_post, ta_kl = tf.while_loop(cond, _loop_body, loop_init) z_shape = (self.max_n_days, self.batch_size, self.z_size) z_prior = tf.reshape(ta_z_prior.stack(), shape=z_shape) z_post = tf.reshape(ta_z_post.stack(), shape=z_shape) kl = tf.reshape(ta_kl.stack(), shape=z_shape) h_s = tf.transpose(h_s, [1, 0, 2])  # batch_size * max_n_days * vmd_h_size z_prior = tf.transpose(z_prior, [1, 0, 2])  # batch_size * max_n_days * z_size z_post = tf.transpose(z_post, [1, 0, 2])  # batch_size * max_n_days * z_size self.kl = tf.reduce_sum(tf.transpose(kl, [1, 0, 2]), axis=2)  # batch_size * max_n_days with tf.variable_scope(\\'g\\'): self.g = self._linear([h_s, z_post], self.g_size, \\'tanh\\')  # batch_size * max_n_days * g_size with tf.variable_scope(\\'y\\'): self.y = self._linear(self.g, self.y_size, \\'softmax\\')  # batch_size * max_n_days * y_size sample_index = tf.reshape(tf.range(self.batch_size), (self.batch_size, 1), name=\\'sample_index\\') self.indexed_T = tf.concat([sample_index, tf.reshape(self.T_ph-1, (self.batch_size, 1))], axis=1) def _infer_func(): g_T = tf.gather_nd(params=self.g, indices=self.indexed_T)  # batch_size * g_size if not self.daily_att: y_T = tf.gather_nd(params=self.y, indices=self.indexed_T)  # batch_size * y_size return g_T, y_T return g_T def _gen_func(): # use prior for g & y z_prior_T = tf.gather_nd(params=z_prior, indices=self.indexed_T)  # batch_size * z_size h_s_T = tf.gather_nd(params=h_s, indices=self.indexed_T) with tf.variable_scope(\\'g\\', reuse=tf.AUTO_REUSE): g_T = self._linear([h_s_T, z_prior_T], self.g_size, \\'tanh\\', use_bn=False) if not self.daily_att: with tf.variable_scope(\\'y\\', reuse=tf.AUTO_REUSE): y_T = self._linear(g_T, self.y_size, \\'softmax\\') return g_T, y_T return g_T if not self.daily_att: self.g_T, self.y_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func) else: self.g_T = tf.cond(tf.equal(self.is_training_phase, True), _infer_func, _gen_func) def _create_discriminative_vmd(self): \"\"\" Create a discriminative movement decoder. x: batch_size * max_n_days * vmd_in_size => vmd_h: batch_size * max_n_days * vmd_h_size => z: batch_size * max_n_days * vmd_z_size => y: batch_size * max_n_days * 2 \"\"\" with tf.name_scope(\\'vmd\\'): with tf.variable_scope(\\'vmd_zh_rec\\', reuse=tf.AUTO_REUSE): x = tf.nn.dropout(self.x, keep_prob=1-self.dropout_vmd_in) self.mask_aux_trading_days = tf.sequence_mask(self.T_ph - 1, self.max_n_days, dtype=tf.bool, name=\\'mask_aux_trading_days\\') if self.vmd_cell_type == \\'ln-lstm\\': cell = tf.contrib.rnn.LayerNormBasicLSTMCell(self.h_size) else: cell = tf.contrib.rnn.GRUCell(self.h_size) cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-self.dropout_vmd) init_state = None h_s, _ = tf.nn.dynamic_rnn(cell, x, sequence_length=self.T_ph, initial_state=init_state, dtype=tf.float32) # forward max_n_days x = tf.transpose(x, [1, 0, 2])  # max_n_days * batch_size * x_size h_s = tf.transpose(h_s, [1, 0, 2])  # max_n_days * batch_size * vmd_h_size def _loop_body(t, ta_z): \"\"\" iter body. iter over trading days. \"\"\" with tf.variable_scope(\\'iter_body\\', reuse=tf.AUTO_REUSE): init = lambda: tf.random_normal(shape=[self.batch_size, self.z_size], name=\\'z_post_t_1\\') subsequent = lambda: tf.reshape(ta_z.read(t-1), [self.batch_size, self.z_size]) z_t_1 = tf.cond(t >= 1, subsequent, init) with tf.variable_scope(\\'h_z\\'): h_z_t = self._linear([x[t], h_s[t], z_t_1], self.z_size, \\'tanh\\') with tf.variable_scope(\\'z\\'): z_t = self._linear(h_z_t, self.z_size, \\'tanh\\') ta_z = ta_z.write(t, z_t)  # write: batch_size * z_size return t + 1, ta_z # loop_init ta_z_init = tf.TensorArray(tf.float32, size=self.max_n_days, clear_after_read=False) loop_init = (0, ta_z_init) cond = lambda t, *args: t < self.max_n_days _, ta_z_init = tf.while_loop(cond, _loop_body, loop_init) z_shape = (self.max_n_days, self.batch_size, self.z_size) z = tf.reshape(ta_z_init.stack(), shape=z_shape) h_s = tf.transpose(h_s, [1, 0, 2])  # batch_size * max_n_days * vmd_h_size z = tf.transpose(z, [1, 0, 2])  # batch_size * max_n_days * z_size with tf.variable_scope(\\'g\\'): self.g = self._linear([h_s, z], self.g_size, \\'tanh\\')  # batch_size * max_n_days * g_size with tf.variable_scope(\\'y\\'): self.y = self._linear(self.g, self.y_size, \\'softmax\\')  # batch_size * max_n_days * y_size # get g_T sample_index = tf.reshape(tf.range(self.batch_size), (self.batch_size, 1), name=\\'sample_index\\') self.indexed_T = tf.concat([sample_index, tf.reshape(self.T_ph-1, (self.batch_size, 1))], axis=1) self.g_T = tf.gather_nd(params=self.g, indices=self.indexed_T) def _build_vmd(self): if self.variant_type == \\'discriminative\\': self._create_discriminative_vmd() else: if self.vmd_rec == \\'h\\': self._create_vmd_with_h_rec() else: self._create_vmd_with_zh_rec() def _build_temporal_att(self): \"\"\" g: batch_size * max_n_days * g_size g_T: batch_size * g_size \"\"\" with tf.name_scope(\\'tda\\'): with tf.variable_scope(\\'tda\\'): with tf.variable_scope(\\'v_i\\'): proj_i = self._linear([self.g], self.g_size, \\'tanh\\', use_bias=False) w_i = tf.get_variable(\\'w_i\\', shape=(self.g_size, 1), initializer=self.initializer) v_i = tf.reduce_sum(tf.tensordot(proj_i, w_i, axes=1), axis=-1)  # batch_size * max_n_days with tf.variable_scope(\\'v_d\\'): proj_d = self._linear([self.g], self.g_size, \\'tanh\\', use_bias=False) g_T = tf.expand_dims(self.g_T, axis=-1)  # batch_size * g_size * 1 v_d = tf.reduce_sum(tf.matmul(proj_d, g_T), axis=-1)  # batch_size * max_n_days aux_score = tf.multiply(v_i, v_d, name=\\'v_stared\\') ninf = tf.fill(tf.shape(aux_score), np.NINF) masked_aux_score = tf.where(self.mask_aux_trading_days, aux_score, ninf) v_stared = tf.nn.softmax(masked_aux_score) # v_stared: batch_size * max_n_days self.v_stared = tf.where(tf.is_nan(v_stared), tf.zeros_like(v_stared), v_stared) if self.daily_att == \\'y\\': context = tf.transpose(self.y, [0, 2, 1])  # batch_size * y_size * max_n_days else: context = tf.transpose(self.g, [0, 2, 1])  # batch_size * g_size * max_n_days v_stared = tf.expand_dims(self.v_stared, -1)  # batch_size * max_n_days * 1 att_c = tf.reduce_sum(tf.matmul(context, v_stared), axis=-1)  # batch_size * g_size / y_size with tf.variable_scope(\\'y_T\\'): self.y_T = self._linear([att_c, self.g_T], self.y_size, \\'softmax\\') def _create_generative_ata(self): \"\"\" calculate loss. g: batch_size * max_n_days * g_size y: batch_size * max_n_days * y_size kl_loss: batch_size * max_n_days => loss: batch_size \"\"\" with tf.name_scope(\\'ata\\'): with tf.variable_scope(\\'ata\\'): v_aux = self.alpha * self.v_stared  # batch_size * max_n_days minor = 0.0  # 0.0, 1e-7 likelihood_aux = tf.reduce_sum(tf.multiply(self.y_ph, tf.log(self.y + minor)), axis=2)  # batch_size * max_n_days kl_lambda = self._kl_lambda() obj_aux = likelihood_aux - kl_lambda * self.kl  # batch_size * max_n_days # deal with T specially, likelihood_T: batch_size, 1 self.y_T_ = tf.gather_nd(params=self.y_ph, indices=self.indexed_T)  # batch_size * y_size likelihood_T = tf.reduce_sum(tf.multiply(self.y_T_, tf.log(self.y_T + minor)), axis=1, keep_dims=True) kl_T = tf.reshape(tf.gather_nd(params=self.kl, indices=self.indexed_T), shape=[self.batch_size, 1]) obj_T = likelihood_T - kl_lambda * kl_T obj = obj_T + tf.reduce_sum(tf.multiply(obj_aux, v_aux), axis=1, keep_dims=True)  # batch_size * 1 self.loss = tf.reduce_mean(-obj, axis=[0, 1]) def _create_discriminative_ata(self): \"\"\" calculate discriminative loss. g: batch_size * max_n_days * g_size y: batch_size * max_n_days * y_size => loss: batch_size \"\"\" with tf.name_scope(\\'ata\\'): with tf.variable_scope(\\'ata\\'): v_aux = self.alpha * self.v_stared  # batch_size * max_n_days minor = 0.0  # 0.0, 1e-7 likelihood_aux = tf.reduce_sum(tf.multiply(self.y_ph, tf.log(self.y + minor)), axis=2)  # batch_size * max_n_days # deal with T specially, likelihood_T: batch_size, 1 self.y_T_ = tf.gather_nd(params=self.y_ph, indices=self.indexed_T)  # batch_size * y_size likelihood_T = tf.reduce_sum(tf.multiply(self.y_T_, tf.log(self.y_T + minor)), axis=1, keep_dims=True) obj = likelihood_T + tf.reduce_sum(tf.multiply(likelihood_aux, v_aux), axis=1, keep_dims=True)  # batch_size * 1 self.loss = tf.reduce_mean(-obj, axis=[0, 1]) def _build_ata(self): if self.variant_type == \\'discriminative\\': self._create_discriminative_ata() else: self._create_generative_ata() def _create_optimizer(self): with tf.name_scope(\\'optimizer\\'): if self.opt == \\'sgd\\': decayed_lr = tf.train.exponential_decay(learning_rate=self.lr, global_step=self.global_step, decay_steps=self.decay_step, decay_rate=self.decay_rate) optimizer = tf.train.MomentumOptimizer(learning_rate=decayed_lr, momentum=self.momentum) else: optimizer = tf.train.AdamOptimizer(self.lr) gradients, variables = zip(*optimizer.compute_gradients(self.loss)) gradients, _ = tf.clip_by_global_norm(gradients, self.clip) self.optimize = optimizer.apply_gradients(zip(gradients, variables)) self.global_step = tf.assign_add(self.global_step, 1) def assemble_graph(self): logger.info(\\'Start graph assembling...\\') with tf.device(\\'/device:GPU:0\\'): self._build_placeholders() self._build_embeds() self._build_mie() self._build_vmd() self._build_temporal_att() self._build_ata() self._create_optimizer() def _kl_lambda(self): def _nonzero_kl_lambda(): if self.use_constant_kl_lambda: return self.constant_kl_lambda else: return tf.minimum(self.kl_lambda_anneal_rate * global_step, 1.0) global_step = tf.cast(self.global_step, tf.float32) return tf.cond(global_step < self.kl_lambda_start_step, lambda: 0.0, _nonzero_kl_lambda) def _linear(self, args, output_size, activation=None, use_bias=True, use_bn=False): if type(args) not in (list, tuple): args = [args] shape = [a if a else -1 for a in args[0].get_shape().as_list()[:-1]] shape.append(output_size) sizes = [a.get_shape()[-1].value for a in args] total_arg_size = sum(sizes) scope = tf.get_variable_scope() x = args[0] if len(args) == 1 else tf.concat(args, -1) with tf.variable_scope(scope): weight = tf.get_variable(\\'weight\\', [total_arg_size, output_size], dtype=tf.float32, initializer=self.initializer) res = tf.tensordot(x, weight, axes=1) if use_bias: bias = tf.get_variable(\\'bias\\', [output_size], dtype=tf.float32, initializer=self.bias_initializer) res = tf.nn.bias_add(res, bias) res = tf.reshape(res, shape) if use_bn: res = batch_norm(res, center=True, scale=True, decay=0.99, updates_collections=None, is_training=self.is_training_phase, scope=scope) if activation == \\'tanh\\': res = tf.nn.tanh(res) elif activation == \\'sigmoid\\': res = tf.nn.sigmoid(res) elif activation == \\'relu\\': res = tf.nn.relu(res) elif activation == \\'softmax\\': res = tf.nn.softmax(res) return res def _z(self, arg, is_prior): mean = self._linear(arg, self.z_size) stddev = self._linear(arg, self.z_size) stddev = tf.sqrt(tf.exp(stddev)) epsilon = tf.random_normal(shape=[self.batch_size, self.z_size]) z = mean if is_prior else mean + tf.multiply(stddev, epsilon) pdf_z = ds.Normal(loc=mean, scale=stddev) return z, pdf_z'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f396f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(doc):\n",
    "#     tokens = doc.split()\n",
    "#     table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "#     tokens = [w.translate(table) for w in tokens]\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#     tokens = [word.lower() for word in tokens]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88f3c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16821fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6adf0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#!/usr/local/bin/python',\n",
       " 'from',\n",
       " '__future__',\n",
       " 'import',\n",
       " 'print_function',\n",
       " 'import',\n",
       " 'os',\n",
       " 'import',\n",
       " 'tensorflow',\n",
       " 'as',\n",
       " 'tf',\n",
       " 'import',\n",
       " 'numpy',\n",
       " 'as',\n",
       " 'np',\n",
       " 'import',\n",
       " 'neural',\n",
       " 'as',\n",
       " 'neural',\n",
       " 'import',\n",
       " 'tensorflow.contrib.distributions',\n",
       " 'as',\n",
       " 'ds',\n",
       " 'from',\n",
       " 'tensorflow.contrib.layers',\n",
       " 'import',\n",
       " 'batch_norm',\n",
       " 'from',\n",
       " 'ConfigLoader',\n",
       " 'import',\n",
       " 'logger,',\n",
       " 'ss_size,',\n",
       " 'vocab_size,',\n",
       " 'config_model,',\n",
       " 'path_parser',\n",
       " 'class',\n",
       " 'Model:',\n",
       " 'def',\n",
       " '__init__(self):',\n",
       " \"logger.info('INIT:\",\n",
       " '#stock:',\n",
       " '{0},',\n",
       " '#vocab+1:',\n",
       " \"{1}'.format(ss_size,\",\n",
       " 'vocab_size))',\n",
       " '#',\n",
       " 'model',\n",
       " 'config',\n",
       " 'self.mode',\n",
       " '=',\n",
       " \"config_model['mode']\",\n",
       " 'self.opt',\n",
       " '=',\n",
       " \"config_model['opt']\",\n",
       " 'self.lr',\n",
       " '=',\n",
       " \"config_model['lr']\",\n",
       " 'self.decay_step',\n",
       " '=',\n",
       " \"config_model['decay_step']\",\n",
       " 'self.decay_rate',\n",
       " '=',\n",
       " \"config_model['decay_rate']\",\n",
       " 'self.momentum',\n",
       " '=',\n",
       " \"config_model['momentum']\",\n",
       " 'self.kl_lambda_anneal_rate',\n",
       " '=',\n",
       " \"config_model['kl_lambda_anneal_rate']\",\n",
       " 'self.kl_lambda_start_step',\n",
       " '=',\n",
       " \"config_model['kl_lambda_start_step']\",\n",
       " 'self.use_constant_kl_lambda',\n",
       " '=',\n",
       " \"config_model['use_constant_kl_lambda']\",\n",
       " 'self.constant_kl_lambda',\n",
       " '=',\n",
       " \"config_model['constant_kl_lambda']\",\n",
       " 'self.daily_att',\n",
       " '=',\n",
       " \"config_model['daily_att']\",\n",
       " 'self.alpha',\n",
       " '=',\n",
       " \"config_model['alpha']\",\n",
       " 'self.clip',\n",
       " '=',\n",
       " \"config_model['clip']\",\n",
       " 'self.n_epochs',\n",
       " '=',\n",
       " \"config_model['n_epochs']\",\n",
       " 'self.batch_size_for_name',\n",
       " '=',\n",
       " \"config_model['batch_size']\",\n",
       " 'self.max_n_days',\n",
       " '=',\n",
       " \"config_model['max_n_days']\",\n",
       " 'self.max_n_msgs',\n",
       " '=',\n",
       " \"config_model['max_n_msgs']\",\n",
       " 'self.max_n_words',\n",
       " '=',\n",
       " \"config_model['max_n_words']\",\n",
       " 'self.weight_init',\n",
       " '=',\n",
       " \"config_model['weight_init']\",\n",
       " 'uniform',\n",
       " '=',\n",
       " 'True',\n",
       " 'if',\n",
       " 'self.weight_init',\n",
       " '==',\n",
       " \"'xavier-uniform'\",\n",
       " 'else',\n",
       " 'False',\n",
       " 'self.initializer',\n",
       " '=',\n",
       " 'tf.contrib.layers.xavier_initializer(uniform=uniform)',\n",
       " 'self.bias_initializer',\n",
       " '=',\n",
       " 'tf.constant_initializer(0.0,',\n",
       " 'dtype=tf.float32)',\n",
       " 'self.word_embed_type',\n",
       " '=',\n",
       " \"config_model['word_embed_type']\",\n",
       " 'self.y_size',\n",
       " '=',\n",
       " \"config_model['y_size']\",\n",
       " 'self.word_embed_size',\n",
       " '=',\n",
       " \"config_model['word_embed_size']\",\n",
       " 'self.stock_embed_size',\n",
       " '=',\n",
       " \"config_model['stock_embed_size']\",\n",
       " 'self.price_embed_size',\n",
       " '=',\n",
       " \"config_model['word_embed_size']\",\n",
       " 'self.mel_cell_type',\n",
       " '=',\n",
       " \"config_model['mel_cell_type']\",\n",
       " 'self.variant_type',\n",
       " '=',\n",
       " \"config_model['variant_type']\",\n",
       " 'self.vmd_cell_type',\n",
       " '=',\n",
       " \"config_model['vmd_cell_type']\",\n",
       " 'self.vmd_rec',\n",
       " '=',\n",
       " \"config_model['vmd_rec']\",\n",
       " 'self.mel_h_size',\n",
       " '=',\n",
       " \"config_model['mel_h_size']\",\n",
       " 'self.msg_embed_size',\n",
       " '=',\n",
       " \"config_model['mel_h_size']\",\n",
       " 'self.corpus_embed_size',\n",
       " '=',\n",
       " \"config_model['mel_h_size']\",\n",
       " 'self.h_size',\n",
       " '=',\n",
       " \"config_model['h_size']\",\n",
       " 'self.z_size',\n",
       " '=',\n",
       " \"config_model['h_size']\",\n",
       " 'self.g_size',\n",
       " '=',\n",
       " \"config_model['g_size']\",\n",
       " 'self.use_in_bn=',\n",
       " \"config_model['use_in_bn']\",\n",
       " 'self.use_o_bn',\n",
       " '=',\n",
       " \"config_model['use_o_bn']\",\n",
       " 'self.use_g_bn',\n",
       " '=',\n",
       " \"config_model['use_g_bn']\",\n",
       " 'self.dropout_train_mel_in',\n",
       " '=',\n",
       " \"config_model['dropout_mel_in']\",\n",
       " 'self.dropout_train_mel',\n",
       " '=',\n",
       " \"config_model['dropout_mel']\",\n",
       " 'self.dropout_train_ce',\n",
       " '=',\n",
       " \"config_model['dropout_ce']\",\n",
       " 'self.dropout_train_vmd_in',\n",
       " '=',\n",
       " \"config_model['dropout_vmd_in']\",\n",
       " 'self.dropout_train_vmd',\n",
       " '=',\n",
       " \"config_model['dropout_vmd']\",\n",
       " 'self.global_step',\n",
       " '=',\n",
       " 'tf.Variable(0,',\n",
       " 'dtype=tf.int32,',\n",
       " 'trainable=False,',\n",
       " \"name='global_step')\",\n",
       " '#',\n",
       " 'model',\n",
       " 'name',\n",
       " 'name_pattern_max_n',\n",
       " '=',\n",
       " \"'days-{0}.msgs-{1}-words-{2}'\",\n",
       " 'name_max_n',\n",
       " '=',\n",
       " 'name_pattern_max_n.format(self.max_n_days,',\n",
       " 'self.max_n_msgs,',\n",
       " 'self.max_n_words)',\n",
       " 'name_pattern_input_type',\n",
       " '=',\n",
       " \"'word_embed-{0}.vmd_in-{1}'\",\n",
       " 'name_input_type',\n",
       " '=',\n",
       " 'name_pattern_input_type.format(self.word_embed_type,',\n",
       " 'self.variant_type)',\n",
       " 'name_pattern_key',\n",
       " '=',\n",
       " \"'alpha-{0}.anneal-{1}.rec-{2}'\",\n",
       " 'name_key',\n",
       " '=',\n",
       " 'name_pattern_key.format(self.alpha,',\n",
       " 'self.kl_lambda_anneal_rate,',\n",
       " 'self.vmd_rec)',\n",
       " 'name_pattern_train',\n",
       " '=',\n",
       " \"'batch-{0}.opt-{1}.lr-{2}-drop-{3}-cell-{4}'\",\n",
       " 'name_train',\n",
       " '=',\n",
       " 'name_pattern_train.format(self.batch_size_for_name,',\n",
       " 'self.opt,',\n",
       " 'self.lr,',\n",
       " 'self.dropout_train_mel_in,',\n",
       " 'self.mel_cell_type)',\n",
       " 'name_tuple',\n",
       " '=',\n",
       " '(self.mode,',\n",
       " 'name_max_n,',\n",
       " 'name_input_type,',\n",
       " 'name_key,',\n",
       " 'name_train)',\n",
       " 'self.model_name',\n",
       " '=',\n",
       " \"'_'.join(name_tuple)\",\n",
       " '#',\n",
       " 'paths',\n",
       " 'self.tf_graph_path',\n",
       " '=',\n",
       " 'os.path.join(path_parser.graphs,',\n",
       " 'self.model_name)',\n",
       " '#',\n",
       " 'summary',\n",
       " 'self.tf_checkpoints_path',\n",
       " '=',\n",
       " 'os.path.join(path_parser.checkpoints,',\n",
       " 'self.model_name)',\n",
       " '#',\n",
       " 'checkpoints',\n",
       " 'self.tf_checkpoint_file_path',\n",
       " '=',\n",
       " 'os.path.join(self.tf_checkpoints_path,',\n",
       " \"'checkpoint')\",\n",
       " '#',\n",
       " 'for',\n",
       " 'restore',\n",
       " 'self.tf_saver_path',\n",
       " '=',\n",
       " 'os.path.join(self.tf_checkpoints_path,',\n",
       " \"'sess')\",\n",
       " '#',\n",
       " 'for',\n",
       " 'save',\n",
       " '#',\n",
       " 'verification',\n",
       " 'assert',\n",
       " 'self.opt',\n",
       " 'in',\n",
       " \"('sgd',\",\n",
       " \"'adam')\",\n",
       " 'assert',\n",
       " 'self.mel_cell_type',\n",
       " 'in',\n",
       " \"('ln-lstm',\",\n",
       " \"'gru',\",\n",
       " \"'basic')\",\n",
       " 'assert',\n",
       " 'self.vmd_cell_type',\n",
       " 'in',\n",
       " \"('ln-lstm',\",\n",
       " \"'gru')\",\n",
       " 'assert',\n",
       " 'self.variant_type',\n",
       " 'in',\n",
       " \"('hedge',\",\n",
       " \"'fund',\",\n",
       " \"'tech',\",\n",
       " \"'discriminative')\",\n",
       " 'assert',\n",
       " 'self.vmd_rec',\n",
       " 'in',\n",
       " \"('zh',\",\n",
       " \"'h')\",\n",
       " 'assert',\n",
       " 'self.weight_init',\n",
       " 'in',\n",
       " \"('xavier-uniform',\",\n",
       " \"'xavier-normal')\",\n",
       " 'def',\n",
       " '_build_placeholders(self):',\n",
       " 'with',\n",
       " \"tf.name_scope('placeholder'):\",\n",
       " 'self.is_training_phase',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.bool,',\n",
       " 'shape=())',\n",
       " 'self.batch_size',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=())',\n",
       " '#',\n",
       " 'init',\n",
       " 'self.word_table_init',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.float32,',\n",
       " 'shape=[vocab_size,',\n",
       " 'self.word_embed_size])',\n",
       " '#',\n",
       " 'model',\n",
       " 'self.stock_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=[None])',\n",
       " 'self.T_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=[None,',\n",
       " '])',\n",
       " 'self.n_words_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=[None,',\n",
       " 'self.max_n_days,',\n",
       " 'self.max_n_msgs])',\n",
       " 'self.n_msgs_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=[None,',\n",
       " 'self.max_n_days])',\n",
       " 'self.y_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.float32,',\n",
       " 'shape=[None,',\n",
       " 'self.max_n_days,',\n",
       " 'self.y_size])',\n",
       " '#',\n",
       " '2-d',\n",
       " 'vectorised',\n",
       " 'movement',\n",
       " 'self.mv_percent_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=[None,',\n",
       " 'self.max_n_days])',\n",
       " '#',\n",
       " 'movement',\n",
       " 'percent',\n",
       " 'self.price_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.float32,',\n",
       " 'shape=[None,',\n",
       " 'self.max_n_days,',\n",
       " '3])',\n",
       " '#',\n",
       " 'high,',\n",
       " 'low,',\n",
       " 'close',\n",
       " 'self.word_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=[None,',\n",
       " 'self.max_n_days,',\n",
       " 'self.max_n_msgs,',\n",
       " 'self.max_n_words])',\n",
       " 'self.ss_index_ph',\n",
       " '=',\n",
       " 'tf.placeholder(dtype=tf.int32,',\n",
       " 'shape=[None,',\n",
       " 'self.max_n_days,',\n",
       " 'self.max_n_msgs])',\n",
       " '#',\n",
       " 'dropout',\n",
       " 'self.dropout_mel_in',\n",
       " '=',\n",
       " 'tf.placeholder_with_default(self.dropout_train_mel_in,',\n",
       " 'shape=())',\n",
       " 'self.dropout_mel',\n",
       " '=',\n",
       " 'tf.placeholder_with_default(self.dropout_train_mel,',\n",
       " 'shape=())',\n",
       " 'self.dropout_ce',\n",
       " '=',\n",
       " 'tf.placeholder_with_default(self.dropout_train_ce,',\n",
       " 'shape=())',\n",
       " 'self.dropout_vmd_in',\n",
       " '=',\n",
       " 'tf.placeholder_with_default(self.dropout_train_vmd_in,',\n",
       " 'shape=())',\n",
       " 'self.dropout_vmd',\n",
       " '=',\n",
       " 'tf.placeholder_with_default(self.dropout_train_vmd,',\n",
       " 'shape=())',\n",
       " 'def',\n",
       " '_build_embeds(self):',\n",
       " 'with',\n",
       " \"tf.name_scope('embeds'):\",\n",
       " 'with',\n",
       " \"tf.variable_scope('embeds'):\",\n",
       " 'word_table',\n",
       " '=',\n",
       " \"tf.get_variable('word_table',\",\n",
       " 'initializer=self.word_table_init,',\n",
       " 'trainable=False)',\n",
       " 'self.word_embed',\n",
       " '=',\n",
       " 'tf.nn.embedding_lookup(word_table,',\n",
       " 'self.word_ph,',\n",
       " \"name='word_embed')\",\n",
       " 'def',\n",
       " '_create_msg_embed_layer_in(self):',\n",
       " '\"\"\"',\n",
       " 'acquire',\n",
       " 'the',\n",
       " 'inputs',\n",
       " 'for',\n",
       " 'MEL.',\n",
       " 'Input:',\n",
       " 'word_embed:',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'max_n_msgs',\n",
       " '*',\n",
       " 'max_n_words',\n",
       " '*',\n",
       " 'word_embed_size',\n",
       " 'Output:',\n",
       " 'mel_in:',\n",
       " 'same',\n",
       " 'as',\n",
       " 'word_embed',\n",
       " '\"\"\"',\n",
       " 'with',\n",
       " \"tf.name_scope('mel_in'):\",\n",
       " 'with',\n",
       " \"tf.variable_scope('mel_in'):\",\n",
       " 'mel_in',\n",
       " '=',\n",
       " 'self.word_embed',\n",
       " 'if',\n",
       " 'self.use_in_bn:',\n",
       " 'mel_in',\n",
       " '=',\n",
       " 'neural.bn(mel_in,',\n",
       " 'self.is_training_phase,',\n",
       " \"bn_scope='bn-mel_inputs')\",\n",
       " 'self.mel_in',\n",
       " '=',\n",
       " 'tf.nn.dropout(mel_in,',\n",
       " 'keep_prob=1-self.dropout_mel_in)',\n",
       " 'def',\n",
       " '_create_msg_embed_layer(self):',\n",
       " '\"\"\"',\n",
       " 'Input:',\n",
       " 'mel_in:',\n",
       " 'same',\n",
       " 'as',\n",
       " 'word_embed',\n",
       " 'Output:',\n",
       " 'msg_embed:',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'max_n_msgs',\n",
       " '*',\n",
       " 'msg_embed_size',\n",
       " '\"\"\"',\n",
       " 'def',\n",
       " '_for_one_trading_day(daily_in,',\n",
       " 'daily_ss_index_vec,',\n",
       " 'daily_mask):',\n",
       " '\"\"\"',\n",
       " 'daily_in:',\n",
       " 'max_n_msgs',\n",
       " '*',\n",
       " 'max_n_words',\n",
       " '*',\n",
       " 'word_embed_size',\n",
       " '\"\"\"',\n",
       " 'out,',\n",
       " '_',\n",
       " '=',\n",
       " 'tf.nn.bidirectional_dynamic_rnn(mel_cell_f,',\n",
       " 'mel_cell_b,',\n",
       " 'daily_in,',\n",
       " 'daily_mask,',\n",
       " 'mel_init_f,',\n",
       " 'mel_init_b,',\n",
       " 'dtype=tf.float32)',\n",
       " 'out_f,',\n",
       " 'out_b',\n",
       " '=',\n",
       " 'out',\n",
       " 'ss_indices',\n",
       " '=',\n",
       " 'tf.reshape(daily_ss_index_vec,',\n",
       " '[-1,',\n",
       " '1])',\n",
       " 'msg_ids',\n",
       " '=',\n",
       " 'tf.constant(range(0,',\n",
       " 'self.max_n_msgs),',\n",
       " 'dtype=tf.int32,',\n",
       " 'shape=[self.max_n_msgs,',\n",
       " '1])',\n",
       " '#',\n",
       " '[0,',\n",
       " '1,',\n",
       " '2,',\n",
       " '...]',\n",
       " 'out_id',\n",
       " '=',\n",
       " 'tf.concat([msg_ids,',\n",
       " 'ss_indices],',\n",
       " 'axis=1)',\n",
       " '#',\n",
       " 'fw,',\n",
       " 'bw',\n",
       " 'and',\n",
       " 'average',\n",
       " 'mel_h_f,',\n",
       " 'mel_h_b',\n",
       " '=',\n",
       " 'tf.gather_nd(out_f,',\n",
       " 'out_id),',\n",
       " 'tf.gather_nd(out_b,',\n",
       " 'out_id)',\n",
       " 'msg_embed',\n",
       " '=',\n",
       " '(mel_h_f',\n",
       " '+',\n",
       " 'mel_h_b)',\n",
       " '/',\n",
       " '2',\n",
       " 'return',\n",
       " 'msg_embed',\n",
       " 'def',\n",
       " '_for_one_sample(sample,',\n",
       " 'sample_ss_index,',\n",
       " 'sample_mask):',\n",
       " 'return',\n",
       " 'neural.iter(size=self.max_n_days,',\n",
       " 'func=_for_one_trading_day,',\n",
       " 'iter_arg=sample,',\n",
       " 'iter_arg2=sample_ss_index,',\n",
       " 'iter_arg3=sample_mask)',\n",
       " 'def',\n",
       " '_for_one_batch():',\n",
       " 'return',\n",
       " 'neural.iter(size=self.batch_size,',\n",
       " 'func=_for_one_sample,',\n",
       " 'iter_arg=self.mel_in,',\n",
       " 'iter_arg2=self.ss_index_ph,',\n",
       " 'iter_arg3=self.n_words_ph)',\n",
       " 'with',\n",
       " \"tf.name_scope('mel'):\",\n",
       " 'with',\n",
       " \"tf.variable_scope('mel_iter',\",\n",
       " 'reuse=tf.AUTO_REUSE):',\n",
       " 'if',\n",
       " 'self.mel_cell_type',\n",
       " '==',\n",
       " \"'ln-lstm':\",\n",
       " 'mel_cell_f',\n",
       " '=',\n",
       " 'tf.contrib.rnn.LayerNormBasicLSTMCell(self.mel_h_size)',\n",
       " 'mel_cell_b',\n",
       " '=',\n",
       " 'tf.contrib.rnn.LayerNormBasicLSTMCell(self.mel_h_size)',\n",
       " 'elif',\n",
       " 'self.mel_cell_type',\n",
       " '==',\n",
       " \"'gru':\",\n",
       " 'mel_cell_f',\n",
       " '=',\n",
       " 'tf.contrib.rnn.GRUCell(self.mel_h_size)',\n",
       " 'mel_cell_b',\n",
       " '=',\n",
       " 'tf.contrib.rnn.GRUCell(self.mel_h_size)',\n",
       " 'else:',\n",
       " 'mel_cell_f',\n",
       " '=',\n",
       " 'tf.contrib.rnn.BasicRNNCell(self.mel_h_size)',\n",
       " 'mel_cell_b',\n",
       " '=',\n",
       " 'tf.contrib.rnn.BasicRNNCell(self.mel_h_size)',\n",
       " 'mel_cell_f',\n",
       " '=',\n",
       " 'tf.contrib.rnn.DropoutWrapper(mel_cell_f,',\n",
       " 'output_keep_prob=1.0-self.dropout_mel)',\n",
       " 'mel_cell_b',\n",
       " '=',\n",
       " 'tf.contrib.rnn.DropoutWrapper(mel_cell_b,',\n",
       " 'output_keep_prob=1.0-self.dropout_mel)',\n",
       " 'mel_init_f',\n",
       " '=',\n",
       " 'mel_cell_f.zero_state([self.max_n_msgs],',\n",
       " 'tf.float32)',\n",
       " 'mel_init_b',\n",
       " '=',\n",
       " 'mel_cell_f.zero_state([self.max_n_msgs],',\n",
       " 'tf.float32)',\n",
       " 'msg_embed_shape',\n",
       " '=',\n",
       " '(self.batch_size,',\n",
       " 'self.max_n_days,',\n",
       " 'self.max_n_msgs,',\n",
       " 'self.msg_embed_size)',\n",
       " 'msg_embed',\n",
       " '=',\n",
       " 'tf.reshape(_for_one_batch(),',\n",
       " 'shape=msg_embed_shape)',\n",
       " 'self.msg_embed',\n",
       " '=',\n",
       " 'tf.nn.dropout(msg_embed,',\n",
       " 'keep_prob=1-self.dropout_mel,',\n",
       " \"name='msg_embed')\",\n",
       " 'def',\n",
       " '_create_corpus_embed(self):',\n",
       " '\"\"\"',\n",
       " 'msg_embed:',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'max_n_msgs',\n",
       " '*',\n",
       " 'msg_embed_size',\n",
       " '=>',\n",
       " 'corpus_embed:',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'corpus_embed_size',\n",
       " '\"\"\"',\n",
       " 'with',\n",
       " \"tf.name_scope('corpus_embed'):\",\n",
       " 'with',\n",
       " \"tf.variable_scope('u_t'):\",\n",
       " 'proj_u',\n",
       " '=',\n",
       " 'self._linear(self.msg_embed,',\n",
       " 'self.msg_embed_size,',\n",
       " \"'tanh',\",\n",
       " 'use_bias=False)',\n",
       " 'w_u',\n",
       " '=',\n",
       " \"tf.get_variable('w_u',\",\n",
       " 'shape=(self.msg_embed_size,',\n",
       " '1),',\n",
       " 'initializer=self.initializer)',\n",
       " 'u',\n",
       " '=',\n",
       " 'tf.reduce_mean(tf.tensordot(proj_u,',\n",
       " 'w_u,',\n",
       " 'axes=1),',\n",
       " 'axis=-1)',\n",
       " '#',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'max_n_msgs',\n",
       " 'mask_msgs',\n",
       " '=',\n",
       " 'tf.sequence_mask(self.n_msgs_ph,',\n",
       " 'maxlen=self.max_n_msgs,',\n",
       " 'dtype=tf.bool,',\n",
       " \"name='mask_msgs')\",\n",
       " 'ninf',\n",
       " '=',\n",
       " 'tf.fill(tf.shape(mask_msgs),',\n",
       " 'np.NINF)',\n",
       " 'masked_score',\n",
       " '=',\n",
       " 'tf.where(mask_msgs,',\n",
       " 'u,',\n",
       " 'ninf)',\n",
       " 'u',\n",
       " '=',\n",
       " 'neural.softmax(masked_score)',\n",
       " '#',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'max_n_msgs',\n",
       " 'u',\n",
       " '=',\n",
       " 'tf.where(tf.is_nan(u),',\n",
       " 'tf.zeros_like(u),',\n",
       " 'u)',\n",
       " '#',\n",
       " 'replace',\n",
       " 'nan',\n",
       " 'with',\n",
       " '0.0',\n",
       " 'u',\n",
       " '=',\n",
       " 'tf.expand_dims(u,',\n",
       " 'axis=-2)',\n",
       " '#',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " '1',\n",
       " '*',\n",
       " 'max_n_msgs',\n",
       " 'corpus_embed',\n",
       " '=',\n",
       " 'tf.matmul(u,',\n",
       " 'self.msg_embed)',\n",
       " '#',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " '1',\n",
       " '*',\n",
       " 'msg_embed_size',\n",
       " 'corpus_embed',\n",
       " '=',\n",
       " 'tf.reduce_mean(corpus_embed,',\n",
       " 'axis=-2)',\n",
       " '#',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'msg_embed_size',\n",
       " 'self.corpus_embed',\n",
       " '=',\n",
       " 'tf.nn.dropout(corpus_embed,',\n",
       " 'keep_prob=1-self.dropout_ce,',\n",
       " \"name='corpus_embed')\",\n",
       " 'def',\n",
       " '_build_mie(self):',\n",
       " '\"\"\"',\n",
       " 'Create',\n",
       " 'market',\n",
       " 'information',\n",
       " 'encoder.',\n",
       " 'corpus_embed:',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'corpus_embed_size',\n",
       " 'price:',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " '3',\n",
       " '=>',\n",
       " 'x:',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'x_size',\n",
       " '\"\"\"',\n",
       " 'with',\n",
       " \"tf.name_scope('mie'):\",\n",
       " 'self.price',\n",
       " '=',\n",
       " 'self.price_ph',\n",
       " 'self.price_size',\n",
       " '=',\n",
       " '3',\n",
       " 'if',\n",
       " 'self.variant_type',\n",
       " '==',\n",
       " \"'tech':\",\n",
       " 'self.x',\n",
       " '=',\n",
       " 'self.price',\n",
       " 'self.x_size',\n",
       " '=',\n",
       " 'self.price_size',\n",
       " 'else:',\n",
       " 'self._create_msg_embed_layer_in()',\n",
       " 'self._create_msg_embed_layer()',\n",
       " 'self._create_corpus_embed()',\n",
       " 'if',\n",
       " 'self.variant_type',\n",
       " '==',\n",
       " \"'fund':\",\n",
       " 'self.x',\n",
       " '=',\n",
       " 'self.corpus_embed',\n",
       " 'self.x_size',\n",
       " '=',\n",
       " 'self.corpus_embed_size',\n",
       " 'else:',\n",
       " 'self.x',\n",
       " '=',\n",
       " 'tf.concat([self.corpus_embed,',\n",
       " 'self.price],',\n",
       " 'axis=2)',\n",
       " 'self.x_size',\n",
       " '=',\n",
       " 'self.corpus_embed_size',\n",
       " '+',\n",
       " 'self.price_size',\n",
       " 'def',\n",
       " '_create_vmd_with_h_rec(self):',\n",
       " 'with',\n",
       " \"tf.name_scope('vmd'):\",\n",
       " 'with',\n",
       " \"tf.variable_scope('vmd_h_rec'):\",\n",
       " 'x',\n",
       " '=',\n",
       " 'tf.nn.dropout(self.x,',\n",
       " 'keep_prob=1-self.dropout_vmd_in)',\n",
       " 'x',\n",
       " '=',\n",
       " 'tf.transpose(x,',\n",
       " '[1,',\n",
       " '0,',\n",
       " '2])',\n",
       " '#',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'x_size',\n",
       " 'y_',\n",
       " '=',\n",
       " 'tf.transpose(self.y_ph,',\n",
       " '[1,',\n",
       " '0,',\n",
       " '2])',\n",
       " '#',\n",
       " 'max_n_days',\n",
       " '*',\n",
       " 'batch_size',\n",
       " '*',\n",
       " 'y_size',\n",
       " 'self.mask_aux_trading_days',\n",
       " '=',\n",
       " 'tf.sequence_mask(self.T_ph',\n",
       " '-',\n",
       " '1,',\n",
       " 'self.max_n_days,',\n",
       " 'dtype=tf.bool,',\n",
       " \"name='mask_aux_trading_days')\",\n",
       " 'def',\n",
       " '_loop_body(t,',\n",
       " 'ta_h_s,',\n",
       " 'ta_z_prior,',\n",
       " 'ta_z_post,',\n",
       " 'ta_kl):',\n",
       " 'with',\n",
       " \"tf.variable_scope('iter_body',\",\n",
       " 'reuse=tf.AUTO_REUSE):',\n",
       " 'def',\n",
       " '_init():',\n",
       " 'h_s_init',\n",
       " '=',\n",
       " 'tf.nn.tanh(tf.random_normal(shape=[self.batch_size,',\n",
       " 'self.h_size]))',\n",
       " 'h_z_init',\n",
       " '=',\n",
       " 'tf.nn.tanh(tf.random_normal(shape=[self.batch_size,',\n",
       " 'self.z_size]))',\n",
       " 'z_init,',\n",
       " '_',\n",
       " '=',\n",
       " 'self._z(arg=h_z_init,',\n",
       " 'is_prior=False)',\n",
       " 'return',\n",
       " 'h_s_init,',\n",
       " 'z_init',\n",
       " 'def',\n",
       " '_subsequent():',\n",
       " 'h_s_t_1',\n",
       " '=',\n",
       " 'tf.reshape(ta_h_s.read(t-1),',\n",
       " '[self.batch_size,',\n",
       " 'self.h_size])',\n",
       " 'z_t_1',\n",
       " '=',\n",
       " 'tf.reshape(ta_z_post.read(t-1),',\n",
       " '[self.batch_size,',\n",
       " 'self.z_size])',\n",
       " 'return',\n",
       " 'h_s_t_1,',\n",
       " 'z_t_1',\n",
       " 'h_s_t_1,',\n",
       " 'z_t_1',\n",
       " '=',\n",
       " 'tf.cond(t',\n",
       " '>=',\n",
       " '1,',\n",
       " '_subsequent,',\n",
       " '_init)',\n",
       " 'gate_args',\n",
       " '=',\n",
       " '[x[t],',\n",
       " 'h_s_t_1,',\n",
       " 'z_t_1]',\n",
       " 'with',\n",
       " \"tf.variable_scope('gru_r'):\",\n",
       " 'r',\n",
       " '=',\n",
       " 'self._linear(gate_args,',\n",
       " 'self.h_size,',\n",
       " \"'sigmoid')\",\n",
       " 'with',\n",
       " \"tf.variable_scope('gru_u'):\",\n",
       " 'u',\n",
       " '=',\n",
       " 'self._linear(gate_args,',\n",
       " 'self.h_size,',\n",
       " \"'sigmoid')\",\n",
       " 'h_args',\n",
       " '=',\n",
       " '[x[t],',\n",
       " 'tf.multiply(r,',\n",
       " 'h_s_t_1),',\n",
       " 'z_t_1]',\n",
       " 'with',\n",
       " \"tf.variable_scope('gru_h'):\",\n",
       " 'h_tilde',\n",
       " '=',\n",
       " 'self._linear(h_args,',\n",
       " 'self.h_size,',\n",
       " \"'tanh')\",\n",
       " 'h_s_t',\n",
       " '=',\n",
       " 'tf.multiply(1',\n",
       " '-',\n",
       " 'u,',\n",
       " 'h_s_t_1)',\n",
       " '+',\n",
       " 'tf.multiply(u,',\n",
       " 'h_tilde)',\n",
       " 'with',\n",
       " \"tf.variable_scope('h_z_prior'):\",\n",
       " 'h_z_prior_t',\n",
       " '=',\n",
       " 'self._linear([x[t],',\n",
       " 'h_s_t],',\n",
       " 'self.z_size,',\n",
       " \"'tanh')\",\n",
       " 'with',\n",
       " \"tf.variable_scope('z_prior'):\",\n",
       " 'z_prior_t,',\n",
       " 'z_prior_t_pdf',\n",
       " '=',\n",
       " 'self._z(h_z_prior_t,',\n",
       " 'is_prior=True)',\n",
       " 'with',\n",
       " \"tf.variable_scope('h_z_post'):\",\n",
       " 'h_z_post_t',\n",
       " '=',\n",
       " 'self._linear([x[t],',\n",
       " 'h_s_t,',\n",
       " 'y_[t]],',\n",
       " 'self.z_size,',\n",
       " \"'tanh')\",\n",
       " 'with',\n",
       " \"tf.variable_scope('z_post'):\",\n",
       " 'z_post_t,',\n",
       " 'z_post_t_pdf',\n",
       " '=',\n",
       " 'self._z(h_z_post_t,',\n",
       " 'is_prior=False)',\n",
       " 'kl_t',\n",
       " '=',\n",
       " 'ds.kl_divergence(z_post_t_pdf,',\n",
       " 'z_prior_t_pdf)',\n",
       " '#',\n",
       " 'write',\n",
       " 'ta_h_s',\n",
       " '=',\n",
       " 'ta_h_s.write(t,',\n",
       " 'h_s_t)',\n",
       " 'ta_z_prior',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088217a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2601\n",
      "[\"#!/usr/local/bin/python from __future__ import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode']\", \"from __future__ import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode'] self.opt\", \"__future__ import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode'] self.opt =\", \"import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode'] self.opt = config_model['opt']\", \"print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode'] self.opt = config_model['opt'] self.lr\", \"import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode'] self.opt = config_model['opt'] self.lr =\"]\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "lines = []\n",
    "for i in range(length,len(tokens)):\n",
    "    sequence = tokens[i-length:i]\n",
    "    line = \" \".join(sequence)\n",
    "    lines.append(line)\n",
    "print(len(lines)) \n",
    "print(lines[:6])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ea2b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#!/usr/local/bin/python from __future__ import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode']\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a39914f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line: #!/usr/local/bin/python from __future__ import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode']\n",
      "First token: #!/usr/local/bin/python\n"
     ]
    }
   ],
   "source": [
    "print(\"First line:\",lines[0])\n",
    "print(\"First token:\",tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4e830b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!/usr/local/bin/python'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66e7929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines[10].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "812eb4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#!/usr/local/bin/python from __future__ import print_function import os import tensorflow as tf import numpy as np import neural as neural import tensorflow.contrib.distributions as ds from tensorflow.contrib.layers import batch_norm from ConfigLoader import logger, ss_size, vocab_size, config_model, path_parser class Model: def __init__(self): logger.info('INIT: #stock: {0}, #vocab+1: {1}'.format(ss_size, vocab_size)) # model config self.mode = config_model['mode']\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0372f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',mode='min' ,patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00dfce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9382945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in sequences:\n",
    "    a.append(i[:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "901352f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0f38bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2601, 65)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecd23cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sequences[:,:-1],sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cddb9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size  = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb85493",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y,num_classes = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb26f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,64,input_length = 64))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100,activation = \"relu\"))\n",
    "model.add(Dense(vocab_size,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fdc448f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 64, 64)            28352     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64, 100)           66000     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 443)               44743     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229,595\n",
      "Trainable params: 229,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c40f867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer =\"adam\" , loss =\"categorical_crossentropy\"  ,metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7509ccd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "82/82 [==============================] - 10s 65ms/step - loss: 5.1874 - accuracy: 0.0696\n",
      "Epoch 2/150\n",
      "82/82 [==============================] - 5s 60ms/step - loss: 4.7742 - accuracy: 0.0804\n",
      "Epoch 3/150\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 4.7386 - accuracy: 0.0834\n",
      "Epoch 4/150\n",
      "82/82 [==============================] - 5s 65ms/step - loss: 4.6288 - accuracy: 0.0861\n",
      "Epoch 5/150\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 4.4497 - accuracy: 0.0869\n",
      "Epoch 6/150\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 4.2641 - accuracy: 0.1192\n",
      "Epoch 7/150\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 3.9935 - accuracy: 0.1542\n",
      "Epoch 8/150\n",
      "82/82 [==============================] - 5s 65ms/step - loss: 3.7012 - accuracy: 0.2057\n",
      "Epoch 9/150\n",
      "82/82 [==============================] - 5s 64ms/step - loss: 3.4347 - accuracy: 0.2453\n",
      "Epoch 10/150\n",
      "82/82 [==============================] - 5s 64ms/step - loss: 3.2039 - accuracy: 0.2787\n",
      "Epoch 11/150\n",
      "82/82 [==============================] - 5s 66ms/step - loss: 3.0138 - accuracy: 0.2987\n",
      "Epoch 12/150\n",
      "82/82 [==============================] - 5s 66ms/step - loss: 2.8240 - accuracy: 0.3345\n",
      "Epoch 13/150\n",
      "82/82 [==============================] - 5s 65ms/step - loss: 2.6698 - accuracy: 0.3429\n",
      "Epoch 14/150\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 2.5598 - accuracy: 0.3733\n",
      "Epoch 15/150\n",
      "82/82 [==============================] - 6s 69ms/step - loss: 2.4307 - accuracy: 0.3933\n",
      "Epoch 16/150\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 2.3214 - accuracy: 0.4156\n",
      "Epoch 17/150\n",
      "82/82 [==============================] - 6s 73ms/step - loss: 2.2030 - accuracy: 0.4248\n",
      "Epoch 18/150\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 2.1190 - accuracy: 0.4529\n",
      "Epoch 19/150\n",
      "82/82 [==============================] - 6s 67ms/step - loss: 2.0237 - accuracy: 0.4721\n",
      "Epoch 20/150\n",
      "82/82 [==============================] - 5s 65ms/step - loss: 1.9410 - accuracy: 0.4871\n",
      "Epoch 21/150\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 1.8648 - accuracy: 0.5025\n",
      "Epoch 22/150\n",
      "82/82 [==============================] - 5s 66ms/step - loss: 1.7834 - accuracy: 0.5140\n",
      "Epoch 23/150\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 1.6959 - accuracy: 0.5402\n",
      "Epoch 24/150\n",
      "82/82 [==============================] - 6s 69ms/step - loss: 1.6422 - accuracy: 0.5559\n",
      "Epoch 25/150\n",
      "82/82 [==============================] - 5s 66ms/step - loss: 1.5611 - accuracy: 0.5782\n",
      "Epoch 26/150\n",
      "82/82 [==============================] - 5s 66ms/step - loss: 1.5105 - accuracy: 0.5805\n",
      "Epoch 27/150\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 1.4299 - accuracy: 0.6155\n",
      "Epoch 28/150\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 1.3731 - accuracy: 0.6294\n",
      "Epoch 29/150\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 1.2955 - accuracy: 0.6459\n",
      "Epoch 30/150\n",
      "82/82 [==============================] - 6s 69ms/step - loss: 1.2513 - accuracy: 0.6567\n",
      "Epoch 31/150\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 1.1973 - accuracy: 0.6782\n",
      "Epoch 32/150\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 1.1325 - accuracy: 0.6870\n",
      "Epoch 33/150\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 1.0820 - accuracy: 0.7063\n",
      "Epoch 34/150\n",
      "82/82 [==============================] - 6s 75ms/step - loss: 1.0255 - accuracy: 0.7232\n",
      "Epoch 35/150\n",
      "82/82 [==============================] - 6s 76ms/step - loss: 0.9960 - accuracy: 0.7324\n",
      "Epoch 36/150\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.9461 - accuracy: 0.7463\n",
      "Epoch 37/150\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.8803 - accuracy: 0.7732\n",
      "Epoch 38/150\n",
      "82/82 [==============================] - 6s 78ms/step - loss: 0.8238 - accuracy: 0.7862\n",
      "Epoch 39/150\n",
      "82/82 [==============================] - 11s 132ms/step - loss: 0.7864 - accuracy: 0.7974\n",
      "Epoch 40/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.7630 - accuracy: 0.8016\n",
      "Epoch 41/150\n",
      "82/82 [==============================] - 11s 135ms/step - loss: 0.7169 - accuracy: 0.8143\n",
      "Epoch 42/150\n",
      "82/82 [==============================] - 10s 122ms/step - loss: 0.6598 - accuracy: 0.8308\n",
      "Epoch 43/150\n",
      "82/82 [==============================] - 11s 129ms/step - loss: 0.6227 - accuracy: 0.8408\n",
      "Epoch 44/150\n",
      "82/82 [==============================] - 12s 145ms/step - loss: 0.5897 - accuracy: 0.8516\n",
      "Epoch 45/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.5651 - accuracy: 0.8558\n",
      "Epoch 46/150\n",
      "82/82 [==============================] - 12s 146ms/step - loss: 0.5397 - accuracy: 0.8666\n",
      "Epoch 47/150\n",
      "82/82 [==============================] - 10s 123ms/step - loss: 0.5197 - accuracy: 0.8681\n",
      "Epoch 48/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.4983 - accuracy: 0.8704\n",
      "Epoch 49/150\n",
      "82/82 [==============================] - 11s 138ms/step - loss: 0.4545 - accuracy: 0.8885\n",
      "Epoch 50/150\n",
      "82/82 [==============================] - 12s 142ms/step - loss: 0.4416 - accuracy: 0.8877\n",
      "Epoch 51/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.4114 - accuracy: 0.8993\n",
      "Epoch 52/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.3798 - accuracy: 0.9077\n",
      "Epoch 53/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.3571 - accuracy: 0.9166\n",
      "Epoch 54/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.3328 - accuracy: 0.9266\n",
      "Epoch 55/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.3333 - accuracy: 0.9173\n",
      "Epoch 56/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.3068 - accuracy: 0.9316\n",
      "Epoch 57/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.2924 - accuracy: 0.9323\n",
      "Epoch 58/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.2866 - accuracy: 0.9339\n",
      "Epoch 59/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.2561 - accuracy: 0.9408\n",
      "Epoch 60/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.2362 - accuracy: 0.9485\n",
      "Epoch 61/150\n",
      "82/82 [==============================] - 12s 142ms/step - loss: 0.2299 - accuracy: 0.9493\n",
      "Epoch 62/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.2259 - accuracy: 0.9562\n",
      "Epoch 63/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.2167 - accuracy: 0.9535\n",
      "Epoch 64/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.2010 - accuracy: 0.9581\n",
      "Epoch 65/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.1953 - accuracy: 0.9554\n",
      "Epoch 66/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.1762 - accuracy: 0.9619\n",
      "Epoch 67/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.1531 - accuracy: 0.9689\n",
      "Epoch 68/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.1405 - accuracy: 0.9742\n",
      "Epoch 69/150\n",
      "82/82 [==============================] - 12s 142ms/step - loss: 0.1370 - accuracy: 0.9696\n",
      "Epoch 70/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.1280 - accuracy: 0.9758\n",
      "Epoch 71/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.1176 - accuracy: 0.9792\n",
      "Epoch 72/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.1123 - accuracy: 0.9815\n",
      "Epoch 73/150\n",
      "82/82 [==============================] - 12s 142ms/step - loss: 0.1081 - accuracy: 0.9827\n",
      "Epoch 74/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.1055 - accuracy: 0.9819\n",
      "Epoch 75/150\n",
      "82/82 [==============================] - 12s 141ms/step - loss: 0.1093 - accuracy: 0.9789\n",
      "Epoch 76/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.1054 - accuracy: 0.9789\n",
      "Epoch 77/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.0877 - accuracy: 0.9835\n",
      "Epoch 78/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.0902 - accuracy: 0.9823\n",
      "Epoch 79/150\n",
      "82/82 [==============================] - 12s 143ms/step - loss: 0.2469 - accuracy: 0.9254\n",
      "Epoch 80/150\n",
      "82/82 [==============================] - 12s 144ms/step - loss: 0.2506 - accuracy: 0.9243\n"
     ]
    }
   ],
   "source": [
    "record = model.fit(X,y, epochs = 150,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c38b74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('text_generate.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29252b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('text_generate.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6b0602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 1: 5.18744421005249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x228756af580>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"loss at epoch 1: {record.history['loss'][0]}\")\n",
    "# print(f\"loss at epoch 150: {record.history['loss'][149]}\")\n",
    "plt.plot(record.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93d4bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa7d02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model,tokenizer,text_seq_length,seed_text,n_words):\n",
    "    text = []\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded],maxlen = text_seq_length,truncating = 'pre')\n",
    "        \n",
    "#         y_predict = model.predict_classes(encoded)\n",
    "        predict_x = model.predict(encoded) \n",
    "        y_predict = np.argmax(predict_x,axis=1)\n",
    "        \n",
    "        predicted_words = \" \"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == y_predict:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        seed_text = seed_text + \" \" + predicted_word\n",
    "        text.append(predicted_word)\n",
    "    return \" \".join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3ba62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"config model 'momentum' self decay rate config model 'momentum' self decay embed apply anneal rate' self kl use gradients use gradients use gradients use gradients use true res self input config model 'mode' self kl lambda anneal use gradients lambda anneal use f zero n 'days size config model 'mode' self opt config model 'opt' model 'opt' model 'opt' model 'n rate' self kl output config model 'momentum' self kl apply gradients ph self input kl use bw create inputs att' layers assert decay rate uniform key exponential decay rate uniform 'weight' 'xavier uniform' 'xavier normal' vec opt 'sgd' decay\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_seq(model,tokenizer,64,seed_text,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cee5de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/damminhtien/python-code-generation-with-rnn/notebook\n",
    "# https://cainvas.ai-tech.systems/use-cases/text-generation-app-using-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf29d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
