{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bda2a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 230) (50000, 1)\n",
      "191 38\n",
      "Removed features:  ['Var1' 'Var2' 'Var3' 'Var4' 'Var5' 'Var8' 'Var9' 'Var10' 'Var11' 'Var12'\n",
      " 'Var14' 'Var15' 'Var16' 'Var17' 'Var18' 'Var19' 'Var20' 'Var23' 'Var26'\n",
      " 'Var27' 'Var29' 'Var30' 'Var31' 'Var32' 'Var33' 'Var34' 'Var36' 'Var37'\n",
      " 'Var39' 'Var40' 'Var41' 'Var42' 'Var43' 'Var45' 'Var46' 'Var47' 'Var48'\n",
      " 'Var49' 'Var50' 'Var51' 'Var52' 'Var53' 'Var54' 'Var55' 'Var56' 'Var58'\n",
      " 'Var59' 'Var60' 'Var61' 'Var62' 'Var63' 'Var64' 'Var66' 'Var67' 'Var68'\n",
      " 'Var69' 'Var70' 'Var71' 'Var72' 'Var75' 'Var77' 'Var79' 'Var80' 'Var82'\n",
      " 'Var84' 'Var86' 'Var87' 'Var88' 'Var89' 'Var90' 'Var91' 'Var92' 'Var93'\n",
      " 'Var94' 'Var95' 'Var96' 'Var97' 'Var98' 'Var99' 'Var100' 'Var101'\n",
      " 'Var102' 'Var103' 'Var104' 'Var105' 'Var106' 'Var107' 'Var108' 'Var110'\n",
      " 'Var111' 'Var114' 'Var115' 'Var116' 'Var117' 'Var118' 'Var120' 'Var121'\n",
      " 'Var122' 'Var124' 'Var127' 'Var128' 'Var129' 'Var130' 'Var131' 'Var135'\n",
      " 'Var136' 'Var137' 'Var138' 'Var139' 'Var141' 'Var142' 'Var145' 'Var146'\n",
      " 'Var147' 'Var148' 'Var150' 'Var151' 'Var152' 'Var154' 'Var155' 'Var156'\n",
      " 'Var157' 'Var158' 'Var159' 'Var161' 'Var162' 'Var164' 'Var165' 'Var166'\n",
      " 'Var167' 'Var168' 'Var169' 'Var170' 'Var171' 'Var172' 'Var174' 'Var175'\n",
      " 'Var176' 'Var177' 'Var178' 'Var179' 'Var180' 'Var182' 'Var183' 'Var184'\n",
      " 'Var185' 'Var186' 'Var187' 'Var188' 'Var189' 'Var190' 'Var191' 'Var194'\n",
      " 'Var200' 'Var201' 'Var209' 'Var213' 'Var214' 'Var215' 'Var224' 'Var225'\n",
      " 'Var229' 'Var230']\n",
      "Var6      float64\n",
      "Var7      float64\n",
      "Var13     float64\n",
      "Var21     float64\n",
      "Var22     float64\n",
      "           ...   \n",
      "Var222     object\n",
      "Var223     object\n",
      "Var226     object\n",
      "Var227     object\n",
      "Var228     object\n",
      "Length: 67, dtype: object\n",
      "38 28\n",
      "Var192  categories : 362\n",
      "Var193  categories : 51\n",
      "Var195  categories : 23\n",
      "Var196  categories : 4\n",
      "Var197  categories : 226\n",
      "Var198  categories : 4291\n",
      "Var199  categories : 5074\n",
      "Var202  categories : 5714\n",
      "Var203  categories : 6\n",
      "Var204  categories : 100\n",
      "Var205  categories : 4\n",
      "Var206  categories : 22\n",
      "Var207  categories : 14\n",
      "Var208  categories : 3\n",
      "Var210  categories : 6\n",
      "Var211  categories : 2\n",
      "Var212  categories : 81\n",
      "Var216  categories : 2016\n",
      "Var217  categories : 13991\n",
      "Var218  categories : 3\n",
      "Var219  categories : 23\n",
      "Var220  categories : 4291\n",
      "Var221  categories : 7\n",
      "Var222  categories : 4291\n",
      "Var223  categories : 5\n",
      "Var226  categories : 23\n",
      "Var227  categories : 7\n",
      "Var228  categories : 30\n",
      "Var192  categories : 362\n",
      "50 : 156\n",
      "100 : 9\n",
      "1000 : 197\n",
      "1000 : 0\n",
      "Var197  categories : 226\n",
      "50 : 107\n",
      "100 : 12\n",
      "1000 : 101\n",
      "1000 : 6\n",
      "Var198  categories : 4291\n",
      "50 : 4128\n",
      "100 : 130\n",
      "1000 : 31\n",
      "1000 : 2\n",
      "Var199  categories : 5074\n",
      "50 : 4865\n",
      "100 : 87\n",
      "1000 : 122\n",
      "1000 : 0\n",
      "Var202  categories : 5714\n",
      "50 : 5633\n",
      "100 : 70\n",
      "1000 : 11\n",
      "1000 : 0\n",
      "Var204  categories : 100\n",
      "50 : 1\n",
      "100 : 1\n",
      "1000 : 88\n",
      "1000 : 10\n",
      "Var216  categories : 2016\n",
      "50 : 1902\n",
      "100 : 60\n",
      "1000 : 46\n",
      "1000 : 8\n",
      "Var217  categories : 13991\n",
      "50 : 13916\n",
      "100 : 48\n",
      "1000 : 27\n",
      "1000 : 0\n",
      "Var220  categories : 4291\n",
      "50 : 4128\n",
      "100 : 130\n",
      "1000 : 31\n",
      "1000 : 2\n",
      "Var222  categories : 4291\n",
      "50 : 4128\n",
      "100 : 130\n",
      "1000 : 31\n",
      "1000 : 2\n",
      "Var192  categories : 362\n",
      "50 : 156\n",
      "100 : 9\n",
      "1000 : 197\n",
      "inf : 0\n",
      "Var197  categories : 226\n",
      "50 : 107\n",
      "100 : 12\n",
      "1000 : 101\n",
      "inf : 6\n",
      "Var198  categories : 4291\n",
      "50 : 4128\n",
      "100 : 130\n",
      "1000 : 31\n",
      "inf : 2\n",
      "Var199  categories : 5074\n",
      "50 : 4865\n",
      "100 : 87\n",
      "1000 : 122\n",
      "inf : 0\n",
      "Var202  categories : 5714\n",
      "50 : 5633\n",
      "100 : 70\n",
      "1000 : 11\n",
      "inf : 0\n",
      "Var204  categories : 100\n",
      "50 : 1\n",
      "100 : 1\n",
      "1000 : 88\n",
      "inf : 10\n",
      "Var216  categories : 2016\n",
      "50 : 1902\n",
      "100 : 60\n",
      "1000 : 46\n",
      "inf : 8\n",
      "Var217  categories : 13991\n",
      "50 : 13916\n",
      "100 : 48\n",
      "1000 : 27\n",
      "inf : 0\n",
      "Var220  categories : 4291\n",
      "50 : 4128\n",
      "100 : 130\n",
      "1000 : 31\n",
      "inf : 2\n",
      "Var222  categories : 4291\n",
      "50 : 4128\n",
      "100 : 130\n",
      "1000 : 31\n",
      "inf : 2\n",
      "After reducing the number of categories:\n",
      "Var192  categories : 3\n",
      "Var193  categories : 51\n",
      "Var195  categories : 23\n",
      "Var196  categories : 4\n",
      "Var197  categories : 4\n",
      "Var198  categories : 4\n",
      "Var199  categories : 3\n",
      "Var202  categories : 3\n",
      "Var203  categories : 6\n",
      "Var204  categories : 4\n",
      "Var205  categories : 4\n",
      "Var206  categories : 22\n",
      "Var207  categories : 14\n",
      "Var208  categories : 3\n",
      "Var210  categories : 6\n",
      "Var211  categories : 2\n",
      "Var212  categories : 81\n",
      "Var216  categories : 4\n",
      "Var217  categories : 3\n",
      "Var218  categories : 3\n",
      "Var219  categories : 23\n",
      "Var220  categories : 4\n",
      "Var221  categories : 7\n",
      "Var222  categories : 4\n",
      "Var223  categories : 5\n",
      "Var226  categories : 23\n",
      "Var227  categories : 7\n",
      "Var228  categories : 30\n",
      "322\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## Read Data\n",
    "base_path = './'\n",
    "\n",
    "\n",
    "train = pd.read_table(f'{base_path}/orange_small_train.data').replace('\\\\', '/')\n",
    "\n",
    "test= pd.read_csv(f'{base_path}/orange_small_train_upselling.labels', header=None)\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "\n",
    "# ## Data Cleaning\n",
    "\n",
    "\n",
    "train_dtypes = train.dtypes\n",
    "float_dtypes = train_dtypes[train_dtypes =='float64'].index.values.tolist()\n",
    "object_dtypes = train_dtypes[train_dtypes =='object'].index.values.tolist()\n",
    "print(len(float_dtypes),len(object_dtypes))\n",
    "\n",
    "\n",
    "\n",
    "na_vals = train.isna().sum() > (train.shape[0]*0.30) # Atleast 70% values should be available\n",
    "removed_features = (na_vals[na_vals==True]).index.values\n",
    "train = train[(na_vals[na_vals==False]).index.values]\n",
    "print('Removed features: ', removed_features)\n",
    "\n",
    "train_dtypes = train.dtypes\n",
    "print(train_dtypes)\n",
    "num_var = train_dtypes[train_dtypes =='float64'].index.values.tolist()\n",
    "cat_vars = train_dtypes[train_dtypes =='object'].index.values.tolist()\n",
    "print(len(num_var),len(cat_vars))\n",
    "\n",
    "\n",
    "# Numerical variables\n",
    "for col in num_var: \n",
    "    train[col] = train[col].fillna(train[col].mean())\n",
    "\n",
    "for col in cat_vars:\n",
    "    train[col] = train[col].astype('category')\n",
    "    train[col] = train[col].cat.add_categories('MISSED')\n",
    "    train[col] = train[col].fillna('MISSED')\n",
    "\n",
    "\n",
    "#for col in cat_vars:\n",
    "#    print(col,\" categories :\", train[col].nunique())\n",
    "#    train[col].value_counts().plot()\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for col in cat_vars:\n",
    "    print(col,\" categories :\", train[col].nunique())\n",
    "\n",
    "\n",
    "def analyse_cats(cat):\n",
    "    val_cnts=cat.value_counts()\n",
    "    levs = [0,50, 100, 1000]\n",
    "    for i in range(1,len(levs)):\n",
    "        print(levs[i],\":\",((val_cnts<=levs[i])&(val_cnts>levs[i-1])).sum())\n",
    "    print(levs[i],\":\",((val_cnts>levs[-1])).sum())\n",
    "\n",
    "for col in cat_vars:\n",
    "    if  train[col].nunique() >= 100:\n",
    "        print(col,\" categories :\", train[col].nunique())\n",
    "        analyse_cats(train[col])\n",
    "\n",
    "\n",
    "# As we can see there are a lot of categories that have count less than 50,100 and 1000 , we can combine all these in different categories\n",
    "\n",
    "\n",
    "for col in cat_vars:\n",
    "    if  train[col].nunique() >= 100:\n",
    "        print(col,\" categories :\", train[col].nunique())\n",
    "        col_val_cnt = train[col].value_counts()\n",
    "        lev_names= ['l','m','h','vh']\n",
    "        levs = [0,50, 100, 1000, np.inf]\n",
    "        lev_cat_names =[]\n",
    "        for i in range(1,len(levs)):\n",
    "            print(levs[i],\":\",((col_val_cnt<=levs[i])&(col_val_cnt>levs[i-1])).sum())\n",
    "            cat_to_be_replaced = col_val_cnt[(col_val_cnt<=levs[i])&(col_val_cnt>levs[i-1])].index.values.tolist()\n",
    "            lev_cat_names.append(cat_to_be_replaced)\n",
    "        mapping = {}\n",
    "        for i in range(len(lev_cat_names)):\n",
    "            for cat in lev_cat_names[i]:\n",
    "                mapping[cat]=lev_names[i]\n",
    "        train[col]=train[col].map(mapping)\n",
    "\n",
    "print('After reducing the number of categories:')\n",
    "cnt=0\n",
    "for col in cat_vars:\n",
    "    cnt += (train[col].nunique()-1)\n",
    "    print(col,\" categories :\", train[col].nunique())\n",
    "print(cnt)\n",
    "\n",
    "\n",
    "\n",
    "# One hot encode the category variables \n",
    "train = pd.get_dummies(train)\n",
    "\n",
    "test[0]= test[0].map({-1:0,1:1})\n",
    "\n",
    "\n",
    "# ### Train and testing \n",
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "X,Y = train ,test\n",
    "\n",
    "sns.countplot(Y[0])\n",
    "plt.show()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 2)\n",
    "X, Y = sm.fit_resample(X, Y)\n",
    "sns.countplot(Y[0])\n",
    "plt.show()\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Train Features Shape:', X_train.shape)\n",
    "print('Train Labels Shape:', y_train.shape)\n",
    "print('Validation Features Shape:', X_valid.shape)\n",
    "print('Validation Labels Shape:', y_valid.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "def evaluate_model(rf, X, y):\n",
    "    y_pred =(rf.predict(X) >0.5).astype(int)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 50, random_state = 42,n_jobs=-1)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Use the forest's predict method on the valdation data\n",
    "print('Validation Data')\n",
    "evaluate_model(rf, X_valid, y_valid)\n",
    "print('Training Data')\n",
    "# Use the forest's predict method on the train data\n",
    "evaluate_model(rf, X_train, y_train)\n",
    "\n",
    "f_i = list(zip(train.columns,rf.feature_importances_))\n",
    "f_i.sort(key = lambda x : x[1],reverse=True)\n",
    "\n",
    "feat_imp=pd.DataFrame(f_i)\n",
    "feat_imp['imp']=feat_imp[1].cumsum()\n",
    "feat_imp=feat_imp[feat_imp['imp'] <=0.95]\n",
    "#feat_imp['imp'].plot()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "choosen_feat = feat_imp[0].tolist()\n",
    "\n",
    "#NOw we have choosen the important features , we can start training our final model\n",
    "\n",
    "# ## Choose parameter using cross validation and grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "n_estimators = [40]\n",
    "max_depth = [10,30, 40]\n",
    "min_samples_split = [2,5, 10]\n",
    "min_samples_leaf = [2,5, 10] \n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "forest=RandomForestRegressor(random_state = 1) \n",
    "gridF = GridSearchCV(forest, hyperF, cv = 5, verbose = 3, n_jobs = -1,scoring='f1')\n",
    "bestF = gridF.fit(X_train[choosen_feat], y_train)\n",
    "\n",
    "\n",
    "best_params= pd.DataFrame(gridF.cv_results_).sort_values('rank_test_score',ascending =False).iloc[0]['params']\n",
    "print('Best Params: ', best_params)\n",
    "\n",
    "\n",
    "# Use the forest's predict method on the valdation data\n",
    "print('Validation Data')\n",
    "evaluate_model(bestF, X_valid[choosen_feat], y_valid)\n",
    "print('Training Data')\n",
    "# Use the forest's predict method on the train data\n",
    "evaluate_model(bestF, X_train[choosen_feat], y_train)\n",
    "\n",
    "best_rf_clf = bestF.best_estimator_\n",
    "\n",
    "\n",
    "# # Final model on the whole dataset and its performance\n",
    "\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = best_rf_clf\n",
    "# Train the model on training data\n",
    "rf.fit(train_features[choosen_feat],train_labels)\n",
    "\n",
    "\n",
    "print('Final Results on Testing data')\n",
    "evaluate_model(rf, test_features[choosen_feat], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be2803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
