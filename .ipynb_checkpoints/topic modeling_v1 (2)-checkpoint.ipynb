{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zGiXUGe1Ud6Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import re   # regex\n",
    "import nltk # text handling\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from textblob import TextBlob # WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZNbFsfa_00Od"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import seaborn as sns \n",
    "from sklearn import preprocessing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "import re\n",
    "import string\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkQG38Mns_Sm",
    "outputId": "7444369a-7b29-4f86-ae19-a8158004592c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avitr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wdfNHINEnE0k"
   },
   "outputs": [],
   "source": [
    "# To see the whole tweet text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zlt2wFw-AdrY"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pld5-njokxdI"
   },
   "outputs": [],
   "source": [
    "# load specific columns from the csv file\n",
    "#col_list = [\"created_at\", \"id\", \"in_reply_to_status_id\", \n",
    " #           \"in_reply_to_user_id\", \"text\", \"user_screen_name\"]\n",
    "\n",
    "tweets = pd.read_excel('Sentiment Analysis Training.xlsx')\n",
    "#\n",
    " #                 , usecols = col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-t6yBIZQmXwc",
    "outputId": "24c906de-365d-42c1-b457-ee8cd8607995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9qVVKShBZ5Z"
   },
   "source": [
    "# Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oFXQ99ueoC1F"
   },
   "outputs": [],
   "source": [
    "mentions = re.compile('(@[a-zA-Z0-9_]{1,50})') # mentions regex @...\n",
    "urls = re.compile('http\\S+')         # links regex\n",
    "diacritics = re.compile('ٰ ّ َ ً ُ ٌ ِ ٍ ْـ')       # Tashkeel regex\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ«»'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(mentions, '', text)\n",
    "    return text\n",
    "\n",
    "# removing URLs\n",
    "def remove_urls(text):\n",
    "    # twitter converts all links to thier own domain t.co\n",
    "    text = re.sub(urls, '', text)\n",
    "    return text\n",
    "\n",
    "# removing tashkeel\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(r'[\\u064b-\\u065f]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # https://www.linkedin.com/pulse/extracting-twitter-data-pre-processing-sentiment-using-jayasekara\n",
    "    return re.sub(emoji_pattern, '', text)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # lemmatizes text\n",
    "    lemmatized_text = []\n",
    "\n",
    "    # set up arabic lemmatizer Farasa\n",
    "    url = 'https://farasa.qcri.org/webapi/lemmatization/'\n",
    "    api_key = \"lErIOPgmHZtflLMgIf\"\n",
    "\n",
    "    # set up english lemmatizer\n",
    "    eng_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    for word in text:\n",
    "        # Detect language to use proper lemmatizer\n",
    "        if TextBlob(word).detect_language() == 'en':\n",
    "            lemmatized_text.append(eng_lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            payload = {'text': word, 'api_key': api_key}\n",
    "            data = requests.post(url, data=payload)\n",
    "            lemmatized_text.append(json.loads(data.text))\n",
    "    return lemmatized_text\n",
    "\n",
    "def text_stemming(text):\n",
    "    return ISRIStemmer().suf32(text)\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    translator = str.maketrans(' ', ' ', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "    \n",
    "# remove hashtags marks but keep the words itself\n",
    "def normalize_hashtags(text):\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"_\", \" \", text)\n",
    "    text = re.sub(\"_\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1jefUK1oOrW",
    "outputId": "d31497b0-a9c9-447c-c543-bb4ed446505c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['انتِ', 'اثنان', 'سبعماءه', 'فاء', 'راي']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_stopwords = map(normalize_arabic, stopwords)\n",
    "normalized_stopwords = list(normalized_stopwords)\n",
    "normalized_stopwords = list(set(normalized_stopwords))\n",
    "normalized_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iXZawvM4sqfD"
   },
   "outputs": [],
   "source": [
    "# clean text\n",
    "def text_preprocessing(text):\n",
    "    text = remove_diacritics(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = normalize_hashtags(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = ' '.join(word for word in text.split() if word not in normalized_stopwords)\n",
    "    return text#.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-v1YRoLqNAdT"
   },
   "outputs": [],
   "source": [
    "tweets['text']=tweets['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q12YEnowtE4c"
   },
   "outputs": [],
   "source": [
    "tweets['clean_tweet'] = tweets['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VaNcuYsLOJd"
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =10000)\n",
    "\n",
    "word_vectorizer = word_vectorizer.fit(tweets['clean_tweet'])\n",
    "unigramdataGet = word_vectorizer.transform(tweets['clean_tweet'])\n",
    "unigramdataGet = unigramdataGet.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcوش</th>\n",
       "      <th>...</th>\n",
       "      <th>١٨</th>\n",
       "      <th>٢٠</th>\n",
       "      <th>٢٠٣٠</th>\n",
       "      <th>٢٤</th>\n",
       "      <th>٢٩</th>\n",
       "      <th>٣خطوط</th>\n",
       "      <th>٣٠١٢</th>\n",
       "      <th>٤ساعات</th>\n",
       "      <th>٥٠</th>\n",
       "      <th>٨سنوات</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 813 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10   13   5g   90  iphone   my  mystc  stc  stcpay  stcوش  ...   ١٨   ٢٠  \\\n",
       "0  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "\n",
       "   ٢٠٣٠   ٢٤   ٢٩  ٣خطوط  ٣٠١٢  ٤ساعات   ٥٠  ٨سنوات  \n",
       "0   1.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "1   0.0  0.0  0.0    1.0   0.0     0.0  0.0     0.0  \n",
       "2   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "3   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "4   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "\n",
       "[5 rows x 813 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jnctNBe8scne"
   },
   "outputs": [],
   "source": [
    "pro = preprocessing.LabelEncoder()\n",
    "tweets['label_encoded'] = pro.fit_transform(tweets['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Negative\n",
      "1 Neutral\n",
      "2 Positive\n"
     ]
    }
   ],
   "source": [
    "for indx,clas in enumerate(pro.classes_):\n",
    "    print(indx,clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Z3p4RTPSsc0d"
   },
   "outputs": [],
   "source": [
    "y = tweets['label_encoded']\n",
    "X = unigramdata_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iYfurQbTsC8g"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "DsnssSJbbLur",
    "outputId": "e9ae2e90-aa8c-43fc-a384-dc406edc09b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcوش</th>\n",
       "      <th>...</th>\n",
       "      <th>١٨</th>\n",
       "      <th>٢٠</th>\n",
       "      <th>٢٠٣٠</th>\n",
       "      <th>٢٤</th>\n",
       "      <th>٢٩</th>\n",
       "      <th>٣خطوط</th>\n",
       "      <th>٣٠١٢</th>\n",
       "      <th>٤ساعات</th>\n",
       "      <th>٥٠</th>\n",
       "      <th>٨سنوات</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 813 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      10   13   5g   90  iphone   my  mystc  stc  stcpay  stcوش  ...   ١٨  \\\n",
       "53   0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0   \n",
       "100  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0   \n",
       "11   0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0   \n",
       "126  0.0  1.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0   \n",
       "45   0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0   \n",
       "\n",
       "      ٢٠  ٢٠٣٠   ٢٤   ٢٩  ٣خطوط  ٣٠١٢  ٤ساعات   ٥٠  ٨سنوات  \n",
       "53   0.0   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "100  0.0   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "11   0.0   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "126  0.0   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "45   0.0   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "\n",
       "[5 rows x 813 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nY4TOoL_sC_Z",
    "outputId": "20fcb077-b68c-4268-cd93-baa27938ab9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=GaussianNB()\n",
    "nb= nb.fit(X_train , y_train)\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwU29NoltQ_B",
    "outputId": "71edd4ad-d880-41e1-abe1-332d9b9cf924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.593\n",
      "Accuracy= 0.953\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.predict(X_test)\n",
    "# nb_1=nb.score(X_test, y_test)\n",
    "print('Accuracy= {:.3f}'.format(nb.score(X_test, y_test)))\n",
    "print('Accuracy= {:.3f}'.format(nb.score(X_train , y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73        16\n",
      "           1       0.50      0.38      0.43         8\n",
      "           2       0.25      0.33      0.29         3\n",
      "\n",
      "    accuracy                           0.59        27\n",
      "   macro avg       0.49      0.49      0.48        27\n",
      "weighted avg       0.59      0.59      0.59        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 1, 2, 2, 0, 1,\n",
       "       0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(nb, open('classification.pickle', 'wb'))\n",
    "\n",
    "pickle.dump(word_vectorizer, open('word_vectorizer.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read test file for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = pd.read_excel('test_set.xlsx') # it is the same file with different name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['text']=replies['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['clean_tweet'] = replies['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open('word_vectorizer.pickle','rb'))\n",
    "model = pickle.load(open('classification.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramdataGet = vectorizer.transform(replies['clean_tweet'])\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "unigramdataGet[unigramdataGet>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(unigramdataGet)\n",
    "\n",
    "replies['Predicted_label'] = y_pred\n",
    "replies['Predicted_label'] = replies['Predicted_label'].replace([0,1,2],['Negative', 'Neutral', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@NASEER_OB @ ابشرك بتدخل اليوم العاشر عشان يردوا عليك او يخدمونك ياخي تحس stc مايدرون عن رؤية ٢٠٣٠ 😅 الى الحين ونفس الاخطاء من عشرات السنين</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@NASEER_OB @ ابشرك ٣خطوط نقلتها من @ الى @Mobily  وتوبة وماعاد لي رجعة لهم</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ اخذته على رقمي الثاني من فرع الظهران مول  الموضوع ما اخذ ولا عشر دقايق شكراً #stc @stc https://t.co/eAtmgQJKgv</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@s9sb @ من يعوضني ي stc 🥺</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@NASEER_OB @ ترا ماينفع معهم الا انك تروح لاقرب فرع وتتفاهم معهم وقلهم بحول عنكم شوف كيف يرجعو</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>@ @mjeid_0 ياخي ليه ماتردون وتخلصون مشكلتي 🤌🏼</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>@ خلاص فعلته بطريقة ثانيه من mystc وحطيته بالرمز اليدوي</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>@ \\nصباح الخير \\nألغيت رقمي المفوتر ونزلت لي مديوانية \\nسلاماااات .. \\nكيف كذا ؟؟</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>@ سوا بوست بلس</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>@ بكم تفعيل مكالمات سوا لا محدود حق شهر؟</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text  \\\n",
       "0    @NASEER_OB @ ابشرك بتدخل اليوم العاشر عشان يردوا عليك او يخدمونك ياخي تحس stc مايدرون عن رؤية ٢٠٣٠ 😅 الى الحين ونفس الاخطاء من عشرات السنين   \n",
       "1                                                                     @NASEER_OB @ ابشرك ٣خطوط نقلتها من @ الى @Mobily  وتوبة وماعاد لي رجعة لهم   \n",
       "2                               @ اخذته على رقمي الثاني من فرع الظهران مول  الموضوع ما اخذ ولا عشر دقايق شكراً #stc @stc https://t.co/eAtmgQJKgv   \n",
       "3                                                                                                                      @s9sb @ من يعوضني ي stc 🥺   \n",
       "4                                                 @NASEER_OB @ ترا ماينفع معهم الا انك تروح لاقرب فرع وتتفاهم معهم وقلهم بحول عنكم شوف كيف يرجعو   \n",
       "..                                                                                                                                           ...   \n",
       "128                                                                                                @ @mjeid_0 ياخي ليه ماتردون وتخلصون مشكلتي 🤌🏼   \n",
       "129                                                                                      @ خلاص فعلته بطريقة ثانيه من mystc وحطيته بالرمز اليدوي   \n",
       "130                                                            @ \\nصباح الخير \\nألغيت رقمي المفوتر ونزلت لي مديوانية \\nسلاماااات .. \\nكيف كذا ؟؟   \n",
       "131                                                                                                                               @ سوا بوست بلس   \n",
       "132                                                                                                     @ بكم تفعيل مكالمات سوا لا محدود حق شهر؟   \n",
       "\n",
       "        label Predicted_label  \n",
       "0    Negative        Negative  \n",
       "1    Positive        Positive  \n",
       "2    Positive        Negative  \n",
       "3    Negative        Negative  \n",
       "4     Neutral         Neutral  \n",
       "..        ...             ...  \n",
       "128  Negative        Negative  \n",
       "129   Neutral         Neutral  \n",
       "130  Negative        Negative  \n",
       "131   Neutral         Neutral  \n",
       "132   Neutral         Neutral  \n",
       "\n",
       "[133 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[['text','label','Predicted_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Tweets Clustering using Word2Vec and K-means",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
