{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52e1115-b7bd-426c-b8ca-dd5a18281b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buildapcsales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 98/98 [01:19<00:00,  1.24it/s]\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\avitr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 98/98 [00:01<00:00, 61.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 98/98 [00:00<00:00, 99453.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 98/98 [00:00<00:00, 32312.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many likes per hour ratio does the post have: 7\n",
      "What type of sale item are they looking for: Amazon\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "user_agent=\"agent01\"\n",
    "reddit = praw.Reddit(username=\"No-Professional-8030\",\n",
    "                     password=\"Tutorial01\",\n",
    "                     client_id=\"2MS-Ef8sTpSYzdbqEpIxAg\",\n",
    "                     client_secret=\"Cnht4sjg8xG0EZTaPI8Yk0Kn3zGKqw\",\n",
    "                     user_agent=user_agent\n",
    ")\n",
    "subreddit_name=\"buildapcsales\"\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "print(subreddit.display_name)\n",
    "\n",
    "hot_subreddit=subreddit.hot()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "post=[]\n",
    "type = ''\n",
    "price = 0.00\n",
    "item = ''\n",
    "regex= r\"\\[([^\\]]+)\\](.+?)(\\$\\d+)\"\n",
    "for submission in subreddit.hot():\n",
    "    if(submission.id in ('yvocv1','z2atkk')):\n",
    "        continue\n",
    "    result = re.search(r'\\[([^\\]]+)\\](.+?)(\\$\\d+\\.?\\d+)', submission.title)\n",
    "    type=result.group(1)\n",
    "    item=result.group(2).lstrip()\n",
    "    price=result.group(3)\n",
    "#     print(type)\n",
    "#     print(item)\n",
    "#     print(price)\n",
    "    post.append([submission.id,type,item,price,submission.score,submission.url,submission.created])\n",
    "\n",
    "post = pd.DataFrame(post,columns=['id','Type','Item','Price','score','url','Time Created'])\n",
    "\n",
    "def get_comments(submission_id):\n",
    "    submission = reddit.submission(submission_id)\n",
    "    comments = []\n",
    "    for top_level_comment in submission.comments:\n",
    "        comments.append(top_level_comment.body)\n",
    "        \n",
    "    comment = ' '.join(comments)\n",
    "    comment = comment.replace(\"\\n\",' ')\n",
    "    return comment\n",
    "\n",
    "post['Comment'] = post['id'].progress_apply(lambda x: get_comments(x))\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "def sentiment_scores(sentence):\n",
    "\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    \n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"Negative\"\n",
    "    else :\n",
    "        return \"Neutral\"\n",
    "\n",
    "post['Sentiment'] = post['Comment'].progress_apply(lambda x: sentiment_scores(x))\n",
    "\n",
    "def get_date_time(x):\n",
    "    return datetime.fromtimestamp(x)#.strftime('%Y-%m-%d')\n",
    "\n",
    "post['Time Created1'] = post['Time Created'].progress_apply(lambda x: get_date_time(x))\n",
    "\n",
    "def get_post_hours(x):\n",
    "    return (time.time() - x)/3600\n",
    "\n",
    "post['LikesPerHour'] = post.progress_apply(lambda x: math.ceil(x['score']/get_post_hours(x['Time Created'])),axis=1)\n",
    "\n",
    "post.rename(columns={'score':'Overall Likes'},inplace=True)\n",
    "\n",
    "post.drop(['Time Created'],axis=1,inplace=True)\n",
    "\n",
    "post.rename(columns={'Time Created1':'Post Datetime'},inplace=True)\n",
    "\n",
    "n = int(input(\"How many likes per hour ratio does the post have: \"))\n",
    "item = input(\"What type of sale item are they looking for: \")\n",
    "\n",
    "post = post[post['LikesPerHour']>=n]\n",
    "\n",
    "def search_item(x):\n",
    "    return any([True if i.lower() in x.lower() else False for i in item.split()])\n",
    "\n",
    "post = post[post['Item'].apply(lambda name: search_item(name))]\n",
    "\n",
    "post.to_csv('post.csv',index=False) # use this post.csv to load into google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295438db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410b9a17",
   "metadata": {},
   "source": [
    "# Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your client id, follow this link\n",
    "#https://docs.gspread.org/en/latest/oauth2.html#for-end-users-using-oauth-client-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "gc = gspread.authorize(credentials)\n",
    "# Read CSV file contents\n",
    "content = open('post.csv', 'r').read()\n",
    "\n",
    "gc.import_csv('<SPREADSHEET_ID>', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744a4e4",
   "metadata": {},
   "source": [
    "# or\n",
    "\n",
    "https://towardsdatascience.com/using-python-to-push-your-pandas-dataframe-to-google-sheets-de69422508f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/62917910/how-can-i-export-pandas-dataframe-to-google-sheets-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d9ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
