{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"ADHD_classification-absolute.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1DPy5GA9knMvyibltdWJNHOx_32RvOKv3\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Enter your path here which contains directories named ADHD_part1 and so on and so forth, the /content is path on my setup\n",
    "PATH = \"/content/drive/MyDrive/Archive/\"\n",
    "\n",
    "ADHD_directories = [\"ADHD_part1\",\"ADHD_part2\"]\n",
    "NonADHD_directories = [\"Control_part1\",\"Control_part2\"]\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import os\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def get_params(data):  \n",
    "  # Define sampling frequency and time vector\n",
    "  sf = 128.\n",
    "  time = np.arange(data.size) / sf\n",
    "  return sf,time\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "def get_welch_params(data,sf):\n",
    "# Define window length (4 seconds)\n",
    "  win = 4 * sf\n",
    "  freqs, psd = signal.welch(data, sf, nperseg=win)\n",
    "  return freqs, psd\n",
    "\n",
    "from scipy.integrate import simps\n",
    "def get_absolute_power(psd,freqs,idx):\n",
    "  freq_res = freqs[1] - freqs[0] \n",
    "  # Compute the absolute power by approximating the area under the curve\n",
    "  power = simps(psd[idx],dx=freq_res)\n",
    "  return power\n",
    "\n",
    "def get_relative_power(psd,freqs,power):\n",
    "  freq_res = freqs[1] - freqs[0]\n",
    "  total_power = simps(psd, dx=freq_res)\n",
    "  rel_power = power / total_power\n",
    "  return rel_power\n",
    "\n",
    "def bandpower(data, sf, band, window_sec=None, relative=False):\n",
    "    \n",
    "    from scipy.signal import welch\n",
    "    from scipy.integrate import simps\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "\n",
    "    # Define window length\n",
    "    if window_sec is not None:\n",
    "        nperseg = window_sec * sf\n",
    "    else:\n",
    "        nperseg = (2 / low) * sf\n",
    "\n",
    "    # Compute the modified periodogram (Welch)\n",
    "    freqs, psd = welch(data, sf, nperseg=nperseg)\n",
    "\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "\n",
    "    # Find closest indices of band in frequency vector\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "\n",
    "    # Integral approximation of the spectrum using Simpson's rule.\n",
    "    bp = simps(psd[idx_band], dx=freq_res)\n",
    "\n",
    "    if relative:\n",
    "        bp /= simps(psd, dx=freq_res)\n",
    "    return bp\n",
    "\n",
    "def get_ratio(data,sf):\n",
    "  win_sec = 4\n",
    "\n",
    "  # Delta/beta ratio based on the absolute power\n",
    "  db = bandpower(data, sf, [0.5, 4], win_sec) / bandpower(data, sf, [12, 30], win_sec)\n",
    "\n",
    "  # Delta/beta ratio based on the relative power\n",
    "  db_rel = bandpower(data, sf, [0.5, 4], win_sec, True) / bandpower(data, sf, [12, 30], win_sec, True)\n",
    "\n",
    "channel_dict = {'Fp1':0,'Fp2':1,'F3':2,'F4':3,'C3':4,'C4':5,'P3':6,'P4':7,'O1':8,'O2':9,'F7':10,'F8':11,'T7':12,'T8':13,'P7':14,'P8':15,'Fz':16,'Cz':17,'Pz':18}\n",
    "\n",
    "bands = {'alpha':[8,12],'beta':[12,30],'theta':[4,8],'delta':[0.5,4]}\n",
    "\n",
    "def get_band_indexes(low,high,freqs):\n",
    "  idx = np.logical_and(freqs >= low, freqs <= high)\n",
    "  return idx\n",
    "\n",
    "#Considering all channels and generating ratios for all channels now\n",
    "for channel_name in channel_dict:\n",
    "#Change the channel name here whichever data or ratios needed\n",
    "  #Dataframe to store absolute bandpower\n",
    "  dataframe_bands = pd.DataFrame()\n",
    "  for directory in ADHD_directories: \n",
    "    for mat_file in os.listdir(os.path.join(PATH,directory)):\n",
    "      #loading the file\n",
    "      if \".mat\" in mat_file:\n",
    "        loaded_file = scipy.io.loadmat(os.path.join(PATH,directory,mat_file))\n",
    "      #getting data from the loaded file, mat_file[:-4] gives name of file without .mat\n",
    "        per_person_data = loaded_file[mat_file[:-4]]\n",
    "      #Get the exact column information\n",
    "        data = per_person_data[:,channel_dict[channel_name]]\n",
    "        sf,time = get_params(data)\n",
    "        freqs,psd = get_welch_params(data,sf)\n",
    "      #add alpha ,beta,theta and delta info one by one\n",
    "        idx_alpha = get_band_indexes(bands['alpha'][0],bands['alpha'][1],freqs)\n",
    "        alpha = get_absolute_power(psd,freqs,idx_alpha)\n",
    "        rel_alpha = get_relative_power(psd,freqs,alpha)\n",
    "        idx_beta = get_band_indexes(bands['beta'][0],bands['beta'][1],freqs)\n",
    "        beta = get_absolute_power(psd,freqs,idx_beta)\n",
    "        rel_beta = get_relative_power(psd,freqs,beta)\n",
    "        idx_theta = get_band_indexes(bands['theta'][0],bands['theta'][1],freqs)\n",
    "        theta = get_absolute_power(psd,freqs,idx_theta)\n",
    "        rel_theta = get_relative_power(psd,freqs,theta)\n",
    "        idx_delta = get_band_indexes(bands['delta'][0],bands['delta'][1],freqs)\n",
    "        delta = get_absolute_power(psd,freqs,idx_delta)\n",
    "        rel_delta = get_relative_power(psd,freqs,delta)\n",
    "        dataframe_bands = dataframe_bands.append({'Filename' : mat_file,'Label': 'ADHD', 'Absolute_Alpha':alpha,'Relative_Alpha':rel_alpha,'Absolute_Beta':beta,'Relative_Beta':rel_beta,'Absolute_Theta':theta,'Relative_Theta':rel_theta,'Absolute_Delta':delta,'Relative_Delta':rel_delta},ignore_index=True)\n",
    "\n",
    "  for directory in NonADHD_directories:\n",
    "    for mat_file in os.listdir(os.path.join(PATH,directory)):\n",
    "      loaded_file = scipy.io.loadmat(os.path.join(PATH,directory,mat_file))\n",
    "      if \".mat\" in mat_file:\n",
    "        per_person_data = loaded_file[mat_file[:-4]]\n",
    "        #Get the exact column information\n",
    "        data = per_person_data[:,channel_dict[channel_name]]\n",
    "        sf,time = get_params(data)\n",
    "        freqs,psd = get_welch_params(data,sf)\n",
    "        #add alpha ,beta,theta and delta info one by one\n",
    "        idx_alpha = get_band_indexes(bands['alpha'][0],bands['alpha'][1],freqs)\n",
    "        alpha = get_absolute_power(psd,freqs,idx_alpha)\n",
    "        rel_alpha = get_relative_power(psd,freqs,alpha)\n",
    "        idx_beta = get_band_indexes(bands['beta'][0],bands['beta'][1],freqs)\n",
    "        beta = get_absolute_power(psd,freqs,idx_beta)\n",
    "        rel_beta = get_relative_power(psd,freqs,beta)\n",
    "        idx_theta = get_band_indexes(bands['theta'][0],bands['theta'][1],freqs)\n",
    "        theta = get_absolute_power(psd,freqs,idx_theta)\n",
    "        rel_theta = get_relative_power(psd,freqs,theta)\n",
    "        idx_delta = get_band_indexes(bands['delta'][0],bands['delta'][1],freqs)\n",
    "        delta = get_absolute_power(psd,freqs,idx_delta)\n",
    "        rel_delta = get_relative_power(psd,freqs,delta)\n",
    "        dataframe_bands = dataframe_bands.append({'Filename' : mat_file,'Label': 'Non_ADHD', 'Absolute_Alpha':alpha,'Relative_Alpha':rel_alpha,'Absolute_Beta':beta,'Relative_Beta':rel_beta,'Absolute_Theta':theta,'Relative_Theta':rel_theta,'Absolute_Delta':delta,'Relative_Delta':rel_delta},ignore_index=True)\n",
    "  dataframe_bands.to_csv('/content/'+channel_name+'.csv')\n",
    "\n",
    "#Pass nominator and denominator as whose ratio needed\n",
    "#Example Theta to Beta ratio \n",
    "#For above example check column names in csv for the channel \n",
    "#It is Absolute_Theta and Absolute_Beta\n",
    "#So, call function with get_ratio(\"Absolute_Theta\",\"Absolute_Beta\",\"Cz\")\n",
    "def get_ratio_c(nominator,denominator,channel_name):\n",
    "  band_data = pd.read_csv(\"/content/\" + channel_name + \".csv\")\n",
    "  nominator_array = np.array(band_data[nominator])\n",
    "  denominator_array = np.array(band_data[denominator])\n",
    "  ratios = nominator_array/denominator_array\n",
    "  \n",
    "  labels = band_data['Label']\n",
    "  del band_data\n",
    "  return np.array(ratios),np.array(labels)\n",
    "\n",
    "#Write channel names here which you want to use for calculation. For now I have included all channels\n",
    "channels_for_training =  ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2','F7','F8','T7','T8','P7','P8','Fz','Cz','Pz']\n",
    "#Included two ratios here, if more needed add here in same way\n",
    "ratios_to_be_included = [(\"Absolute_Theta\",\"Absolute_Beta\"),(\"Absolute_Alpha\",\"Absolute_Beta\")]\n",
    "\n",
    "#power band is whichever pwer is needed specify Absolute_Alpha,Absolute_Beta and so on and forth\n",
    "def get_absolute_power(channel_name,power_band):\n",
    "  band_data = pd.read_csv(\"/content/\" + channel_name + \".csv\")\n",
    "  data_array = np.array(band_data[power_band])\n",
    "  \n",
    "  labels = band_data['Label']\n",
    "  return data_array,labels\n",
    "\n",
    "# #Create datset with ratio don't run this for creating dataset with absolute power\n",
    "# tensors_data_temp = []\n",
    "# for ratio_name in ratios_to_be_included:\n",
    "#   for channel in channels_for_training:\n",
    "#     a = ratio_name[0]\n",
    "#     b = ratio_name[1] \n",
    "#     ratios,labels = get_ratio_c(a,b,channel) \n",
    "#     tensors_data_temp.append(ratios)\n",
    "# tensors_data_temp = np.transpose(np.array(tensors_data_temp))\n",
    "# tensors_data = tf.data.Dataset.from_tensor_slices(tensors_data_temp)\n",
    "# label_encode = []\n",
    "# for i in labels:\n",
    "#   if i ==\"ADHD\":\n",
    "#     label_encode.append(0)\n",
    "#   else:\n",
    "#     label_encode.append(1)\n",
    "# print(label_encode)\n",
    "# tensors_labels = tf.data.Dataset.from_tensor_slices(np.array(label_encode))\n",
    "# # adding two datasets together so that we have data with classes/labels\n",
    "# dataset = tf.data.Dataset.zip((tensors_data,tensors_labels))\n",
    "\n",
    "#Create dataset with absolute power\n",
    "tensors_data_temp = []\n",
    "for power_name in [\"Absolute_Alpha\",\"Absolute_Beta\",\"Absolute_Theta\"]:\n",
    "  for channel in channels_for_training:\n",
    "    data_arrays,labels = get_absolute_power(channel,power_name) \n",
    "    tensors_data_temp.append(data_arrays)\n",
    "tensors_data_temp = np.transpose(np.array(tensors_data_temp))\n",
    "tensors_data = tf.data.Dataset.from_tensor_slices(tensors_data_temp)\n",
    "label_encode = []\n",
    "for i in labels:\n",
    "  if i ==\"ADHD\":\n",
    "    label_encode.append(0)\n",
    "  else:\n",
    "    label_encode.append(1)\n",
    "print(label_encode)\n",
    "tensors_labels = tf.data.Dataset.from_tensor_slices(np.array(label_encode))\n",
    "# adding two datasets together so that we have data with classes/labels\n",
    "dataset = tf.data.Dataset.zip((tensors_data,tensors_labels))\n",
    "\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "  \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds,val_ds,test_ds = get_dataset_partitions_tf(dataset,len(dataset))\n",
    "print(\"Total number of people involved in dataset \",len(dataset))\n",
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)\n",
    "test_ds = test_ds.batch(32)\n",
    "\n",
    "# # This model is for ratios do not use in absolute power\n",
    "# def create_model():\n",
    "#   keras_model = tf.keras.Sequential([\n",
    "#       tf.keras.layers.Input(shape=(len(channels_for_training)*len(ratios_to_be_included))),\n",
    "#       tf.keras.layers.Dense(50),\n",
    "#       tf.keras.layers.Activation(tf.nn.relu),\n",
    "#       tf.keras.layers.Dense(30),\n",
    "#       tf.keras.layers.Activation(tf.nn.relu),\n",
    "#       tf.keras.layers.Dense(2),\n",
    "#       tf.keras.layers.Activation(tf.nn.softmax),\n",
    "# ])\n",
    "#   return keras_model\n",
    "\n",
    "#This model is for absolute values\n",
    "def create_model():\n",
    "  keras_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Input(shape=(len(channels_for_training)*3)),\n",
    "      tf.keras.layers.Dense(50),\n",
    "      tf.keras.layers.Activation(tf.nn.relu),\n",
    "      tf.keras.layers.Dense(30),\n",
    "      tf.keras.layers.Activation(tf.nn.relu),\n",
    "      tf.keras.layers.Dense(2),\n",
    "      tf.keras.layers.Activation(tf.nn.softmax),\n",
    "])\n",
    "  return keras_model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "#Optimization\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001,momentum=0.0),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,    \n",
    "    batch_size=32,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    validation_data = val_ds,verbose='auto'\n",
    ")\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(model.predict(test_ds))\n",
    "\n",
    "def print_accuracy(ds):\n",
    "  predictions = model.predict(ds)\n",
    "  predicted_labels = np.argmax(predictions, axis=1)\n",
    "  true_values = np.concatenate([y for x, y in ds], axis=0)\n",
    "  count = 0\n",
    "  for i in range(len(predicted_labels)):\n",
    "    if predicted_labels[i] == true_values[i]:\n",
    "      count = count + 1\n",
    "  print(\"The accuracy for given dataset is: \", count/len(predicted_labels)*100)\n",
    "\n",
    "print_accuracy(test_ds)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "predictions = model.predict(test_ds)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(predicted_labels)\n",
    "true_values = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "print(true_values)\n",
    "cm =confusion_matrix(true_values,predicted_labels,labels=[0, 1])\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
