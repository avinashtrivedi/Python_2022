{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1728dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Vudu Revenue Forecast(1).py\n",
    "\"\"\"\n",
    "\n",
    "@author: Divya.Veeramani\n",
    "\"\"\"\n",
    "\n",
    "# Import Required Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "from prophet import Prophet\n",
    "import datetime as dt\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io.sql import SQLTable\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Establish redshift connection\n",
    "user_name = '*****'\n",
    "password = '*****'\n",
    "db_name = '*****'\n",
    "server = '*****'\n",
    "driver = '*****'\n",
    "port = '*****'\n",
    "conn = psycopg2.connect(dbname=db_name, host=server, port=port, user=user_name,password=password)\n",
    "\n",
    "# Change current directory\n",
    "os.chdir('D:/Fandango/Weekly forecast Vudu')\n",
    "\n",
    "# Set Maximum Date for forecast\n",
    "max_date = '2022-12-31'\n",
    "\n",
    "# Get Daily Level Data\n",
    "file1 = open(\"Vudu_Forecast_data_daily.sql\",\"rt\")\n",
    "QUERY1  = file1.read()\n",
    "trans_data = sqlio.read_sql_query(QUERY1, conn)\n",
    "cols = ['Transaction Date', 'Est Revenue', 'Vod Revenue', 'Est Trannx',\n",
    "       'Vod Trannx', 'Nr Est Rev', 'Nr Vod Rev', 'Lib Est Rev', 'Lib Vod Rev',\n",
    "       'D2d Revenue', 'Total Rev', 'Discounted Rev', 'New Paids', 'Ex Paids']\n",
    "trans_data.columns = cols\n",
    "trans_data['Transaction Date'] = pd.to_datetime(trans_data['Transaction Date'])\n",
    "date_range = pd.period_range(min(trans_data['Transaction Date']),max_date)\n",
    "date_range = pd.DataFrame(date_range)\n",
    "date_range.columns = ['Transaction Date2']\n",
    "date_range['Transaction Date2'] = pd.to_datetime(date_range['Transaction Date2'].astype(str))\n",
    "#trans_data = pd.concat([trans_data,date_range], ignore_index=True, axis=1)\n",
    "trans_data = pd.merge(trans_data,date_range, left_on = 'Transaction Date', right_on = 'Transaction Date2', how = 'right')\n",
    "trans_data = trans_data.iloc[:,1:]\n",
    "temp_cols = list(trans_data.columns)\n",
    "temp_cols = [temp_cols[-1]] + temp_cols[:-1]\n",
    "trans_data = trans_data[temp_cols]\n",
    "trans_data.columns = cols\n",
    "trans_data = trans_data.sort_values(by = ['Transaction Date'])\n",
    "trans_data = trans_data.reset_index()\n",
    "\n",
    "# Get Weekly Level Data\n",
    "file1 = open(\"Vudu_Forecast_data_weekly.sql\",\"rt\")\n",
    "QUERY1  = file1.read()\n",
    "trans_data_weekly = sqlio.read_sql_query(QUERY1, conn)\n",
    "trans_data_weekly.columns = cols\n",
    "trans_data_weekly['Transaction Date'] = pd.to_datetime(trans_data_weekly['Transaction Date'])\n",
    "date_range = pd.period_range(min(trans_data_weekly['Transaction Date']),max_date,freq = 'W')\n",
    "date_range = pd.DataFrame(date_range)\n",
    "date_range[0] = date_range[0].astype(str).str.split(\"/\", n=1, expand = True)[0]\n",
    "date_range[0] = pd.to_datetime(date_range[0])\n",
    "date_range.columns = ['Transaction Date2']\n",
    "date_range['Transaction Date2'] = pd.to_datetime(date_range['Transaction Date2'].astype(str))\n",
    "#trans_data = pd.concat([trans_data,date_range], ignore_index=True, axis=1)\n",
    "trans_data_weekly = pd.merge(trans_data_weekly,date_range, left_on = 'Transaction Date', right_on = 'Transaction Date2', how = 'right')\n",
    "trans_data_weekly = trans_data_weekly.iloc[:,1:]\n",
    "temp_cols = list(trans_data_weekly.columns)\n",
    "temp_cols = [temp_cols[-1]] + temp_cols[:-1]\n",
    "trans_data_weekly = trans_data_weekly[temp_cols]\n",
    "trans_data_weekly.columns = cols\n",
    "trans_data_weekly = trans_data_weekly.sort_values(by = ['Transaction Date'])\n",
    "trans_data_weekly = trans_data_weekly.reset_index()\n",
    "\n",
    "# Get Monthly Level Data\n",
    "file1 = open(\"Vudu_Forecast_data_monthly.sql\",\"rt\")\n",
    "QUERY1  = file1.read()\n",
    "trans_data_monthly = sqlio.read_sql_query(QUERY1, conn)\n",
    "trans_data_monthly.columns = cols\n",
    "trans_data_monthly['Transaction Date'] = pd.to_datetime(trans_data_monthly['Transaction Date'])\n",
    "date_range = pd.period_range(min(trans_data_monthly['Transaction Date']),max_date,freq = 'M')\n",
    "date_range = pd.DataFrame(date_range)\n",
    "date_range.columns = ['Transaction Date2']\n",
    "date_range['Transaction Date2'] = pd.to_datetime(date_range['Transaction Date2'].astype(str))\n",
    "#trans_data = pd.concat([trans_data,date_range], ignore_index=True, axis=1)\n",
    "trans_data_monthly = pd.merge(trans_data_monthly,date_range, left_on = 'Transaction Date', right_on = 'Transaction Date2', how = 'right')\n",
    "trans_data_monthly = trans_data_monthly.iloc[:,1:]\n",
    "temp_cols = list(trans_data_monthly.columns)\n",
    "temp_cols = [temp_cols[-1]] + temp_cols[:-1]\n",
    "trans_data_monthly = trans_data_monthly[temp_cols]\n",
    "trans_data_monthly.columns = cols\n",
    "trans_data_monthly = trans_data_monthly.sort_values(by = ['Transaction Date'])\n",
    "trans_data_monthly = trans_data_monthly.reset_index()\n",
    "\n",
    "# Give train cut off\n",
    "now = dt.datetime.now()\n",
    "dow = now.weekday()\n",
    "train_cut_off = now - dt.timedelta(days = dow+7)\n",
    "train_cut_off = train_cut_off.strftime(\"%Y-%m-%d\")\n",
    "# train_cut_off = '2021-10-04'\n",
    "print(train_cut_off)\n",
    "\n",
    "# Accuracy metrics\n",
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
    "    me = np.mean(forecast - actual)             # ME\n",
    "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
    "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    mins = np.amin(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    maxs = np.amax(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    #acf1 = acf(fc-test)[1]                      # ACF1\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, \n",
    "            'corr':corr, 'minmax':minmax})\n",
    "\n",
    "trans_data['Transaction Date'] = trans_data['Transaction Date'].astype(str)\n",
    "trans_data['Transaction Date'] = pd.to_datetime(trans_data['Transaction Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Remove First half of 2020\n",
    "trans_data = trans_data.loc[(trans_data['Transaction Date']< '2020-01-01')| (trans_data['Transaction Date']> '2020-06-30'),:].reset_index()\n",
    "trans_data['week_day'] = pd.DatetimeIndex(trans_data['Transaction Date']).dayofweek\n",
    "\n",
    "#---------------------------EX PAIDS DAILY FORECAST--------------------------\n",
    "# Train and Test Split\n",
    "train = trans_data.loc[(trans_data['Transaction Date']< train_cut_off),['Transaction Date','Total Rev','Ex Paids','week_day']]\n",
    "test = trans_data.loc[(trans_data['Transaction Date']>= train_cut_off),['Transaction Date','Total Rev','Ex Paids','week_day']]\n",
    "train.rename(columns = {'Transaction Date' : 'ds', 'Ex Paids' : 'y'}, inplace = True)\n",
    "\n",
    "# Train prophet - Added weekly and yearly seasonality, US holidays\n",
    "m = Prophet(weekly_seasonality = True, yearly_seasonality = True)\n",
    "m.add_country_holidays(country_name='US')\n",
    "m.fit(train)\n",
    "\n",
    "# Create future data frame with seasonality variables\n",
    "future = m.make_future_dataframe(periods=test.shape[0])\n",
    "\n",
    "# Predict and Plot\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot_components(forecast)\n",
    "forecast['week_day'] = trans_data['week_day']\n",
    "forecast['yhat'] = forecast['yhat'] * 0.90 #1.05#1.02\n",
    "forecast['yhat_lower'] = forecast['yhat_lower'] * 0.90\n",
    "forecast['yhat_upper'] = forecast['yhat_upper'] * 0.90\n",
    "\n",
    "# Get Accuracy metrics\n",
    "print(forecast_accuracy(forecast.loc[forecast['ds']>=train_cut_off,'yhat'], test['Ex Paids']))\n",
    "ex_paids_pred = forecast.loc[forecast['ds']>=train_cut_off,['ds','yhat','yhat_lower','yhat_upper']]\n",
    "ex_paids_pred['Ex Paids'] = test['Ex Paids']\n",
    "ex_paids_pred['week_day'] = test['week_day']\n",
    "ex_paids_pred.head()\n",
    "\n",
    "# Get predicted Ex Paids and concatenate data for Total Revenue prediction\n",
    "# Taken Actual value of Ex Paids till July 2021 after which predicted values are taken \n",
    "test.loc[test['Transaction Date']>=train_cut_off,'Ex Paids'] = forecast.loc[forecast['ds']>=train_cut_off,'yhat']\n",
    "train.rename(columns = {'y':'Ex Paids','ds':'Transaction Date'}, inplace = True)\n",
    "print(test.head())\n",
    "print(train.head())\n",
    "trans_data = pd.concat([train[['Transaction Date','Total Rev','Ex Paids']],test[['Transaction Date','Total Rev','Ex Paids']]])\n",
    "trans_data['year'] = pd.to_datetime(trans_data['Transaction Date']).dt.year\n",
    "trans_data['year'].value_counts()\n",
    "\n",
    "#---------------------------EX PAIDS WEEKLY FORECAST--------------------------\n",
    "trans_data_weekly['week_day'] = pd.DatetimeIndex(trans_data_weekly['Transaction Date']).dayofweek\n",
    "\n",
    "# Train and Test Split\n",
    "train = trans_data_weekly.loc[(trans_data_weekly['Transaction Date']< train_cut_off),['Transaction Date','Total Rev','Ex Paids','week_day']]\n",
    "test = trans_data_weekly.loc[(trans_data_weekly['Transaction Date']>= train_cut_off),['Transaction Date','Total Rev','Ex Paids','week_day']]\n",
    "train.rename(columns = {'Transaction Date' : 'ds', 'Ex Paids' : 'y'}, inplace = True)\n",
    "\n",
    "# Train prophet - Added weekly and yearly seasonality, US holidays\n",
    "m = Prophet(yearly_seasonality = True)\n",
    "m.add_country_holidays(country_name='US')\n",
    "m.fit(train)\n",
    "\n",
    "# Create future data frame with seasonality variables\n",
    "future = m.make_future_dataframe(freq='W',periods=test.shape[0])\n",
    "print(future.tail())\n",
    "\n",
    "# Predict and Plot\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot_components(forecast)\n",
    "forecast['week_day'] = trans_data_weekly['week_day']\n",
    "forecast['ds'] = forecast['ds']+dt.timedelta(days = 1) \n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "forecast['yhat'] = forecast['yhat'] * 0.95 # 1.12\n",
    "forecast['yhat_lower'] = forecast['yhat_lower'] * 0.95\n",
    "forecast['yhat_upper'] = forecast['yhat_upper'] * 0.95\n",
    "\n",
    "# Get Accuracy metrics\n",
    "print(forecast_accuracy(forecast.loc[forecast['ds']>=train_cut_off,'yhat'], test['Ex Paids']))\n",
    "ex_paids_week = forecast.loc[forecast['ds']>=train_cut_off,['ds','yhat','yhat_lower','yhat_upper']]\n",
    "ex_paids_week['Ex Paids'] = test['Ex Paids']\n",
    "ex_paids_week['week_day'] = test['week_day']\n",
    "ex_paids_week['Ex Paids'].iloc[1] = 0\n",
    "print(ex_paids_week.head())\n",
    "\n",
    "#---------------------------EX PAIDS MONTHLY FORECAST--------------------------\n",
    "trans_data_monthly['Transaction Date'] = trans_data_monthly['Transaction Date'].astype(str)\n",
    "trans_data_monthly['Transaction Date'] = pd.to_datetime(trans_data_monthly['Transaction Date'])\n",
    "trans_data_monthly['week_day'] = pd.DatetimeIndex(trans_data_monthly['Transaction Date']).dayofweek\n",
    "\n",
    "train_cut_off_month = (dt.datetime.now() - dt.timedelta(days = 7)).replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Train and Test Split\n",
    "train = trans_data_monthly.loc[(trans_data_monthly['Transaction Date']< train_cut_off_month),['Transaction Date','Total Rev','Ex Paids']]\n",
    "test = trans_data_monthly.loc[(trans_data_monthly['Transaction Date']>= train_cut_off_month),['Transaction Date','Total Rev','Ex Paids']]\n",
    "train.rename(columns = {'Transaction Date' : 'ds', 'Ex Paids' : 'y'}, inplace = True)\n",
    "\n",
    "# Train prophet - Added weekly and yearly seasonality, US holidays\n",
    "m = Prophet(yearly_seasonality = True)\n",
    "m.add_country_holidays(country_name='US')\n",
    "m.fit(train)\n",
    "\n",
    "# Create future data frame with seasonality variables\n",
    "future = m.make_future_dataframe(freq='M',periods=test.shape[0])\n",
    "future.loc[future['ds'].dt.day>1,'ds'] = future.loc[future['ds'].dt.day>1,'ds'] + dt.timedelta(days = 1)\n",
    "print(future.tail())\n",
    "\n",
    "# Predict and Plot\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot_components(forecast)\n",
    "#forecast['week_day'] = trans_data_monthly['week_day']\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "forecast['yhat'] = forecast['yhat'] * 0.95 #1.12\n",
    "forecast['yhat_lower'] = forecast['yhat_lower'] * 0.95\n",
    "forecast['yhat_upper'] = forecast['yhat_upper'] * 0.95\n",
    "\n",
    "# Get Accuracy metrics\n",
    "print(forecast_accuracy(forecast.loc[forecast['ds']>=train_cut_off_month,'yhat'], test['Ex Paids']))\n",
    "ex_paids_month = forecast.loc[forecast['ds']>=train_cut_off_month,['ds','yhat','yhat_lower','yhat_upper']]\n",
    "ex_paids_month['Ex Paids'] = test['Ex Paids']\n",
    "ex_paids_month.reset_index(inplace = True, drop = True)\n",
    "\n",
    "first_month = ex_paids_month['ds'].iloc[0].month\n",
    "curr_month = dt.date.today().month\n",
    "\n",
    "if first_month != curr_month:\n",
    "    ex_paids_month = ex_paids_month.drop([0], axis=0).reset_index(drop = True)\n",
    "    \n",
    "print(ex_paids_month.head())\n",
    "\n",
    "# Ex Paids Output\n",
    "ex_paids_pred['% error'] = pd.Series(ex_paids_pred.yhat)/pd.Series(ex_paids_pred['Ex Paids'])-1\n",
    "ex_paids_pred[\"mape\"] = np.abs(ex_paids_pred[\"% error\"])\n",
    "\n",
    "ex_paids_week['% error'] = pd.Series(ex_paids_week.yhat)/pd.Series(ex_paids_week['Ex Paids'])-1\n",
    "ex_paids_week[\"mape\"] = np.abs(ex_paids_week[\"% error\"])\n",
    "\n",
    "ex_paids_month['% error'] = pd.Series(ex_paids_month.yhat)/pd.Series(ex_paids_month['Ex Paids'])-1\n",
    "ex_paids_month[\"mape\"] = np.abs(ex_paids_month[\"% error\"])\n",
    "\n",
    "\n",
    "#---------------------------TOTAL REVENUE FORECAST--------------------------\n",
    "# Prophet\n",
    "trans_data['week_day'] = pd.DatetimeIndex(trans_data['Transaction Date']).dayofweek\n",
    "# Train and Test Split\n",
    "train = trans_data.loc[(trans_data['Transaction Date']< train_cut_off),['Transaction Date','Total Rev','Ex Paids','week_day']]\n",
    "test = trans_data.loc[(trans_data['Transaction Date']>= train_cut_off),['Transaction Date','Total Rev','Ex Paids','week_day']]\n",
    "train.rename(columns = {'Transaction Date' : 'ds', 'Total Rev' : 'y'}, inplace = True)\n",
    "\n",
    "# Train Prophet - Added only Ex Paids regressor\n",
    "m = Prophet(weekly_seasonality=True, yearly_seasonality=True)\n",
    "m.add_regressor('Ex Paids')\n",
    "m.add_country_holidays(country_name='US')\n",
    "m.fit(train)\n",
    "\n",
    "# Prepare future data frame for prediction\n",
    "future = m.make_future_dataframe(periods=test.shape[0])\n",
    "future['Ex Paids'] = trans_data['Ex Paids']\n",
    "future.tail()\n",
    "future.fillna(0, inplace = True)\n",
    "\n",
    "# Predict and Plot\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot_components(forecast)\n",
    "\n",
    "# Get Accuracy metrics\n",
    "print(forecast_accuracy(forecast.loc[forecast['ds']>=train_cut_off,'yhat'], test['Total Rev']))\n",
    "total_rev_pred = forecast.loc[forecast['ds']>=train_cut_off,['ds','yhat','yhat_lower','yhat_upper']]\n",
    "total_rev_pred['Total Rev'] = test['Total Rev']\n",
    "total_rev_pred['week_day'] = test['week_day']\n",
    "total_rev_pred.loc[total_rev_pred['week_day']<=4,'yhat'] = total_rev_pred.loc[total_rev_pred['week_day']<=4,'yhat'] * 0.89 #1.04\n",
    "total_rev_pred.loc[total_rev_pred['week_day']<=4,'yhat_lower'] = total_rev_pred.loc[total_rev_pred['week_day']<=4,'yhat_lower'] * 0.89\n",
    "total_rev_pred.loc[total_rev_pred['week_day']<=4,'yhat_upper'] = total_rev_pred.loc[total_rev_pred['week_day']<=4,'yhat_upper'] * 0.89\n",
    "\n",
    "total_rev_pred.loc[total_rev_pred['week_day']>4,'yhat'] = total_rev_pred.loc[total_rev_pred['week_day']>4,'yhat'] * 0.97 #1.06\n",
    "total_rev_pred.loc[total_rev_pred['week_day']>4,'yhat_lower'] = total_rev_pred.loc[total_rev_pred['week_day']>4,'yhat_lower'] * 0.97\n",
    "total_rev_pred.loc[total_rev_pred['week_day']>4,'yhat_upper'] = total_rev_pred.loc[total_rev_pred['week_day']>4,'yhat_upper'] * 0.97\n",
    "\n",
    "# Total Revenue Output\n",
    "total_rev_pred['ds'] = pd.to_datetime(total_rev_pred['ds'])\n",
    "total_rev_pred['WeekDate'] = total_rev_pred.apply(lambda row: row['ds'] - dt.timedelta(days=row['ds'].weekday()), axis=1)\n",
    "total_rev_pred['Year Month'] = total_rev_pred['ds'].dt.to_period('M')\n",
    "total_rev_pred['% error'] = pd.Series(total_rev_pred.yhat)/pd.Series(total_rev_pred['Total Rev'])-1\n",
    "total_rev_pred[\"mape\"] = np.abs(total_rev_pred[\"% error\"])\n",
    "\n",
    "total_rev_week = total_rev_pred.groupby(by = ['WeekDate'])['Total Rev','yhat','yhat_lower','yhat_upper'].agg({'Total Rev' : 'sum','yhat':'sum','yhat_lower':'sum', 'yhat_upper':'sum'})\n",
    "total_rev_week.reset_index(inplace = True)\n",
    "total_rev_week = total_rev_week.loc[total_rev_week['WeekDate'] >= train_cut_off,:]\n",
    "total_rev_week['Total Rev'].iloc[1] = 0\n",
    "total_rev_week['% error'] = pd.Series(total_rev_week.yhat)/pd.Series(total_rev_week['Total Rev'])-1\n",
    "total_rev_week[\"mape\"] = np.abs(total_rev_week[\"% error\"])\n",
    "\n",
    "\n",
    "total_rev_month = total_rev_pred.groupby(by = ['Year Month'])['Total Rev','yhat','yhat_lower','yhat_upper'].agg({'Total Rev' : 'sum','yhat':'sum','yhat_lower':'sum', 'yhat_upper':'sum'})\n",
    "total_rev_month.reset_index(inplace = True)\n",
    "total_rev_month = total_rev_month.loc[total_rev_month['Year Month'] >= train_cut_off,:]\n",
    "total_rev_month['% error'] = pd.Series(total_rev_month.yhat)/pd.Series(total_rev_month['Total Rev'])-1\n",
    "total_rev_month[\"mape\"] = np.abs(total_rev_month[\"% error\"])\n",
    "\n",
    "first_month = total_rev_month['Year Month'].iloc[0].month\n",
    "curr_month = dt.date.today().month\n",
    "\n",
    "next_month = dt.datetime.now().replace(day=28) + dt.timedelta(days=4)\n",
    "last_day = int((next_month - dt.timedelta(days=next_month.day)).day)\n",
    "\n",
    "forecasted_days = last_day - (int(pd.to_datetime(train_cut_off).day) + 1)\n",
    "\n",
    "\n",
    "if first_month != curr_month:\n",
    "    total_rev_month = total_rev_month.drop([0], axis=0).reset_index(drop = True)\n",
    "else:\n",
    "    total_rev_month['yhat'].iloc[0] = (total_rev_month['yhat'].iloc[0]/forecasted_days) * last_day\n",
    "    total_rev_month['yhat_lower'].iloc[0] = (total_rev_month['yhat_lower'].iloc[0]/forecasted_days) * last_day\n",
    "    total_rev_month['yhat_upper'].iloc[0] = (total_rev_month['yhat_upper'].iloc[0]/forecasted_days) * last_day\n",
    "\n",
    "# Get the required columns\n",
    "ex_paids_pred = ex_paids_pred.loc[:,['ds','yhat','yhat_lower','yhat_upper','Ex Paids','mape']]\n",
    "ex_paids_week = ex_paids_week.loc[:,['ds','yhat','yhat_lower','yhat_upper','Ex Paids','mape']]\n",
    "ex_paids_month = ex_paids_month.loc[:,['ds','yhat','yhat_lower','yhat_upper','Ex Paids','mape']]\n",
    "total_rev_pred = total_rev_pred.loc[:,['ds','yhat','yhat_lower','yhat_upper','Total Rev','mape']]\n",
    "total_rev_week = total_rev_week.loc[:,['WeekDate','yhat','yhat_lower','yhat_upper','Total Rev','mape']]\n",
    "total_rev_month = total_rev_month.loc[:,['Year Month','yhat','yhat_lower','yhat_upper','Total Rev','mape']]\n",
    "\n",
    "# Rename columns\n",
    "cols = ['Date','Prediction','Prediction Lower','Prediction Upper','Actual','MAPE']\n",
    "ex_paids_pred.columns = cols\n",
    "ex_paids_week.columns = cols\n",
    "ex_paids_month.columns = cols\n",
    "total_rev_pred.columns = cols\n",
    "total_rev_week.columns = cols\n",
    "total_rev_month.columns = cols\n",
    "\n",
    "# Remove inf and nan\n",
    "ex_paids_pred = ex_paids_pred.replace([np.nan,np.inf,0],'', regex = True)\n",
    "ex_paids_week = ex_paids_week.replace([np.nan,np.inf,0],'', regex = True)\n",
    "ex_paids_month = ex_paids_month.replace([np.nan,np.inf,0],'', regex = True)\n",
    "\n",
    "total_rev_pred = total_rev_pred.replace([np.nan,np.inf,0],'', regex = True)\n",
    "total_rev_week = total_rev_week.replace([np.nan,np.inf,0],'', regex = True)\n",
    "total_rev_month = total_rev_month.replace([np.nan,np.inf,0],'', regex = True)\n",
    "\n",
    "# Output files for Table upload\n",
    "total_rev_month['Date'] = pd.to_datetime(total_rev_month['Date'].astype(str))\n",
    "weekly_output = pd.merge(ex_paids_week,total_rev_week,on = 'Date', how = 'left')\n",
    "monthly_output = pd.merge(ex_paids_month,total_rev_month,on = 'Date', how = 'left')\n",
    "\n",
    "#weekly_output.drop(['Actual_x','MAPE_x','Actual_y','MAPE_y'], axis=1, inplace = True)\n",
    "weekly_output.columns = ['Week_Starting_Date','Ex_Paids_Prediction','Ex_Paids_Pred_Low','Ex_Paids_Pred_High', 'Ex_Paids_Actual','Ex_Paids_Mape','Total_Revenue_Prediction','Total_Revenue_Pred_Low','Total_Revenue_Pred_High','Total_Revenue_Actual','Total_Revenue_Mape']\n",
    "monthly_output.columns = ['Month_Starting_Date','Ex_Paids_Prediction','Ex_Paids_Pred_Low','Ex_Paids_Pred_High', 'Ex_Paids_Actual','Ex_Paids_Mape','Total_Revenue_Prediction','Total_Revenue_Pred_Low','Total_Revenue_Pred_High','Total_Revenue_Actual','Total_Revenue_Mape']\n",
    "\n",
    "weekly_output['Refresh_Date'] = pd.to_datetime(dt.date.today())\n",
    "monthly_output['Refresh_Date'] = pd.to_datetime(dt.date.today())\n",
    "\n",
    "ex_paids_month.drop(['Actual','MAPE'], inplace = True, axis = 1)\n",
    "total_rev_month.drop(['Actual','MAPE'], inplace = True, axis = 1)\n",
    "\n",
    "#%%\n",
    "# Export output Files\n",
    "now = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "filename = \"Vudu Forecast_\"+str(now)+\".xlsx\"\n",
    "writer = pd.ExcelWriter(filename)\n",
    "ex_paids_pred.to_excel(writer,'Ex Paids Daily Prediction', index = False)\n",
    "ex_paids_week.to_excel(writer,'Ex Paids Weekly Prediction', index = False)\n",
    "ex_paids_month.to_excel(writer,'Ex Paids Monthly Prediction', index = False)\n",
    "total_rev_pred.to_excel(writer,'Tot Rev Daily Prediction', index = False)\n",
    "total_rev_week.to_excel(writer,'Tot Rev Weekly Prediction', index = False)\n",
    "total_rev_month.to_excel(writer,'Tot Rev Monthly Prediction', index = False)\n",
    "writer.save()\n",
    "\n",
    "filename = \"Weekly_Output_\"+str(now)+\".xlsx\"\n",
    "weekly_output.to_excel(filename, index = False)\n",
    "filename = \"Monthly_Output_\"+str(now)+\".xlsx\"\n",
    "monthly_output.to_excel(filename, index = False)\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# Upload Forecasts to Redshift tables\n",
    "# Upload weekly prediction output\n",
    "# Insert Records into table\n",
    "def _execute_insert(self, conn, keys, data_iter):\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "    conn.execute(self.table.insert().values(data))\n",
    "SQLTable._execute_insert = _execute_insert\n",
    "conn = create_engine('postgresql://' + user_name + ':' + password + '@' + server + ':' + port + '/' + db_name)\n",
    "\n",
    "# give the table name alone below.. schema should be given separately afterward\n",
    "import time\n",
    "START_COLUMN = 0\n",
    "END_COLUMN = len(weekly_output)\n",
    "BATCH_LIMIT = 500\n",
    "\n",
    "for x in range(START_COLUMN, END_COLUMN, BATCH_LIMIT):\n",
    "    print(\"Batch : \" , x)\n",
    "    start_time = time.time()\n",
    "    data_df_sub = weekly_output.iloc[x: x + BATCH_LIMIT]\n",
    "    data_df_sub.to_sql('lv_weekly_revenue_forecast',conn, schema = 'analytics_manual',index = False, if_exists = 'append',chunksize = BATCH_LIMIT)\n",
    "    print(\"Completed\", x, len(data_df_sub))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Upload monthly prediction output\n",
    "# Insert Records into table\n",
    "def _execute_insert(self, conn, keys, data_iter):\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "    conn.execute(self.table.insert().values(data))\n",
    "SQLTable._execute_insert = _execute_insert\n",
    "conn = create_engine('postgresql://' + user_name + ':' + password + '@' + server + ':' + port + '/' + db_name)\n",
    "\n",
    "# give the table name alone below.. schema should be given separately afterward\n",
    "import time\n",
    "START_COLUMN = 0\n",
    "END_COLUMN = len(monthly_output)\n",
    "BATCH_LIMIT = 500\n",
    "\n",
    "for x in range(START_COLUMN, END_COLUMN, BATCH_LIMIT):\n",
    "    print(\"Batch : \" , x)\n",
    "    start_time = time.time()\n",
    "    data_df_sub = monthly_output.iloc[x: x + BATCH_LIMIT]\n",
    "    data_df_sub.to_sql('lv_monthly_revenue_forecast',conn, schema = 'analytics_manual',index = False, if_exists = 'append',chunksize = BATCH_LIMIT)\n",
    "    print(\"Completed\", x, len(data_df_sub))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c1bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
