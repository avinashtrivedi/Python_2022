{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Gg_9DpxDI6Bz"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# https://www.kaggle.com/code/ankan1998/pca-from-scratch/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20WfsvGLVkxq"
   },
   "source": [
    "# 1. implement PCA on a dataset with 1000 rows and 4 features and map it to a space with 2 features with minimum loss of information. \n",
    "\n",
    "Get the covariance of the data manually for the given data. </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6Pk2IfDXXlwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08181203  0.00047874 -0.0036502   0.0048999 ]\n",
      " [ 0.00047874  0.08269679  0.00164304  0.00180146]\n",
      " [-0.0036502   0.00164304  0.08134354 -0.00011491]\n",
      " [ 0.0048999   0.00180146 -0.00011491  0.08377326]]\n"
     ]
    }
   ],
   "source": [
    "Data = np.random.random_sample((1000,4))\n",
    "\n",
    "### CODE HERE ###\n",
    "X = Data\n",
    "avg = np.mean(X,axis=0)\n",
    "X = X-avg\n",
    "        \n",
    "covariance=np.dot(X.T,X)/(X.shape[0]-1)\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgsCJgMFXjuv"
   },
   "source": [
    "Use the code in the following cell to get the eigen vectors and values. What should we do next to finish implementing PCA? Please leave comments for each part of your code. </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FAdHxckVXkTQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09236352, -0.16763788],\n",
       "       [-0.48765862, -0.00515214],\n",
       "       [ 0.07411928,  0.10550703],\n",
       "       ...,\n",
       "       [-0.11379416,  0.39216928],\n",
       "       [-0.51900669,  0.01059154],\n",
       "       [-0.09456409, -0.3570192 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, vector = np.linalg.eig(covariance)\n",
    "######################\n",
    "### CODE HERE ###\n",
    "n = 2\n",
    "vector=vector.T\n",
    "indexs=np.argsort(value)[::-1]\n",
    "vector=vector[indexs]\n",
    "value=value[indexs]\n",
    "total = sum(value)\n",
    "variance_of_each_feature = [(i / total)*100 for i in value]\n",
    "features=vector[:n]\n",
    "np.dot(X,features.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp6JESLNMCeU"
   },
   "source": [
    "## a) Mean Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=5, random_state=0)\n",
    "# Add missing values\n",
    "X.ravel()[np.random.choice(X.size, 100, replace=False)] = np.nan\n",
    "# Train test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SR6_KDZNlfi"
   },
   "source": [
    "- There are missing values in your dataset. Replace the missing values with the mean of the feature in your **training set**.\n",
    "- Avinash use core Python, Numpy and Pandas functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAHrjqnCPB8e"
   },
   "source": [
    "**HINT**: To ignore the NaNs in the mean computation use `np.nanmean` instead of `np.mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMlG5yWoMw3t",
    "outputId": "e284890a-4e92-442d-eb84-6a353667a4d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing values in the training and the test set\n",
    "sum(np.isnan(train_x.flatten())), sum(np.isnan(test_x.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YYMwZIKgKCUt"
   },
   "outputs": [],
   "source": [
    "### CODE HERE ###\n",
    "\n",
    "col_avg = np.nanmean(train_x, axis=0)\n",
    "inds = np.where(np.isnan(train_x))\n",
    "train_x[inds] = np.take(col_avg, inds[1])\n",
    "\n",
    "col_avg = np.nanmean(test_x, axis=0)\n",
    "inds = np.where(np.isnan(test_x))\n",
    "test_x[inds] = np.take(col_avg, inds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTuYLUTjNe3g"
   },
   "source": [
    "<b> Train the model </b> \n",
    "- Just run the following cells, no need to do anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_siFNVZaNiML"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nrfdcUw-Q6Po"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(train_x, train_y)\n",
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_wIsh2hS2ut"
   },
   "source": [
    "## b) Compute Error Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IMBffLOTo9K"
   },
   "source": [
    "$$\n",
    "\\mathrm{MSE}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSTLugUqTrc8"
   },
   "source": [
    "$$\n",
    "\\mathrm{MAE}=\\frac{1}{n} \\sum_{i=1}^{n} \\mid y_{i}-\\hat{y}_{i} \\mid\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hlGS2X6QRxXZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 192.92387482566934\n",
      "MAE 4.028447096113364\n"
     ]
    }
   ],
   "source": [
    "### CODE HERE ###\n",
    "MSE = np.square(np.subtract(test_y,y_pred)).mean()\n",
    "MAE = abs(np.subtract(test_y,y_pred)).mean()\n",
    "\n",
    "print('MSE',MSE)\n",
    "print('MAE',MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `Iris` dataset using the below code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, Y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this question, you need to apply 3-fold cross-validation on the IRIS dataset.**\n",
    "    \n",
    "**You CANNOT use any package that provides you a fast CV; \n",
    "instead, you have to implement it by yourself from scratch. \n",
    "You need to take all the steps related to CV implementation. \n",
    "What we expect to see at the end includes:**\n",
    "\n",
    "* implementing the 3-fold CV correctly\n",
    "\n",
    "* applying a decision tree classifier on each fold\n",
    "\n",
    "* printing the ``precision`` of the model on each of the three folds. \n",
    "\n",
    "* reporting the ``mean`` and ``variance`` of the ``precision`` for these folds.\n",
    "\n",
    "**Hints**: Using the below code, you can implement Decision Tree on any given dataset:\n",
    "```python\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "```\n",
    "and if you use ``predict`` method, then you can see the predicted values given the trained model. \n",
    "```python\n",
    "clf.predict(X_test)\n",
    "```\n",
    "<a href=\"https://scikit-learn.org/stable/modules/tree.html\">[Source for help on DT]</a> No need to manually create your DT. Simply use the above package for Decision Tree.\n",
    "\n",
    "For `precision` caluclation, you can use scikit-learn as well:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_true, y_pred, average='weighted')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of model on fold-1 1.0\n",
      "Precision of model on fold-2 0.9215686274509803\n",
      "Precision of model on fold-3 0.9814814814814815\n",
      "----------------------------------------\n",
      "Mean: 0.9676833696441539\n",
      "variance: 0.0011204406451248855\n"
     ]
    }
   ],
   "source": [
    "### CODE HERE ###\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import tree\n",
    "\n",
    "class0_indx = np.where(Y==0)\n",
    "class1_indx = np.where(Y==1)\n",
    "class2_indx = np.where(Y==2)\n",
    "\n",
    "n = int(np.ceil(len(class0_indx[0])/3))\n",
    "\n",
    "folds_indx = []\n",
    "start = 0\n",
    "end = len(class0_indx[0])\n",
    "i = n\n",
    "while start<end:\n",
    "    arr1 = class0_indx[0][start:i]\n",
    "    arr2 = class1_indx[0][start:i]\n",
    "    arr3 = class2_indx[0][start:i]\n",
    "    indx = np.concatenate([arr1,arr2,arr3])\n",
    "    folds_indx.append(indx)\n",
    "    \n",
    "    start = i\n",
    "    i = i + n\n",
    "    \n",
    "#folds                                                              \n",
    "fold1 = folds_indx[0]                                          \n",
    "fold2 = folds_indx[1]\n",
    "fold3 = folds_indx[2]\n",
    "\n",
    "train1 = np.concatenate([fold1,fold2])\n",
    "test1 = fold3\n",
    "\n",
    "train2 = np.concatenate([fold1,fold3])\n",
    "test2 = fold2\n",
    "\n",
    "train3 = np.concatenate([fold2,fold3])\n",
    "test3 = fold1\n",
    "\n",
    "CV_Folds = [(train1,test1),(train2,test2),(train3,test3)]\n",
    "\n",
    "stored_precision = []\n",
    "for i,(train_index, test_index) in enumerate(CV_Folds):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
    "    stored_precision.append(precision)\n",
    "    print(f'Precision of model on fold-{i+1}',precision)\n",
    "    \n",
    "print('----------------------------------------')\n",
    "print('Mean:',np.array(stored_precision).mean())\n",
    "print('variance:',np.array(stored_precision).var())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Makeup Question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
