{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4135d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, substring, col, when, desc, min, max, sum, \\\n",
    "split, udf, count, countDistinct, rand, struct, isnan, isnull, from_unixtime, dayofmonth, month, datediff, to_date, lit, current_date\n",
    "from pyspark.sql.types import IntegerType, ArrayType, FloatType, DoubleType, Row, DateType\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b9b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|          artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|  Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|           Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|              Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "|    Adam Lambert|Logged In|    Colin|     M|           51| Freeman| 282.8273| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|   Time For Miracles|   200|1538352394000|Mozilla/5.0 (Wind...|    30|\n",
      "|          Enigma|Logged In|    Micah|     M|           80|    Long|262.71302| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|Knocking On Forbi...|   200|1538352416000|\"Mozilla/5.0 (Win...|     9|\n",
      "|       Daft Punk|Logged In|    Colin|     M|           52| Freeman|223.60771| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Harder Better Fas...|   200|1538352676000|Mozilla/5.0 (Wind...|    30|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the spark session and load the dataframe\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Churn Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = \"D:\\OneDrive - NITT\\Custom_Download\\mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(path)\n",
    "df.persist()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30e075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a000cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId| song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "| 58392|   0|     8346|  8346|            0|    8346| 58392|    0|    8346|     0|   0|        8346|        0|58392|     0|  0|     8346|     0|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+-----+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465a6e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|artist|      auth|firstName|gender|itemInSession|lastName|length|level|location|method| page|registration|sessionId|song|status|           ts|userAgent|userId|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "|  null|Logged Out|     null|  null|          100|    null|  null| free|    null|   GET| Home|        null|        8|null|   200|1538355745000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          101|    null|  null| free|    null|   GET| Help|        null|        8|null|   200|1538355807000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          102|    null|  null| free|    null|   GET| Home|        null|        8|null|   200|1538355841000|     null|      |\n",
      "|  null|Logged Out|     null|  null|          103|    null|  null| free|    null|   PUT|Login|        null|        8|null|   307|1538355842000|     null|      |\n",
      "|  null|Logged Out|     null|  null|            2|    null|  null| free|    null|   GET| Home|        null|      240|null|   200|1538356678000|     null|      |\n",
      "|  null|Logged Out|     null|  null|            3|    null|  null| free|    null|   PUT|Login|        null|      240|null|   307|1538356679000|     null|      |\n",
      "|  null|Logged Out|     null|  null|            0|    null|  null| free|    null|   PUT|Login|        null|      100|null|   307|1538358102000|     null|      |\n",
      "|  null|Logged Out|     null|  null|            0|    null|  null| free|    null|   PUT|Login|        null|      241|null|   307|1538360117000|     null|      |\n",
      "|  null|Logged Out|     null|  null|           14|    null|  null| free|    null|   GET| Home|        null|      187|null|   200|1538361527000|     null|      |\n",
      "|  null|Logged Out|     null|  null|           15|    null|  null| free|    null|   PUT|Login|        null|      187|null|   307|1538361528000|     null|      |\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('firstName').isNull()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce935a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|artist|      auth|firstName|gender|itemInSession|lastName|length|level|            location|method|           page| registration|sessionId|song|status|           ts|           userAgent|userId|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|  null| Logged In|    Colin|     M|           54| Freeman|  null| paid|     Bakersfield, CA|   PUT|Add to Playlist|1538173362000|       29|null|   200|1538352905000|Mozilla/5.0 (Wind...|    30|\n",
      "|  null| Logged In|    Micah|     M|           84|    Long|  null| free|Boston-Cambridge-...|   GET|    Roll Advert|1538331630000|        8|null|   200|1538353150000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null| Logged In|    Micah|     M|           86|    Long|  null| free|Boston-Cambridge-...|   PUT|      Thumbs Up|1538331630000|        8|null|   307|1538353376000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null| Logged In|    Alexi|     F|            4|  Warren|  null| paid|Spokane-Spokane V...|   GET|      Downgrade|1532482662000|       53|null|   200|1538354749000|Mozilla/5.0 (Wind...|    54|\n",
      "|  null| Logged In|    Alexi|     F|            7|  Warren|  null| paid|Spokane-Spokane V...|   PUT|      Thumbs Up|1532482662000|       53|null|   307|1538355255000|Mozilla/5.0 (Wind...|    54|\n",
      "|  null| Logged In|    Micah|     M|           95|    Long|  null| free|Boston-Cambridge-...|   PUT|    Thumbs Down|1538331630000|        8|null|   307|1538355306000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null| Logged In|    Micah|     M|           97|    Long|  null| free|Boston-Cambridge-...|   GET|           Home|1538331630000|        8|null|   200|1538355504000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null| Logged In|    Micah|     M|           99|    Long|  null| free|Boston-Cambridge-...|   PUT|         Logout|1538331630000|        8|null|   307|1538355687000|\"Mozilla/5.0 (Win...|     9|\n",
      "|  null| Logged In|  Ashlynn|     F|            9|Williams|  null| free|     Tallahassee, FL|   PUT|      Thumbs Up|1537365219000|      217|null|   307|1538355711000|\"Mozilla/5.0 (Mac...|    74|\n",
      "|  null|Logged Out|     null|  null|          100|    null|  null| free|                null|   GET|           Home|         null|        8|null|   200|1538355745000|                null|      |\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------------------+------+---------------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('artist').isNull()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c89f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(col('auth') != 'Logged Out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6234be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9412544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|          artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|  Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|           Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|              Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "|    Adam Lambert|Logged In|    Colin|     M|           51| Freeman| 282.8273| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|   Time For Miracles|   200|1538352394000|Mozilla/5.0 (Wind...|    30|\n",
      "|          Enigma|Logged In|    Micah|     M|           80|    Long|262.71302| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|Knocking On Forbi...|   200|1538352416000|\"Mozilla/5.0 (Win...|     9|\n",
      "|       Daft Punk|Logged In|    Colin|     M|           52| Freeman|223.60771| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Harder Better Fas...|   200|1538352676000|Mozilla/5.0 (Wind...|    30|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c48ce419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Page| count|\n",
      "+--------------------+------+\n",
      "|              Cancel|    52|\n",
      "|    Submit Downgrade|    63|\n",
      "|         Thumbs Down|  2546|\n",
      "|                Home| 10118|\n",
      "|           Downgrade|  2055|\n",
      "|         Roll Advert|  3933|\n",
      "|              Logout|  3226|\n",
      "|       Save Settings|   310|\n",
      "|Cancellation Conf...|    52|\n",
      "|               About|   509|\n",
      "|            Settings|  1514|\n",
      "|     Add to Playlist|  6526|\n",
      "|          Add Friend|  4277|\n",
      "|            NextSong|228108|\n",
      "|           Thumbs Up| 12551|\n",
      "|                Help|  1477|\n",
      "|             Upgrade|   499|\n",
      "|               Error|   253|\n",
      "|      Submit Upgrade|   159|\n",
      "| Submit Registration|     5|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Page').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e1e901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|nb_users|\n",
      "+--------+\n",
      "|     226|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(DISTINCT userId) AS nb_users FROM logs').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedae6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataset with all user ID's and a label which specifies whether that individual churned or not\n",
    "\n",
    "# First a dataset with users who churned\n",
    "churn = spark.sql('SELECT DISTINCT userId, 1 as churn FROM logs WHERE Page = \"Cancellation Confirmation\"')\n",
    "\n",
    "# Then the other users\n",
    "no_churn = spark.sql('SELECT DISTINCT userId, 0 as churn FROM logs \\\n",
    "                      WHERE userId NOT IN (SELECT DISTINCT userId FROM logs WHERE Page = \"Cancellation Confirmation\")')\n",
    "\n",
    "# Create a dataframe from the union of the two, shuffling the rows\n",
    "churn_df = churn.union(no_churn)\n",
    "churn_df.createOrReplaceTempView('churn')\n",
    "churn_df = spark.sql('SELECT * FROM churn ORDER BY RAND()')\n",
    "churn_df.createOrReplaceTempView('churn')\n",
    "\n",
    "# Check that we have the right number of users\n",
    "churn_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5c8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  userId\n",
       "0      0     174\n",
       "1      1      52"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the distribution of users between churning or not\n",
    "churn_pd_df = churn_df.toPandas()\n",
    "churn_pd_df.groupby('churn', as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584960f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|churn|level| count|\n",
      "+-----+-----+------+\n",
      "|    0| free| 43430|\n",
      "|    0| paid|189957|\n",
      "|    1| paid| 32476|\n",
      "|    1| free| 12388|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join our churn dataset to our full dataset to add a column\n",
    "joined_df = df.join(churn_df, on='userId')\n",
    "\n",
    "# Use this column to create some aggregates, first comparing the level of the account\n",
    "joined_df.groupBy(['churn', 'level']).count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8841b81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a data visualization: a grouped bar chart of proportions\n",
    "level_df = joined_df.groupBy(['churn', 'level']).count().toPandas()\n",
    "churn_sum = level_df.groupby('churn', as_index=False).sum()\n",
    "level_df['proportion'] = (level_df.join(churn_sum, on = 'churn', lsuffix='_1')['count_1']/level_df.join(churn_sum, on = 'churn', lsuffix='_1')['count'])*100\n",
    "\n",
    "g = sns.barplot(x=\"churn\", y=\"proportion\", hue=\"level\", data=level_df,\n",
    "                palette=\"muted\")\n",
    "g.set(xlabel='Churn', ylabel=\"Tier Proportion (in %)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c460d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|churn|       avg(length)|\n",
      "+-----+------------------+\n",
      "|    1| 248.6327956440638|\n",
      "|    0|249.20913538881527|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.groupBy('churn').avg('length').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ead5609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the average length for every user, keeping the churn information and transform to a Pandas dataframe\n",
    "length_df = joined_df.groupBy(['userId', 'churn']).avg('length').toPandas()\n",
    "\n",
    "# visualize the distribution of length between two groups of users\n",
    "g = sns.FacetGrid(length_df, col=\"churn\", sharey=False)\n",
    "g.map(plt.hist, \"avg(length)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecfd53b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|churn|avg(itemInSession)|\n",
      "+-----+------------------+\n",
      "|    1|109.23299304564907|\n",
      "|    0|115.94533542999396|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The itemInSession variable, which represents the rank fo the song in the current sessionk\n",
    "joined_df.groupBy('churn').avg('itemInSession').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf0c243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Similar to length, we visualize the distribution between both groups\n",
    "item_df = joined_df.groupBy(['userId','churn']).avg('itemInSession').toPandas()\n",
    "g = sns.FacetGrid(item_df, col=\"churn\", sharey=False)\n",
    "g.map(plt.hist, \"avg(itemInSession)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d590061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|churn|state|count|\n",
      "+-----+-----+-----+\n",
      "|    0|   CA|39158|\n",
      "|    0|   PA|23708|\n",
      "|    0|   TX|22200|\n",
      "|    0|   NH|18637|\n",
      "|    0|   FL|11427|\n",
      "+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-----+-----+\n",
      "|churn|state|count|\n",
      "+-----+-----+-----+\n",
      "|    1|   CA| 7613|\n",
      "|    1|   CO| 4317|\n",
      "|    1|   MS| 3839|\n",
      "|    1|   WA| 3526|\n",
      "|    1|   OH| 3173|\n",
      "+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create a variable representing the state\n",
    "joined_df = joined_df.withColumn('state', substring(col(\"location\"), -2, 3))\n",
    "\n",
    "# See the most present states for people who churned \n",
    "joined_df.groupBy(['churn', 'state']).count().where(joined_df['churn'] == 0).sort(col('count').desc()).show(5)\n",
    "joined_df.groupBy(['churn', 'state']).count().where(joined_df['churn'] == 1).sort(col('count').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "827c2856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|churn|             avg(ts)|\n",
      "+-----+--------------------+\n",
      "|    1|1.539919263874465E12|\n",
      "|    0|1.541159010797756...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Study potential differences based on ts\n",
    "joined_df.groupBy('churn').avg('ts').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9d114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73908f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ts column is actually a unix timestamp, which we convert to a date format\n",
    "joined_df = joined_df.withColumn('date', from_unixtime((col('ts')/1000)).cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f6a1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once this is done, we can create several time based features\n",
    "# first, extract day and month from the date column\n",
    "joined_df = joined_df.withColumn('day', dayofmonth('date')).withColumn('month', month('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce3aaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = joined_df.groupBy(['userId', 'day']).agg(countDistinct('sessionId')).groupBy('userId').avg('count(sessionId)').withColumnRenamed('avg(count(sessionId))', 'daily_sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ded831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df = joined_df.groupBy(['userId', 'month']).agg(countDistinct('sessionId')).groupBy('userId').avg('count(sessionId)').withColumnRenamed('avg(count(sessionId))', 'monthly_sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a9888ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then for a given user get the average number of distinct sessions per day and per month\n",
    "\n",
    "# join this information back to the original joined_df for each row for a given userId, it will be used later\n",
    "joined_df = joined_df.join(day_df, on='userId').join(month_df, on='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32cc6d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId',\n",
       " 'artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'churn',\n",
       " 'state',\n",
       " 'date',\n",
       " 'day',\n",
       " 'month',\n",
       " 'daily_sessions',\n",
       " 'monthly_sessions']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eec4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|churn|avg(daily_sessions)|\n",
      "+-----+-------------------+\n",
      "|    1| 1.3967687741695773|\n",
      "|    0| 1.7275042165377033|\n",
      "+-----+-------------------+\n",
      "\n",
      "+-----+---------------------+\n",
      "|churn|avg(monthly_sessions)|\n",
      "+-----+---------------------+\n",
      "|    1|   10.477721558487875|\n",
      "|    0|   12.582251082249577|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare number of daily and monthly sessions for the two groups of users\n",
    "joined_df.groupBy('churn').avg('daily_sessions').show()\n",
    "joined_df.groupBy('churn').avg('monthly_sessions').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51ca81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# registration is also unix timestamp, representing the registration date. Convert them to an actual date\n",
    "joined_df = joined_df.withColumn('registration_date', from_unixtime((col('registration')/1000)).cast(DateType()))\n",
    "\n",
    "# create a new feature capturing the number of days since registration\n",
    "joined_df = joined_df.withColumn('days_since_registration', datediff(current_date(), col('registration_date')))\n",
    "\n",
    "# create a new feature capturing the month of the year when they registered, to account for potential promotions\n",
    "joined_df = joined_df.withColumn('month_registration', month('registration_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b56c4328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+\n",
      "|churn|           userAgent|count|\n",
      "+-----+--------------------+-----+\n",
      "|    0|\"Mozilla/5.0 (Win...|18226|\n",
      "|    0|\"Mozilla/5.0 (Mac...|16298|\n",
      "|    0|\"Mozilla/5.0 (Mac...|15914|\n",
      "|    0|\"Mozilla/5.0 (Win...|15237|\n",
      "|    0|Mozilla/5.0 (Wind...|15224|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|churn|           userAgent|count|\n",
      "+-----+--------------------+-----+\n",
      "|    1|\"Mozilla/5.0 (Mac...| 4736|\n",
      "|    1|\"Mozilla/5.0 (Win...| 4525|\n",
      "|    1|Mozilla/5.0 (Wind...| 3437|\n",
      "|    1|\"Mozilla/5.0 (Mac...| 2534|\n",
      "|    1|Mozilla/5.0 (Maci...| 2462|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Study potential differences based on user agent\n",
    "joined_df.groupBy(['churn', 'userAgent']).count().where(joined_df['churn'] == 0).sort(col('count').desc()).show(5)\n",
    "joined_df.groupBy(['churn', 'userAgent']).count().where(joined_df['churn'] == 1).sort(col('count').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b45d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given user we compute number of downgrades, upgrades, thumbs up, thumbs down, add friend, add to playlist, roll advert\n",
    "\n",
    "# Create a column for each of these events, with a 1 each time they occur\n",
    "for c in ['Downgrade', 'Roll Advert', 'Thumbs Down', 'Add to Playlist', 'Add Friend', 'Thumbs Up']:\n",
    "    joined_df = joined_df.withColumn(c, (joined_df['Page'] == c).cast(IntegerType()))\n",
    "    \n",
    "# Prepare data for the pipeline\n",
    "features_df = joined_df.groupBy('userId').agg(avg('itemInSession'), \n",
    "                                              avg('length'),\n",
    "                                              min('daily_sessions'),\n",
    "                                              min('monthly_sessions'),\n",
    "                                              min('days_since_registration'),\n",
    "                                              min('month_registration'),\n",
    "                                            max('level'), \n",
    "                                            max('userAgent'),\n",
    "                                           max('state'),\n",
    "                                           sum('Downgrade'),\n",
    "                                           sum('Roll Advert'),\n",
    "                                           sum('Thumbs Down'),\n",
    "                                           sum('Add to Playlist'),\n",
    "                                           sum('Add Friend'),\n",
    "                                           sum('Thumbs Up'),\n",
    "                                           max('churn'))\n",
    "\n",
    "features_df = features_df.select(col('avg(itemInSession)').alias('itemInSession'),\n",
    "                                 col('avg(length)').alias('length'),\n",
    "                                 col('min(daily_sessions)').alias('daily_sessions'),\n",
    "                                 col('min(monthly_sessions)').alias('monthly_sessions'),\n",
    "                                 col('min(days_since_registration)').alias('days_since_registration'),\n",
    "                                 col('min(month_registration)').alias('month_registration'),\n",
    "                  col('max(level)').alias('level'),\n",
    "                  col('max(userAgent)').alias('userAgent'),\n",
    "                  col('max(state)').alias('state'),\n",
    "                  col('sum(Downgrade)').alias('downgrade'),\n",
    "                  col('sum(Roll Advert)').alias('rollAdvert'),\n",
    "                  col('sum(Thumbs Down)').alias('thumbsDown'),\n",
    "                  col('sum(Add to Playlist)').alias('addToPlaylist'),\n",
    "                  col('sum(Add Friend)').alias('addFriend'),\n",
    "                  col('sum(Thumbs Up)').alias('thumbsUp'),\n",
    "                  col('max(churn)').alias('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d10da5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------+----------------+-----------------------+------------------+-----+---------+-----+---------+----------+----------+-------------+---------+--------+-----+\n",
      "|itemInSession|length|daily_sessions|monthly_sessions|days_since_registration|month_registration|level|userAgent|state|downgrade|rollAdvert|thumbsDown|addToPlaylist|addFriend|thumbsUp|label|\n",
      "+-------------+------+--------------+----------------+-----------------------+------------------+-----+---------+-----+---------+----------+----------+-------------+---------+--------+-----+\n",
      "|            0|     1|             0|               0|                      1|                 1|    0|        1|    1|        0|         0|         0|            0|        0|       0|    0|\n",
      "+-------------+------+--------------+----------------+-----------------------+------------------+-----+---------+-----+---------+----------+----------+-------------+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize the presence of potential null values\n",
    "features_df.select([count(when(isnull(c), c)).alias(c) for c in features_df.columns]).show()\n",
    "\n",
    "# Remove these null values\n",
    "features_df = features_df.where(col(\"userAgent\").isNotNull()).where(col(\"state\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93c3d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create a pipeline to prepare our data for modeling\n",
    "indexer_state = StringIndexer(inputCol='state', outputCol='state_index')\n",
    "indexer_level = StringIndexer(inputCol='level', outputCol='level_index')\n",
    "indexer_ua = StringIndexer(inputCol='userAgent', outputCol='userAgent_index')\n",
    "assembler = VectorAssembler(inputCols=['itemInSession', \n",
    "                                       'length',\n",
    "                                       'daily_sessions',\n",
    "                                       'monthly_sessions',\n",
    "                                       'days_since_registration',\n",
    "                                       'month_registration',\n",
    "                                       'level_index', \n",
    "                                       'state_index',\n",
    "                                       'userAgent_index',\n",
    "                                       'downgrade',\n",
    "                                       'rollAdvert',\n",
    "                                       'thumbsDown',\n",
    "                                       'addToPlaylist',\n",
    "                                       'addFriend',\n",
    "                                       'thumbsUp'],\n",
    "                           outputCol='features')\n",
    "features_pipeline = Pipeline(stages=[indexer_state, indexer_level, indexer_ua, assembler])\n",
    "\n",
    "model_df = features_pipeline.fit(features_df).transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c4d5088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the structure of data, we need to convert some features vector from Sparse to Dense\n",
    "df2 = model_df.select(\"label\", \"features\")\n",
    "rdd = df2.rdd.map(lambda x: Row(label=x[0],features=DenseVector(x[1].toArray()))\n",
    "                     if (len(x)>1 and hasattr(x[1], \"toArray\"))\n",
    "                     else Row(label=None, features=DenseVector([])))\n",
    "# model_data = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a630a5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1336] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7e401f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 513.0 failed 1 times, most recent failure: Lost task 0.0 in stage 513.0 (TID 1406) (DESKTOP-JID9S04 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misEmpty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1923\u001b[0m, in \u001b[0;36mRDD.isEmpty\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misEmpty\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1909\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;124;03m    Returns true if and only if the RDD contains no elements at all.\u001b[39;00m\n\u001b[0;32m   1911\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetNumPartitions() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 513.0 failed 1 times, most recent failure: Lost task 0.0 in stage 513.0 (TID 1406) (DESKTOP-JID9S04 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab8d46bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 509.0 failed 1 times, most recent failure: Lost task 0.0 in stage 509.0 (TID 1405) (DESKTOP-JID9S04 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[1;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    527\u001b[0m     rdd: RDD[Any],\n\u001b[0;32m    528\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    529\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    530\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1891\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1903\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 509.0 failed 1 times, most recent failure: Lost task 0.0 in stage 509.0 (TID 1405) (DESKTOP-JID9S04 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d9fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f043ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58c1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2fd90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train and estimate a model\n",
    "def fit_estimate(train, test, model):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    train (Spark df): a Spark data frame with training data\n",
    "    test (Spark df): a Spark data frame with testing data\n",
    "    model (string): a string designating one of the models to fit, either 'logistic_regression', 'random_forest' \n",
    "                    or 'gradient_boosting'\n",
    "    \n",
    "    OUTPUT:\n",
    "    None, prints out accuracy for the model\n",
    "    \n",
    "    DESCRIPTION:\n",
    "    Fits and estimates different classification models with the default parameters\n",
    "    '''\n",
    "    # Choose the model\n",
    "    if model == 'logistic_regression':\n",
    "        ml = LogisticRegression()\n",
    "    elif model == 'random_forest':\n",
    "        ml = RandomForestClassifier()\n",
    "    elif model == 'gradient_boosting':\n",
    "        ml = GBTClassifier()\n",
    "    else:\n",
    "        return \"Please choose an appropriate model\"\n",
    "    \n",
    "    # Fit and calculate predictions\n",
    "    classification = ml.fit(train)\n",
    "    results = classification.transform(test)\n",
    "    \n",
    "    # Calculate accuracy and F-1 score\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "    accuracy = accuracy_evaluator.evaluate(results.select(col('label'), col('prediction')))\n",
    "    \n",
    "    f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "    f1_score = f1_score_evaluator.evaluate(results.select(col('label'), col('prediction')))\n",
    "    \n",
    "    print('For {}, the accuracy on the test set is {:.2%} and the F-1 score is {}'\\\n",
    "    .format(model, accuracy, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split this data between train, validation and test sets\n",
    "train, test = model_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Due to the class imbalance, we upsample the categories who churned in the training dataset\n",
    "print('In our training set, before upsampling we have {} users who churned and {} who did not.'.format(train.where(col('label') == 1).count(),\n",
    "                                                                                           train.where(col('label') == 0).count()))\n",
    "\n",
    "train_churn = train.where(col('label') == 1).sample(True, train.where(col('label') == 0).count()/train.where(col('label') == 1).count())\n",
    "train_no_churn = train.where(col('label') == 0)\n",
    "train = train_churn.unionAll(train_no_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe465bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit various models and visualize their accuracies\n",
    "for model in ['logistic_regression', 'random_forest', 'gradient_boosting']:\n",
    "    fit_estimate(train, test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize a Random Forest model through Grid Search\n",
    "model = RandomForestClassifier()\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(model.minInfoGain, [0, 1]) \\\n",
    "    .addGrid(model.numTrees, [20, 50]) \\\n",
    "    .addGrid(model.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator = Pipeline(stages=[model]),\n",
    "                         estimatorParamMaps = param_grid,\n",
    "                         evaluator = MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                         numFolds = 3)\n",
    "\n",
    "classification = crossval.fit(train)\n",
    "results = classification.transform(test)\n",
    "\n",
    "# Calculate accuracy and F-1 score\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(results.select(col('label'), col('prediction')))\n",
    "\n",
    "f1_score_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "f1_score = f1_score_evaluator.evaluate(results.select(col('label'), col('prediction')))\n",
    "\n",
    "print('For {}, the accuracy on the test set is {:.2%} and the F-1 score is {}'\\\n",
    ".format(model, accuracy, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006fc2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine feature importance\n",
    "DenseVector(classification.bestModel.stages[-1].featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a6a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
