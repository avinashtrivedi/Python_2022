{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacymoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacymoji.Emoji at 0x20063f87dc0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "emoji = Emoji(nlp)\n",
    "nlp.add_pipe('emoji', first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame()\n",
    "testing_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDF(Tweet_folder, y):\n",
    "    result = pd.DataFrame()\n",
    "    # Define relative path to folder containing the text files\n",
    "#     Tweet_folder = \"tweet/train/positive\"\n",
    "    files = []\n",
    "\n",
    "    # Create a dataframe list by using a list comprehension\n",
    "    files = [pd.read_csv(file, names =['Tweet', 'Class']) for file in glob.glob(os.path.join(Tweet_folder ,\"*.txt\"))]\n",
    "\n",
    "    # Concatenate the list of DataFrames into one\n",
    "    result = pd.concat(files)\n",
    "    result['Class'] = y\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pos = pd.DataFrame()\n",
    "training_pos = createDF('tweet/train/positive', '+')\n",
    "\n",
    "training_neg = pd.DataFrame()\n",
    "training_neg = createDF('tweet/train/negative', '-')\n",
    "\n",
    "training_df = pd.concat([training_pos, training_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir I would appreciate that.  Thank ...</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USAirways thank you very much.</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JetBlue I'm all set. About to fly. Not bad fo...</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir I got a flight at 11:55am on Thu...</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AmericanAir you're my early frontrunner for b...</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AmericanAir you delayed me for 15 hours in Ch...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why even ask me to DM you and offer help if yo...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USAirways are you going to do anything to hel...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united you're terrible.</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USAirways so I still need to stay on hold? ht...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet Class\n",
       "0   @SouthwestAir I would appreciate that.  Thank ...     +\n",
       "0                     @USAirways thank you very much.     +\n",
       "0   @JetBlue I'm all set. About to fly. Not bad fo...     +\n",
       "0   @SouthwestAir I got a flight at 11:55am on Thu...     +\n",
       "0   @AmericanAir you're my early frontrunner for b...     +\n",
       "..                                                ...   ...\n",
       "0   @AmericanAir you delayed me for 15 hours in Ch...     -\n",
       "0   Why even ask me to DM you and offer help if yo...     -\n",
       "0   @USAirways are you going to do anything to hel...     -\n",
       "0                            @united you're terrible.     -\n",
       "0   @USAirways so I still need to stay on hold? ht...     -\n",
       "\n",
       "[4232 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Create your Vocabulary: Read the complete training data word by word and create the vocabulary V for the corpus. \n",
    "#     -- You must not include the test set in this process. \n",
    "#     -- Remove any markup tags, e.g., HTML tags, from the data. \n",
    "#     -- Lower case capitalized words (i.e., starts with a capital letter) but not all capital words (e.g., USA). \n",
    "#     -- Keep all stop words. \n",
    "#     -- Create 2 versions of V: with stemming and without stemming. You can use appropriate tools in nltk to stem. \n",
    "#     -- Tokenize at white space and also at each punctuation. In other words, “child’s” consists of two tokens “child and ‘s”, “home.” consists of two tokens “home” and “.”. \n",
    "#     -- Consider emoticons in this process. You can use an emoticon tokenizer, if you so choose. If yes, specify which one.\n",
    "        \n",
    "# def preprocess(df):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing links\n",
    "# training_df['Tweet'] = training_df['Tweet'].str.replace(r'(http[\\:\\/A-Za-z\\.0-9]+)', '', regex=True)\n",
    "\n",
    "#tokenize\n",
    "# training_df['Tweet'] = training_df['Tweet'].str.split(\" \")\n",
    "\n",
    "\n",
    "#Vocab: stemmed and not stemmed\n",
    "\n",
    "#lower case: not all capital words\n",
    "# training_df['Tweet'] = training_df['Tweet'].str.lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['Tweet'] = training_df['Tweet'].str.replace(r'(http[\\:\\/A-Za-z\\.0-9]+)', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Tweet, Class]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df[training_df['Tweet'].apply(lambda x: False if x.find('http')==-1 else True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(txt):\n",
    "    txt_list = txt.split()\n",
    "    return ' '.join([i.lower() if not i.isupper() or len(i)==1 else i for i in txt_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['Tweet'] = training_df['Tweet'].apply(lambda txt: lower(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @southwestair i would appreciate that. thank you.\n",
       "0                      @usairways thank you very much.\n",
       "0    @jetblue i'm all set. about to fly. not bad fo...\n",
       "0    @southwestair i got a flight at 11:55am on thu...\n",
       "0    @americanair you're my early frontrunner for b...\n",
       "                           ...                        \n",
       "0    @americanair you delayed me for 15 hours in ch...\n",
       "0    why even ask me to DM you and offer help if yo...\n",
       "0    @usairways are you going to do anything to hel...\n",
       "0                             @united you're terrible.\n",
       "0          @usairways so i still need to stay on hold?\n",
       "Name: Tweet, Length: 4232, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "# sentence = \"Programmers program with programming languages\"\n",
    "# words = word_tokenize(sentence)\n",
    "\n",
    "# for w in words:\n",
    "# print(w, \" : \", ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    return [token.text for token in nlp(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['Tweet'] = training_df['Tweet'].apply(lambda txt: tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(lst_token):\n",
    "    return [ps.stem(token) for token in lst_token]\n",
    "    \n",
    "training_df['Stemmed_Tweet'] = training_df['Tweet'].apply(lambda txt: stemmer(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [@southwestair, i, would, appreciate, that, .,...\n",
       "0              [@usairways, thank, you, very, much, .]\n",
       "0    [@jetblue, i, 'm, all, set, ., about, to, fly,...\n",
       "0    [@southwestair, i, got, a, flight, at, 11:55am...\n",
       "0    [@americanair, you, 're, my, early, frontrunne...\n",
       "                           ...                        \n",
       "0    [@americanair, you, delayed, me, for, 15, hour...\n",
       "0    [why, even, ask, me, to, DM, you, and, offer, ...\n",
       "0    [@usairways, are, you, going, to, do, anything...\n",
       "0                     [@united, you, 're, terrible, .]\n",
       "0    [@usairways, so, i, still, need, to, stay, on,...\n",
       "Name: Tweet, Length: 4232, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "      <th>Stemmed_Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@southwestair, i, would, appreciate, that, .,...</td>\n",
       "      <td>+</td>\n",
       "      <td>[@southwestair, i, would, appreci, that, ., th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@usairways, thank, you, very, much, .]</td>\n",
       "      <td>+</td>\n",
       "      <td>[@usairway, thank, you, veri, much, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@jetblue, i, 'm, all, set, ., about, to, fly,...</td>\n",
       "      <td>+</td>\n",
       "      <td>[@jetblu, i, 'm, all, set, ., about, to, fli, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@southwestair, i, got, a, flight, at, 11:55am...</td>\n",
       "      <td>+</td>\n",
       "      <td>[@southwestair, i, got, a, flight, at, 11:55am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@americanair, you, 're, my, early, frontrunne...</td>\n",
       "      <td>+</td>\n",
       "      <td>[@americanair, you, 're, my, earli, frontrunn,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@americanair, you, delayed, me, for, 15, hour...</td>\n",
       "      <td>-</td>\n",
       "      <td>[@americanair, you, delay, me, for, 15, hour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[why, even, ask, me, to, DM, you, and, offer, ...</td>\n",
       "      <td>-</td>\n",
       "      <td>[whi, even, ask, me, to, DM, you, and, offer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@usairways, are, you, going, to, do, anything...</td>\n",
       "      <td>-</td>\n",
       "      <td>[@usairway, are, you, go, to, do, anyth, to, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@united, you, 're, terrible, .]</td>\n",
       "      <td>-</td>\n",
       "      <td>[@unit, you, 're, terribl, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@usairways, so, i, still, need, to, stay, on,...</td>\n",
       "      <td>-</td>\n",
       "      <td>[@usairway, so, i, still, need, to, stay, on, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4232 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet Class  \\\n",
       "0   [@southwestair, i, would, appreciate, that, .,...     +   \n",
       "0             [@usairways, thank, you, very, much, .]     +   \n",
       "0   [@jetblue, i, 'm, all, set, ., about, to, fly,...     +   \n",
       "0   [@southwestair, i, got, a, flight, at, 11:55am...     +   \n",
       "0   [@americanair, you, 're, my, early, frontrunne...     +   \n",
       "..                                                ...   ...   \n",
       "0   [@americanair, you, delayed, me, for, 15, hour...     -   \n",
       "0   [why, even, ask, me, to, DM, you, and, offer, ...     -   \n",
       "0   [@usairways, are, you, going, to, do, anything...     -   \n",
       "0                    [@united, you, 're, terrible, .]     -   \n",
       "0   [@usairways, so, i, still, need, to, stay, on,...     -   \n",
       "\n",
       "                                        Stemmed_Tweet  \n",
       "0   [@southwestair, i, would, appreci, that, ., th...  \n",
       "0              [@usairway, thank, you, veri, much, .]  \n",
       "0   [@jetblu, i, 'm, all, set, ., about, to, fli, ...  \n",
       "0   [@southwestair, i, got, a, flight, at, 11:55am...  \n",
       "0   [@americanair, you, 're, my, earli, frontrunn,...  \n",
       "..                                                ...  \n",
       "0   [@americanair, you, delay, me, for, 15, hour, ...  \n",
       "0   [whi, even, ask, me, to, DM, you, and, offer, ...  \n",
       "0   [@usairway, are, you, go, to, do, anyth, to, h...  \n",
       "0                       [@unit, you, 're, terribl, .]  \n",
       "0   [@usairway, so, i, still, need, to, stay, on, ...  \n",
       "\n",
       "[4232 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('This is a test 😻 👍🏿')\n",
    "# assert doc._.has_emoji == True\n",
    "# assert doc[2:5]._.has_emoji == True\n",
    "# assert doc[0]._.is_emoji == False\n",
    "# assert doc[4]._.is_emoji == True\n",
    "# assert doc[5]._.emoji_desc == 'thumbs up dark skin tone'\n",
    "# assert len(doc._.emoji) == 2\n",
    "# assert doc._.emoji[1] == ('👍🏿', 5, 'thumbs up dark skin tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in doc:\n",
    "#     print(i,i._.is_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "stememd_vocab = set()\n",
    "for i in range(len(training_df)):\n",
    "    txt = training_df['Tweet'].iloc[i]\n",
    "    txt_stemmed = training_df['Stemmed_Tweet'].iloc[i]\n",
    "    \n",
    "    for token in txt:\n",
    "        vocab.add(token)\n",
    "        \n",
    "    for token in txt_stemmed:\n",
    "        stememd_vocab.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vocab)\n",
    "vocab.sort()\n",
    "\n",
    "stememd_vocab = list(vocab)\n",
    "stememd_vocab.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7649"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(vocab):\n",
    "    vocab_dict[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['@sa_craig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this BOW is for stemmed and without stemmed tweets\n",
    "\n",
    "def bow(tweet_df,vocab_dict,col):\n",
    "    \"\"\"\n",
    "    tweet_df : training/testing dataframe\n",
    "    vocab is stemmed or without stemmed vocab\n",
    "    col is column name of Tweets, it can be Tweet or Stemmed_Tweet\n",
    "    \"\"\"\n",
    "    if col in tweet_df:\n",
    "        features  = [0]*len(vocab_dict)\n",
    "        df = pd.DataFrame([],columns=list(range(len(features))))\n",
    "        for i in tqdm(range(len(tweet_df))):\n",
    "            tweet = tweet_df[col].iloc[i]\n",
    "            for token in tweet:\n",
    "                features[vocab_dict[token]] = 1\n",
    "                df.loc[len(df)] = features\n",
    "        return df\n",
    "    else:\n",
    "        print(f'Wrong column name {col}')\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▏                                                                       | 285/4232 [11:13<2:35:24,  2.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-6edbb5e91d26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-a7173260ea28>\u001b[0m in \u001b[0;36mbow\u001b[1;34m(tweet_df, vocab_dict, col)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_features = bow(training_df,vocab_dict,'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this BOW is for stemmed and without stemmed tweets\n",
    "\n",
    "def bow(tweet_df,vocab,col):\n",
    "    \"\"\"\n",
    "    tweet_df : training/testing dataframe\n",
    "    vocab is stemmed or without stemmed vocab\n",
    "    col is column name of Tweets, it can be Tweet or Stemmed_Tweet\n",
    "    \"\"\"\n",
    "    if col in tweet_df:\n",
    "        features  = [0]*len(vocab)\n",
    "        df = pd.DataFrame([],columns=list(range(len(features))))\n",
    "        for i in range(len(tweet_df)):\n",
    "            tweet = tweet_df[col].iloc[i]\n",
    "            \n",
    "            for token in tweet:\n",
    "                features[vocab.index(token)] = tweet.count(token)\n",
    "                df.loc[len(df)] = features\n",
    "        return df\n",
    "    else:\n",
    "        print(f'Wrong column name {col}')\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "280385870fbd15c715287f7c4dade702b4363786a50df6d1c4f0e75dc67a5b9b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
