{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WXvY_vW0VTb"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odErKoD_3y2Y"
   },
   "source": [
    "**Please read the following instructions carefully.**\n",
    "- The entire homework assignment is to be completed on this `ipython` notebook. It is designed to be used with `Google Colab`, but you may use other tools (e.g., Jupyter Lab) as well.\n",
    "- Make sure that you execute all cells in a way so their output is printed beneath the corresponding cell. Thus, after successfully executing all cells properly, the resulting notebook has all the questions and your answers.  \n",
    "- Print a PDF copy of the notebook with all its outputs printed and submit the **PDF** on `Canvas` under Assignments.\n",
    "- Make sure you delete any scratch cells before you export this document as a PDF. Do not change the order of the questions and do not remove any part of the questions. Edit at the indicated places only. \n",
    "\n",
    "- **IMPORTANT:** Start your solution with a <font color='red'>**BOLD RED**</font> text that includes the words *solution* and the part of the problem you are working on. For example, start your solution for Part (c) of Problem 2 by having the first line as: \n",
    "<br> <font color='red' > **Solution to Problem 2 Part (c)**</font>. Failing to do so may result in a *20\\% penalty* of the total grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJXN6_xV0kei"
   },
   "source": [
    "## **Assignment Objectives:**\n",
    "\n",
    "- Understand the intuition behind various clustering algorithms discussed in class\n",
    "- Connect concepts related to different clustering algorithms\n",
    "- Implement and evaluate clustering techniques\n",
    "- Implement clustering algorithms on real world datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCRfZU464SZK"
   },
   "source": [
    "## **Guide for Exporting Ipython Notebook to PDF:**\n",
    "Here is a [video](https://www.youtube.com/watch?v=yXzw9Dd_Na0) summarizes how to export Ipythin Notebook into PDF.<br>\n",
    " - **[Method1: Print to PDF]**<br>\n",
    " After you run every cell and get their outputs, you can use **\\[File\\] -> \\[Print\\]** and then choose **\\[Save as PDF\\]** to export this Ipython Notebook to PDF for submission. <br>*Note: Sometimes figures or texts are splited into different pages. Try to tweak the layout by adding empty lines to avoid this effect as much as you can.*\n",
    " - **[Method2: colab-pdf script]**<br>\n",
    " The author of that video provided [an alternative method](https://github.com/brpy/colab-pdf) that can generate better layout PDF. However, it only works for Ipythin Notebook without embedded images.<br>\n",
    "  **How to use:** Put the script below into cells at the end of your Ipythin Notebook. After you run the fisrt cell, it will ask for google drive permission. Executing the second cell will generate the PDF file in your google drive home directory. Make sure you use the correct path and file name.\n",
    " ```\n",
    "## this will link colab with your google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    " ```\n",
    "\n",
    " ```\n",
    " %%capture\n",
    "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
    "from colab_pdf import colab_pdf\n",
    "colab_pdf('LastName_FirstName_ECE_4803_sp22_assignment_#.ipynb') ## change path and file name\n",
    " ```\n",
    " - **[Method3: GoFullPage Chrome Extension] (most recommended)**<br>\n",
    "Install the [extension](https://gofullpage.com/) and generate PDF file of the Ipython Notebook in the browser.<br>\n",
    " \n",
    "**Note:**\n",
    "Georgia Tech provides a student discount for Adobe Acrobat subscription. Further information can be found [here](https://faq.oit.gatech.edu/content/adobe-licensing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hogi5QLR6i6d"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzoM8pEcybgx"
   },
   "source": [
    "## **Problem 1: K-Means and Gaussian Mixture Models (GMMs) on a Toy Example (20pts)**\n",
    "\n",
    "Suppose we are given a dataset with $5$ training examples, each with two features as shown below. \n",
    "\n",
    "| Datapoint ($\\mathbf{x}_{i}$)| Feature 1 Value ($\\mathbf{x}_{i1}$)|  Feature 2 Value ($\\mathbf{x}_{i2}$)|                  \n",
    "|     :---:      | -----------         |   ----           |       \n",
    "| 1              | 0                    | 0                 |\n",
    "| 2              | 2.5                   | 1.5                 |\n",
    "| 3              | 0.5                    | 0.5                 |\n",
    "| 4              | 0.75                    | 1.5                 |\n",
    "| 5              | 0                    | 1                 |\n",
    "\n",
    "In this problem, you will be running the K-means and the GMM clustering algorithms on this dataset step by step to gain an insight into the algorithms.  \n",
    "\n",
    "**(a)**<br>\n",
    "In this part, you will run the K-means clustering algorithm on the data above for two iterations. Fill out the tables below for each iteration. \n",
    "\n",
    "Follow the steps highlighted in Lecture 11 (21-Feb-2022) Page 34 of the PDF where five steps are listed. The difference is that you do not have a convergence criterion but instead you will stop after the second iteration. Use $k=2$ clusters. Initialize the centroids using the following mean values, $\\mathbf{u}_{1}=[0,0]^{T}$ and $\\mathbf{u}_{2}=[0.75,1.5]^{T}$ respectively. You may do the intermediate calculations on scratch paper using either a calculator or a computer program, but do your best to understand every step. \n",
    "\n",
    "Write down the values for the distances of each of the datapoint from the means in each iteration and the resulting cluster assingments in the tables, respectively. \n",
    "\n",
    "The point distances are calculated using the following formula:\n",
    "$$\\|\\mathbf{x}_{i} - \\mathbf{u}_{k}\\|^{2}_{2}$$\n",
    "\n",
    "The cluster assingments are obtained as below:\n",
    "$$\\underset{k}{\\operatorname{argmin}} \\|\\mathbf{x}_{i} - \\mathbf{u}_{k}\\|^{2}_{2}$$\n",
    "\n",
    "Provide also the means after each iteration. (all numbers round to at least 4 decimals.)\n",
    "<br><br>\n",
    "\n",
    "**(b)**<br>\n",
    "With the dataset above, you will use GMM in this part to determine the clustering assignment. \n",
    "\n",
    "Use $K=2$ clusters. Use the same initializations for the means, $\\mathbf{u}_{1}=[0,0]^{T}$ and $\\mathbf{u}_{2}=[0.75,1.5]^{T}$, respectively. Run $2$ iterations of the GMM algorithm. Assume the initial priors to be equal, i.e., $p(\\mathbf{u}_{k},\\Sigma_{k})=0.5$.  Assume that the initial covariance matrices $\\Sigma_{k}$ to be $2\\times 2$ identity matrices. Refer to Lecture 13 (28-MArch-2022) Page 18-19. \n",
    "\n",
    "Fill in the table below after for each iteration. You may do the intermediate calculations on scratch paper using either a calculator or a computer program, but please show how you get the numbers. Provide the class assignments, in addition to the means and covariance matrices for the two mixtures in each iteration. \n",
    "\n",
    "The posterior for the datapoint $\\mathbf{x}_i$ is obtained using the following formula:\n",
    "\n",
    "$$p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i}) = \\frac{p(\\mathbf{u}_{k},\\Sigma_{k}) \\times \\mathcal{N}(\\mathbf{x}_{i};\\mathbf{u}_{k},\\Sigma_{k})} { \\sum^{K}_{k=1}  p(\\mathbf{u}_{k},\\Sigma_{k}) \\times \\mathcal{N}(\\mathbf{x}_{i};\\mathbf{u}_{k},\\Sigma_{k})} $$\n",
    "\n",
    "where $p(\\mathbf{u}_{k},\\Sigma_{k})$ is the prior for $k$-th mixture,  $\\mathcal{N}(\\mathbf{x}_{i};\\mathbf{u}_{k},\\Sigma_{k})$ is the multivariate normal distribution given as following:\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x}_{i};\\mathbf{u}_{k},\\Sigma_{k}) =  \\frac{1}{\\sqrt{(2\\pi)^{2}|\\Sigma_k|}}\\text{exp}(-\\frac{1}{2}(\\mathbf{x}_i - \\mathbf{u}_k)^{T}\\Sigma_k^{-1}(\\mathbf{x}_i - \\mathbf{u}_k)) $$\n",
    "\n",
    "(all numbers round to at least 4 decimals.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFaw14rk-27l"
   },
   "source": [
    "### <font color='red'>  **Problem 1 (a) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LwKoI5l-7E4"
   },
   "source": [
    "First iteration:\n",
    "<table>\n",
    "<tr><th> Point Distances $\\|\\mathbf{x}_{i} - \\mathbf{u}_{k}\\|^{2}_{2}$</th><th>Class Assignments $\\underset{k}{\\operatorname{argmin}} \\|\\mathbf{x}_{i} - \\mathbf{u}_{k}\\|^{2}_{2}$</th><th> Class Means $\\mathbf{u}_k$</tr></tr>\n",
    "<tr><td>\n",
    "\n",
    "|Datapoint ($\\mathbf{x}_{i}$)| Distance from $\\mathbf{u}_1$ | Distance from $\\mathbf{u}_2$|\n",
    "|:---:|:---:|:---:|\n",
    "|1|  |  |\n",
    "|2|  |  |\n",
    "|3|  |  |\n",
    "|4|  |  |\n",
    "|5|  |  |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "|Datapoint ($\\mathbf{x}_{i}$)| Cluster Assignment (1 or 2) |\n",
    "|:---:|:---:|\n",
    "|1|  | \n",
    "|2|  | \n",
    "|3|  | \n",
    "|4|  |\n",
    "|5|  |\n",
    "\n",
    "</td><td> \n",
    "\n",
    "|K-Means Parameters      |               |\n",
    "| ---                    |     ---       |\n",
    "|Mean Cluster 1   ($\\mathbf{u}_{1}$)|  [, ]             |\n",
    "|Mean Cluster 2  ($\\mathbf{u}_{2}$)|   [, ]            |\n",
    "\n",
    "\n",
    "</td><tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Second iteration:\n",
    "<table>\n",
    "<tr><th> Point Distances $\\|\\mathbf{x}_{i} - \\mathbf{u}_{k}\\|^{2}_{2}$</th><th>Class Assignments $\\underset{k}{\\operatorname{argmin}} \\|\\mathbf{x}_{i} - \\mathbf{u}_{k}\\|^{2}_{2}$</th><th> Class Means $\\mathbf{u}_k$</tr></tr>\n",
    "<tr><td>\n",
    "\n",
    "|Datapoint ($\\mathbf{x}_{i}$)| Distance from $\\mathbf{u}_1$ | Distance from $\\mathbf{u}_2$|\n",
    "|:---:|:---:|:---:|\n",
    "|1|  |  |\n",
    "|2|  |  |\n",
    "|3|  |  |\n",
    "|4|  |  |\n",
    "|5|  |  |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "|Datapoint ($\\mathbf{x}_{i}$)| Cluster Assignment (1 or 2) |\n",
    "|:---:|:---:|\n",
    "|1|  | \n",
    "|2|  | \n",
    "|3|  | \n",
    "|4|  |\n",
    "|5|  |\n",
    "\n",
    "</td><td> \n",
    "\n",
    "|K-Means Parameters      |               |\n",
    "| ---                    |     ---       |\n",
    "|Mean Cluster 1   ($\\mathbf{u}_{1}$)|  [, ]             |\n",
    "|Mean Cluster 2  ($\\mathbf{u}_{2}$)|   [, ]            |\n",
    "\n",
    "</td><tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAbzDG2tMCkM"
   },
   "source": [
    "### <font color='red'>  **Problem 1 (b) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU0mmwqAMEdu"
   },
   "source": [
    "First iteration:\n",
    "<table>\n",
    "<tr><th> Point Posteriors $p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i}) $</th><th>Cluster Assignment $\\underset{k}{\\operatorname{argmax}}p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i})$</th><th> GMM Parameters ($\\mathbf{u}_{k},\\Sigma_{k}$)</th></tr>\n",
    "<tr><td>\n",
    "    \n",
    "|Datapoint | Posterior Mixture 1 | Posterior Mixture 2|\n",
    "|:---:|:---:|:---:|\n",
    "|1    |     |     |\n",
    "|2    |     |     |\n",
    "|3    |     |     |\n",
    "|4    |     |     |\n",
    "|5    |     |     |  \n",
    "\n",
    "</td><td>\n",
    "\n",
    "|Datapoint ($\\mathbf{x}_{i}$)|  Cluster Assignment (1 or 2)|\n",
    "|:---:|:---:|\n",
    "|1|  | \n",
    "|2|  | \n",
    "|3|  | \n",
    "|4|  | \n",
    "|5|  | \n",
    "\n",
    "</td><td> \n",
    "\n",
    "|GMM Parameters          |               |\n",
    "| ---                    |     ---       |\n",
    "|Cluster 1 prior| |\n",
    "|Mean Cluster 1 ($\\mathbf{u}_{1}$)|   [, ]            |\n",
    "|Covariance Cluster 1 ($\\Sigma_{1}$)| [, ; , ]              |\n",
    "|Cluster 2 prior| |\n",
    "|Mean Cluster 2    ($\\mathbf{u}_{2}$)| [, ]              |\n",
    "|Covariance Cluster 2 ($\\Sigma_{2}$)|  [, ; , ]             |\n",
    "\n",
    "</td></tr></table>\n",
    "\n",
    "Second iteration:\n",
    "<table>\n",
    "<tr><th> Point Posteriors $p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i}) $</th><th>Cluster Assignment $\\underset{k}{\\operatorname{argmax}}p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i})$</th><th> GMM Parameters ($\\mathbf{u}_{k},\\Sigma_{k}$)</th></tr>\n",
    "<tr><td>\n",
    "    \n",
    "|Datapoint | Posterior Mixture 1 | Posterior Mixture 2|\n",
    "|:---:|:---:|:---:|\n",
    "|1    |     |     |\n",
    "|2    |     |     |\n",
    "|3    |     |     |\n",
    "|4    |     |     |\n",
    "|5    |     |     |  \n",
    "\n",
    "</td><td>\n",
    "\n",
    "|Datapoint ($\\mathbf{x}_{i}$)|  Cluster Assignment (1 or 2)|\n",
    "|:---:|:---:|\n",
    "|1|  | \n",
    "|2|  | \n",
    "|3|  | \n",
    "|4|  | \n",
    "|5|  | \n",
    "\n",
    "</td><td> \n",
    "\n",
    "|GMM Parameters          |               |\n",
    "| ---                    |     ---       |\n",
    "|Cluster 1 prior| |\n",
    "|Mean Cluster 1 ($\\mathbf{u}_{1}$)|   [,]            |\n",
    "|Covariance Cluster 1 ($\\Sigma_{1}$)| [, ; ,]              |\n",
    "|Cluster 2 prior| |\n",
    "|Mean Cluster 2    ($\\mathbf{u}_{2}$)| [,]              |\n",
    "|Covariance Cluster 2 ($\\Sigma_{2}$)|  [,; ,]             |\n",
    "\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dr8bZs2IRcX"
   },
   "source": [
    "## **Problem 2: K-Means vs GMMs for Modeling Non-Spherical Distributions (15pts)**\n",
    "\n",
    "K-means is good when finding clusters of data sampled from gaussian distributions with zero correlation (and ideally equal variances in all feature directions). In cases where this is not true (i.e., gaussian distributions have correlated features and/or unequal variances), GMMs tend to perform superior to K-means, as you will hopefully see in this question.\n",
    "\n",
    "As seen in Problem 1 above, running both K-means and GMMs requires the setup of an $N\\times K$ dimensional table for each iteration, storing the point distances to the indiviudal cluster means for the former and point posteriors for the latter. $N$ refers to the number of training examples while $K$ is the number of clusters. The **un-normalized** posterior for the $i$-th datapoint having mean and covariance $\\mathbf{u}_{k}$ and $\\Sigma_{k}$ is given as:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i}) &= \\overbrace{p(\\mathbf{u}_{k},\\Sigma_{k})}^{\\text{prior for mixture }k}\\times \\overbrace{p(\\mathbf{x}_{i}|\\mathbf{u}_{k},\\Sigma_{k})}^{\\text{likelihood for }\\mathbf{x}_{i}\\text{ given mxiture }k} \\tag{4} \\\\ \\\\\n",
    "p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i}) &= p(\\mathbf{u}_{k},\\Sigma_{k}) \\times \\mathcal{N}(\\mathbf{x}_{i};\\mathbf{u}_{k},\\Sigma_{k}), \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal{N}(\\mathbf{x}_{i};\\mathbf{u}_{k},\\Sigma_{k})$ is the multivariate normal distribution characterized by mean $\\mathbf{u}_{k}$ and covariance $\\Sigma_{k}$ and \n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x}_{i};\\mathbf{u}_{k},\\Sigma_{k}) =  \\frac{1}{\\sqrt{(2\\pi)^{P}|\\Sigma|}}\\text{exp}(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{u})^{T}\\Sigma^{-1}(\\mathbf{x} - \\mathbf{u})) \\tag{6}$$\n",
    "\n",
    "**(a)**<br>\n",
    "If we want to normalize Equation 4, what is the formula of denominator? After normalization, what is the range for normalized posterior?\n",
    "\n",
    "**(b)**<br>\n",
    "As we have seen in Regression, it is often more convenient to compute these values in the natural log scale. In this part, take the log of both sides of Equation 4. Write down the resulting equation below.\n",
    "\n",
    "**(c)**<br>\n",
    "Plug Equation 6 into the equation you derived in part (b). Simplify and write down a fully expanded expression for $\\text{log}\\,p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i})$. \n",
    "\n",
    "**(d)**<br>\n",
    "Considering the expression in part (c), it is possible to simplify the expression to represent K-means. In other words, K-means can be a special case of GMM. Explain exactly under what constraints and changes, if any, on the prior, means, covariance matrices, and the update process does this statement become true, i.e., K-means is a sepcial case of GMM. \n",
    "\n",
    "**(e)**<br>\n",
    "The code below generates some data sampled from two $2-D$ gaussian distributions with different means and covariance matrices. \n",
    "\n",
    "Using the `sklearn.cluster.KMeans` class for K-means and `sklearn.mixture.GaussianMixture` class for GMMs, set up a class object for each to run for $2$ clusters on this data. Describe the results. Which one of K-means and GMMs captures the original distribution better? How do you relate this to what you explained in part (d)? \n",
    "\n",
    "**_Note:_** To discount the effect of random initialization, you might have to execute the cell multiple times to get a consistent idea of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Too9jUF6AepZ"
   },
   "source": [
    "### <font color='red'>  **Problem 2 (a) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5SbuvC-Afd9"
   },
   "source": [
    "\\[Put your answer in this cell\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPYSYQKB9Sak"
   },
   "source": [
    "### <font color='red'>  **Problem 2 (b) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2P0djjo9Wss"
   },
   "source": [
    "\\[Put your answer in this cell\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_YrxSKu9f1B"
   },
   "source": [
    "### <font color='red'>  **Problem 2 (c) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYaypoxd9iiu"
   },
   "source": [
    "\\[Put your answer in this cell\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WXhdYFF-UEs"
   },
   "source": [
    "### <font color='red'>  **Problem 2 (d) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb6dZSNG-XOk"
   },
   "source": [
    "\\[Put your answer in this cell\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCPxJapJ-qiA"
   },
   "source": [
    "### <font color='red'>  **Problem 2 (e) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAbcFcOQ-wxL"
   },
   "source": [
    "\\[Put your answer in this cell\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jwIH3yq0Ma6"
   },
   "outputs": [],
   "source": [
    "#### Do not change this cell. This a helper cell. Please execute it.\n",
    "\n",
    "# imports and utlity functions\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from itertools import cycle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "import random\n",
    "from skimage import data, color\n",
    "\n",
    " \n",
    "# generate colors for clustering via python generator\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "color_generator = cycle(colors)\n",
    "\n",
    "# helper functions\n",
    "\n",
    "# evaluation metrics\n",
    "\n",
    "# Dunn Index\n",
    "# from the resource: https://en.wikipedia.org/wiki/Cluster_analysis\n",
    "# the distance between two clusters can be any of the measurements, i.e. distance between centroids or any points.\n",
    "\n",
    "def delta(ck, cl):\n",
    "    values = np.ones([len(ck), len(cl)]) * np.finfo(np.float32).max\n",
    "    for i in range(len(ck)):\n",
    "        for j in range(len(cl)):\n",
    "            values[i, j] = np.linalg.norm(ck[i] - cl[j])\n",
    "\n",
    "    return np.min(values)\n",
    "\n",
    "\n",
    "def big_delta(ci):\n",
    "    values = np.zeros([len(ci), len(ci)])\n",
    "\n",
    "    for i in range(0, len(ci)):\n",
    "        for j in range(0, len(ci)):\n",
    "            values[i, j] = np.linalg.norm(ci[i] - ci[j])\n",
    "\n",
    "    return np.max(values)\n",
    "\n",
    "# Dunn Index\n",
    "def dunn_index(X, cluster_labels):\n",
    "    # A list containing a numpy array for each cluster\n",
    "    # k_list[k] is np.array([N, p]) (N : number of samples in cluster k, p : sample dimension)\n",
    "    k_list = []\n",
    "    for k in np.unique(cluster_labels):\n",
    "        k_list.append(X[cluster_labels == k])\n",
    "\n",
    "    deltas = np.ones([len(k_list), len(k_list)]) * np.finfo(np.float32).max\n",
    "    big_deltas = np.zeros([len(k_list), 1])\n",
    "    l_range = list(range(0, len(k_list)))\n",
    "\n",
    "    for k in l_range:\n",
    "        for l in (l_range[0:k] + l_range[k + 1:]):\n",
    "            deltas[k, l] = delta(k_list[k], k_list[l])\n",
    "\n",
    "        big_deltas[k] = big_delta(k_list[k])\n",
    "\n",
    "    di = np.min(deltas) / np.max(big_deltas)\n",
    "\n",
    "    return di\n",
    "\n",
    "\n",
    "def delta_fast(ck, cl, distances):\n",
    "    values = distances[np.where(ck)][:, np.where(cl)]\n",
    "    values = values[np.nonzero(values)]\n",
    "\n",
    "    return np.min(values)\n",
    "\n",
    "\n",
    "def big_delta_fast(ci, distances):\n",
    "    values = distances[np.where(ci)][:, np.where(ci)]\n",
    "\n",
    "    return np.max(values)\n",
    "\n",
    "\n",
    "def dunn_index_fast(X, cluster_labels):\n",
    "    \"\"\"Dunn Index - fast(using sklearn pairwise euclidean_distance function\n",
    "    X: np.array\n",
    "    np.array([N,p] of all samples\n",
    "    cluster_labels: np.array\n",
    "    np.array([N,]) labels of all samples\n",
    "    \"\"\"\n",
    "    distances = euclidean_distances(X)\n",
    "    ks = np.sort(np.unique(cluster_labels))\n",
    "\n",
    "    deltas = np.ones([len(ks), len(ks)]) * np.finfo(np.float32).max\n",
    "    big_deltas = np.zeros([len(ks), 1])\n",
    "    l_range = list((range(0, len(ks))))\n",
    "\n",
    "    for k in l_range:\n",
    "        for l in (l_range[0:k] + l_range[k+1:]):\n",
    "            deltas[k, l] = delta_fast((cluster_labels==ks[k]), (cluster_labels==ks[l]), distances)\n",
    "        \n",
    "        big_deltas[k] = big_delta_fast(cluster_labels == ks[k], distances)\n",
    "\n",
    "    di = np.min(deltas) / np.max(big_deltas)\n",
    "    return di\n",
    "\n",
    "\n",
    "# Cluster Purity\n",
    "def purity_score(labels_true, labels_pred):\n",
    "    # compute contingency matrix\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(labels_true, labels_pred)\n",
    "\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ry2Iv47IxWAe"
   },
   "outputs": [],
   "source": [
    "# for Problem 2 (e)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "mu_1 = np.array([0,0])\n",
    "mu_2 = np.array([0,0])\n",
    "cov1 = np.array([[1.95, 1],[1, 0.5]])\n",
    "theta = np.radians(30)\n",
    "c, s = np.cos(theta), np.sin(theta)\n",
    "R = np.array(((c, -s), (s, c)))\n",
    "X_1 = np.random.multivariate_normal(mu_1, cov1, 500)\n",
    "X_2 = X_1.dot(R) - np.array([2,0]).reshape(1,-1)\n",
    "X = np.concatenate((X_1, X_2), axis=0)\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "#-----------------Don't change anything above------------------------#\n",
    "model_kmeans = ##TODO\n",
    "model_gmms = ##TODO\n",
    "y_pred_kmeans = model_kmeans.fit_predict(X)\n",
    "y_pred_gmms = model_gmms.fit_predict(X)\n",
    "#-----------------Don't change anything below------------------------#\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for cluster_id, color in zip(range(2), color_generator):\n",
    "  data_x = X[y_pred_kmeans==cluster_id,0]\n",
    "  data_y = X[y_pred_kmeans==cluster_id,1]\n",
    "  ax1.scatter(data_x, data_y, color = color, label='Class {}'.format(cluster_id))\n",
    "  ax1.legend()\n",
    "  ax1.set_xlabel('Feature 1')\n",
    "  ax1.set_ylabel('Feature 2')\n",
    "  ax1.set_title('Kmeans')\n",
    "\n",
    "  \n",
    "for cluster_id, c in zip(range(2), color_generator):\n",
    "  data_x = X[y_pred_gmms==cluster_id,0]\n",
    "  data_y = X[y_pred_gmms==cluster_id,1]\n",
    "  ax2.scatter(data_x, data_y, color = c, label='Class {}'.format(cluster_id))\n",
    "  ax2.legend()\n",
    "  ax2.set_xlabel('Feature 1')\n",
    "  ax2.set_ylabel('Feature 2')\n",
    "  ax2.set_title('GMMs')  \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MHmvXfA4SQO"
   },
   "source": [
    "## **Problem 3: Implement K-Means (20pts)**\n",
    "In this problem, you are going to design a python class that implements the k-means algorithm. You are provided with a class template called `MyKmeans` that you have to fill in to implement various stages in the k-means workflow. Follow the steps outlined in the parts below to answer the question.\n",
    "\n",
    "**(a)**<br>\n",
    "The initialization function in the class stores the training data (`X_train`), the number of clusters (`k`), and the number of iterations to run the k-means algorithm for (`num_iter`). The first step in the algorithm entails randomly selecting $k$ data points in `X_train` to be the centroids, one for each cluster. Fill in code for the class function `__init_means()` below to do this. The output should be stored into the `self.means` variable to be used later in the fitting stage. Pay attention to the shape of this array. \n",
    "\n",
    "**(b)**<br>\n",
    "Implement the `fit_predict()` function in the class definition by writing code to execute the Assignment and Update steps, as described in Page 34 of lecture 11. For each iteration, the assignment step involves computing the euclidean distance of each data point in `X_train` to each of the $k$ centroids selected in part(a) above. The result is essentially an $N\\times K$ dimensional table where each column stores the euclidean distance of all data points to the centroid corresponding to the column. This is used to compute the cluster label for each datapoint by choosing the centroid (where centroid label $\\in[1,K]$) corresponding to the smallest distance, resulting in a $N\\times 1$ array. \n",
    "\n",
    "**(c)**<br>\n",
    "Next, implement the Update step, where the $N\\times 1$ label vector just created in part (b) is used to recompute the means for each of the $k$ centroids, and thus update the `self.means` structure. Refer to Page 34 in Lecture 11 (21-Feb-2022) for the details. \n",
    "\n",
    "Execute the cell once you have completed all of the above to classify the `Iris` dataset you used in homework 1 using **two**  preselected features. You may have to run the cell multiple times to discount the effect of random initialization and get consistent results. \n",
    "\n",
    "Plot the clustering results when using features $0$ and $2$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbKskjVGCPWq"
   },
   "source": [
    "### <font color='red'>  **Problem 3 (a)(b)(c) Solution** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DM8vlKNS-CVp"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import davies_bouldin_score, mutual_info_score, adjusted_rand_score\n",
    "\n",
    "class MyKMeans:\n",
    "  def __init__(self, X_train, k, num_iter=20):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: ndarray of shape (number of samples, num of features).\n",
    "      Training data array.\n",
    "      \n",
    "    k: int, \n",
    "      number of clusters.\n",
    " \n",
    "    num_iter:int\n",
    "      number of steps to run algorithm for.\n",
    "    \"\"\"\n",
    " \n",
    "    self.X_train = X_train\n",
    "    self.k = k\n",
    "    self.num_iter = num_iter\n",
    "  \n",
    "  def __init_means(self):\n",
    "    \"\"\"initialize means as an ndarray of shape (k, num of features).\"\"\"\n",
    "    ## part (a)\n",
    "    self.means = ##TODO\n",
    " \n",
    "  def fit_predict(self):\n",
    "    \"\"\"Runs the k means algorithm. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred: ndarray of shape (num of samples, 1)\n",
    "      array of predicted cluster labels for each data point.\n",
    "    \"\"\"\n",
    " \n",
    "    self.__init_means()  # initialize means\n",
    "\n",
    "    for iteration in range(self.num_iter):      # begin the algorithm\n",
    "\n",
    "      # assignment step\n",
    "      ## part (b)\n",
    "      ##TODO\n",
    "\n",
    "      # update means step\n",
    "      ## part (c)\n",
    "      ##TODO\n",
    "\n",
    "    # Final class assignments\n",
    "    ## part (c)\n",
    "    y_pred = ##TODO\n",
    "    \n",
    "    # store means \n",
    "    ## part (c)\n",
    "    self.means = ##TODO\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "#-----------------Don't change anything below------------------------#\n",
    "\n",
    "# k means parameters\n",
    "k = 3\n",
    "num_iter = 20\n",
    "feature_nums = [0,2]  # features to use\n",
    "\n",
    "# load and preprocess the dataset\n",
    "dataset = load_iris()\n",
    "X, y = dataset.data, dataset.target\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "X = X[:,feature_nums]\n",
    " \n",
    "# fit the model\n",
    "kmeans = MyKMeans(X, k=k, num_iter=num_iter)\n",
    "y_pred = kmeans.fit_predict()\n",
    " \n",
    " \n",
    "# plot data \n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "for cluster_id in range(k):\n",
    "  data_x = X[y_pred==cluster_id,0]\n",
    "  data_y = X[y_pred==cluster_id,1]\n",
    "  ax.scatter(data_x, data_y, color = next(color_generator), label='Class {}'.format(cluster_id))\n",
    "  ax.legend()\n",
    "  \n",
    "plt.xlabel('Feature {}'.format(feature_nums[0]))\n",
    "plt.ylabel('Feature {}'.format(feature_nums[1]))\n",
    "plt.show()\n",
    "\n",
    "print('Davies Bouldin Score: {:0.4f}'.format(davies_bouldin_score(X, y_pred)))\n",
    "print('Dunn Index: {:0.4f}'.format(dunn_index(X, y_pred)))\n",
    "print('Mutual Information Score: {:0.4f}'.format(mutual_info_score(y, y_pred)))\n",
    "print('Rand Index: {:0.4f}'.format(adjusted_rand_score(y, y_pred)))\n",
    "print('Purity Score: {:0.4f}'.format(purity_score(y, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVUY58Ry4SQP"
   },
   "source": [
    "## **Problem 4: Implementing Gaussian Mixture Models (20pts)**\n",
    "\n",
    "In this problem, you will create a `MyGMMs` class that implements clustering using Gaussian Mixture Models (GMM). The class intialization function stores the training data array, `X_train`, the prespecified number of mixtures, `k`, and the number of iterations, `num_iteratons`. The means and covariances of the $k$-th cluster and the posteriors structure have already been initalized in the `__init_params()` function. Read every question very carefully before you start your solution.\n",
    "\n",
    "**(a)**<br>\n",
    "Expectation Step: The analogue of the $N\\times K$ dimesnional cluster distances table created in Problem 1 (b) above is the posteriors structure in GMMs. The entries $\\gamma_{ik}$ in this table store the (unnormalized) posterior probabilities of the parameters of the $k$-th mixture given a datapoint $\\mathbf{x}_{i}$ (i.e., $\\gamma_{ik} = p(\\mathbf{u}_{k},\\Sigma_{k}\\,|\\mathbf{x}_{i})$). Using the expression you derived in problem 2 (b), fill in the `fit_predict()` function below to compute the expectation step. We work with natural logs because they are numerically easier to deal with from the computer's point of view. Remember also that since you are working with logs, so you need to take the exponent of the final value and normalize before storing it as a posterior.  \n",
    "\n",
    "**(b)** <br>\n",
    "Maximization Step: Having completed the expectation step in the E-M algorithm you studied in class, you now have a complete $N\\times K$ dimensional table of posterior values. We now move on to the maximization step where we are required to update the priors ($p(\\mathbf{u}_{k},\\Sigma_{k})$), the means ($\\mathbf{u}_{k}$), and the covariances ($\\Sigma_{k}$) for each of the $K$ clusters. \n",
    "\n",
    "**(i)** Using page 19 of Lecture 13 (28-Feb-2022) as a reference, write down the expression for the mean of the $k$-th mixture in terms of $\\gamma_{ik}$ we computed in (a). \n",
    "\n",
    "**(ii)** Again using page 19 as the reference, write down the expression for the covariance of the  $k$-th mixture in terms of $\\gamma_{ik}$ and $\\mathbf{u}_{k}$ we computed above.\n",
    "\n",
    "**(iii)** Once again using page 19 as a reference, write down the expression for the new prior of the  $j$-th mixture in terms of $\\gamma_{ik}$.\n",
    "\n",
    "**(iv)** Code steps (i - iii) into the `fit_predict()` function in the class template below. This completes the maximization step of the iteration. \n",
    "\n",
    "Further, complete the code to calculate the final assignments using the latest posterior table. Do not forget to update the `self.parameter` and `self.priors` variables before the function returns the labels. \n",
    "\n",
    "Execute the cell multiple times until you are able to get an idea of what the consistent results look like. \n",
    "\n",
    "Plot the clustering results when using features $0$ and $2$.\n",
    "\n",
    "**_Practical Tips:_**\n",
    "1. The priors in equation 1 can underflow to zero in the log expression, resulting in nans. You may want to add a small value e.g., $1e-4$ to compensate for that.\n",
    "\n",
    "2. In case the covariance matrix turns out to be singular or badly conditioned, taking the inverse will result in an error. You may want to use `numpy.linalg.pinv` rather than `numpy.linalg.inv`. This will calcualte the pseudo inverse. \n",
    "\n",
    "3. You can improve the conditioning of the covariance matrices in the maximization step by adding an identity matrix scaled by a small number e.g., 1e-4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq2sHCPXLeA0"
   },
   "source": [
    "### <font color='red'>  **Problem 4 (a) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMGAXl-lL21c"
   },
   "source": [
    "Complete code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QvS-EepLvG2"
   },
   "source": [
    "### <font color='red'>  **Problem 4 (b)(i) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag3aCLJFLxem"
   },
   "source": [
    "\\[Put your answer in this cell\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc-iaXweMBvo"
   },
   "source": [
    "### <font color='red'>  **Problem 4 (b)(ii) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvb5gPchMEgY"
   },
   "source": [
    "\\[Put your answer in this cell\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwAKMqxdMecT"
   },
   "source": [
    "### <font color='red'>  **Problem 4 (b)(iii) Solution** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-sXonsPMiJU"
   },
   "source": [
    "\\[Put your answer in this cell\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UHgDzQAMsb_"
   },
   "source": [
    "### <font color='red'>  **Problem 4 (b)(iv) Solution** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rdoTKQ9GToc"
   },
   "outputs": [],
   "source": [
    "# for problem 4\n",
    "from sklearn.metrics import davies_bouldin_score, mutual_info_score, adjusted_rand_score\n",
    "\n",
    "class MyGMMs:\n",
    "  def __init__(self, X_train, k, num_iter=20):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: ndarray of shape (number of samples, num of features).\n",
    "      Training data array.\n",
    "      \n",
    "    k: int, \n",
    "      number of clusters.\n",
    "\n",
    "    num_iter: int\n",
    "      number of iteration to run E-M algorithm for.\n",
    "    \"\"\"\n",
    "\n",
    "    self.X_train = X_train\n",
    "    self.k = k\n",
    "    self.num_iter = num_iter\n",
    "\n",
    "    \n",
    "  def __init_params(self):\n",
    "    \"\"\"Function initializes the means, covariances, and posteriors structure for \n",
    "    the E-M algorithm\"\"\"\n",
    "\n",
    "    \n",
    "    # extract k and data matrices\n",
    "    k = self.k\n",
    "    X_train = self.X_train\n",
    "    \n",
    "    # Initialize priors as uniform \n",
    "    self.priors = 1/k * np.ones((k,1))\n",
    "    \n",
    "    # intialize means and covariances\n",
    "    self.parameters = [[] for i in range(k)]\n",
    "    for i in range(k):\n",
    "        self.parameters[i].append(np.random.randn(X_train.shape[1],1))  # initialize random means\n",
    "        temp = np.random.randn(X_train.shape[1],X_train.shape[1])\n",
    "        self.parameters[i].append(temp.T.dot(temp)+1e-4*np.eye(X_train.shape[1]))  # initialize random covariances\n",
    "        \n",
    "    # set up posterior structure\n",
    "    self.posteriors = np.zeros((X_train.shape[0], k))\n",
    "    \n",
    "\n",
    "  def fit_predict(self):\n",
    "      \"\"\" Returns predicted cluster classes.\n",
    "      \n",
    "      Returns\n",
    "      -------\n",
    "      y_pred: ndarray of shape (number of samples, 1).\n",
    "        Predicted classes for each data point in X_train. \n",
    "      \"\"\"\n",
    "\n",
    "      # begin the E-M Algorithm\n",
    "      for iteration in range(self.num_iter):\n",
    "      # expectation step to compute NxK dimensional array storing posterior values\n",
    "        for mixture_id in range(k):\n",
    "          ## part (a)\n",
    "          ##TODO\n",
    "\n",
    "        for mixture_id in range(k):\n",
    "          ## part (b)\n",
    "          ##TODO\n",
    "\n",
    "      # compute final class predictions\n",
    "      y_pred = ##TODO\n",
    "\n",
    "      # store distribution paramteres\n",
    "      self.priors = ##TODO\n",
    "      self.parameters = ##TODO\n",
    "\n",
    "      return y_pred\n",
    "\n",
    "#-----------------Don't change anything below------------------------#\n",
    "\n",
    "# k means parameters\n",
    "k = 3                     # num of mixtures\n",
    "num_iter = 20\n",
    "feature_nums = [0,2]  # features to use\n",
    "\n",
    "# load and preprocess the dataset\n",
    "dataset = load_iris()\n",
    "X, y = dataset.data, dataset.target\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "X = X[:, feature_nums]\n",
    "\n",
    "gmm = MyGMMs(X, k, num_iter=num_iter)\n",
    "y_pred = gmm.fit_predict()\n",
    "\n",
    "# plot data \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for cluster_id in range(k):\n",
    "  data_x = X[y_pred==cluster_id,0]\n",
    "  data_y = X[y_pred==cluster_id,1]\n",
    "  ax.scatter(data_x, data_y, color = next(color_generator), label='Class {}'.format(cluster_id))\n",
    "  ax.legend()\n",
    "  \n",
    "plt.xlabel('Feature {}'.format(feature_nums[0]))\n",
    "plt.ylabel('Feature {}'.format(feature_nums[1]))\n",
    "plt.show()\n",
    "\n",
    "print('Davies Bouldin Score: {:0.4f}'.format(davies_bouldin_score(X, y_pred)))\n",
    "print('Dunn Index: {:0.4f}'.format(dunn_index(X, y_pred)))\n",
    "print('Mutual Information Score: {:0.4f}'.format(mutual_info_score(y, y_pred)))\n",
    "print('Rand Index: {:0.4f}'.format(adjusted_rand_score(y, y_pred)))\n",
    "print('Purity Score: {:0.4f}'.format(purity_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0J3CVv0Jbb_"
   },
   "source": [
    "## **Problem 5: Implementing Image Segmentation via Unsupervised Clustering on Kaggle Competition (20pts)**\n",
    "\n",
    "After designing your very own classes implementing the popular K-means and GMM algorithms for clustering, we are now going to test them out on image data for segmentation of different structures therein. The first part of this question is designed to guide you in a step-by-step process to convert a simple, gray-scale image in a form that can be processed by the clustering classes you designed above before converting the result back in a spatiotemporal form for visualization of the segmented structures. \n",
    "\n",
    "**(a)**<br>\n",
    "Run the code cell below to load an image from `sklearn`'s `digits` dataset representing images of numbers $0 - 9$. Complete the function template in the cell to reshape the features in the form of an $8\\times 8$ image that can be displayed using `matplotlib`'s `imshow` function. (*Hint: You may find it helpful to use* `np.reshape` *function*.)\n",
    "\n",
    "**(b)**<br>\n",
    "Execute the cell below that takes a digit example as input to each of the clustering classes (K-means and GMM) you designed above to output a segmentation result. Remember that in this case, each pixel in the image is going to be a 'training example' from the perspective of the clustering algorithm, with the 'feature' being the gray scale value itself. Fill in the function template reshaping the digit example into the form needed for your custering classes.\n",
    "\n",
    "**(c)**<br>\n",
    "For a simple problem like above with only gray-scale images, you learnt to process images in a form they could be used to train clustering algorithms (with each individual pixel being a 'training example'). For an RGB image, each training example would have at least 3 features (the Red, Green, and Blue values for the pixel). In addition, one could add the spatial positions of the pixel as another set of features. We are now going test what you have learnt by means of a [Kaggle](https://www.kaggle.com/c/funml-hw4-p6) competition wherein you are asked to segment two RGB images. Your results will be submitted to a Kaggle leaderboard to be graded accordingly. Try different feature combinations, image processing techniques to get the best looking results\n",
    "\n",
    "We provide you with two images: a simple image consisting of geometric shapes and another one containing more complicated objects, both in RGB. The images can be downloaded by running the following snippet of code:\n",
    "\n",
    "```\n",
    "!gdown --id 1ZAvUJktJ0aojeXWJnuxuYqHlLslCZ9T-\n",
    "!gdown --id 1Qh2HppgVSAniVqWxRsbJ7cZFhUpRdUpI\n",
    "```\n",
    "You can then load the two images as shown below:\n",
    "\n",
    "```\n",
    "import imageio\n",
    "\n",
    "im1 = imageio.imread('img1.png')\n",
    "im2 = imageio.imread('img2.png')\n",
    "```\n",
    "\n",
    "Since this is an open-ended question, you should try to design your own features to achieve better classification results. By submitting your result to Kaggle, you will see your multi-class classification accuracy and ranking on the leaderborad. 10 pts will depend on your ranking (top 10% get 10pts, top 11%-20% get 9pts, etc.), and another 5 pts depend on your method explanation and clear, well-labeled plots and images.<br>\n",
    "\n",
    "[Note:]\n",
    "- After having your ideal result, please save your result in `result_img1` and `result_img2` in the cell below.\n",
    "\n",
    "- We have defined the label for each class in each image. Make sure you use the same settings as us and feel free to use the provided swapping label code to swap labels if needed.\n",
    "\n",
    " - for `img1.png`, use k=3, background labels as 0, rectangle labels as 1 and triangle labels as 2\n",
    " - for `img2.png`, use k=2, background labels as 0, dog labels as 1 \n",
    "\n",
    "- After your `result_img1` and `result_img2` are ready, run the cell below to create a `submission.csv`. Please download it from this notebook and submit it in our [Kaggle](https://www.kaggle.com/c/funml-hw4-p6) competition.\n",
    "\n",
    "- Please remember to note your Kaggle competition nickname in this notebook. We will use your ranking to grade.\n",
    "\n",
    "- You have 10 submission quota each day to submit your result and get your multi-class classification accuracy and ranking.\n",
    "\n",
    "- We calculate the multi-class classification accuracy with ground-turth hand-crafted labels in both images by \n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{# of correctly labeled pixels}}{\\text{total # of pixels}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXeFCZYYJxnU"
   },
   "source": [
    "### <font color='red'>  **Problem 5 (a) Solution** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNcf-t7b7kfy"
   },
   "outputs": [],
   "source": [
    "## for problem 5 (a)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# function to reshape\n",
    "def image_reshape(x):\n",
    "  \"\"\"Function reshapes a training example from the Digits dataset into an image\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  x: ndarray of shape (1, num_of_features)\n",
    "    flattened image example from the digits dataset\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  img: ndarray of shape (8,8)\n",
    "    ndarray containing reshaped image for visualization\n",
    "  \"\"\"\n",
    "\n",
    "  return ##TODO\n",
    "\n",
    "#-----------------Don't change anything below------------------------#\n",
    "\n",
    "# load data and extract a digit example\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "digit = X[8].reshape(1, -1)\n",
    "\n",
    "# reshape image\n",
    "reshaped_img = image_reshape(digit)\n",
    "\n",
    "# visualize\n",
    "plt.imshow(reshaped_img, cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK9DDBjRJ_0x"
   },
   "source": [
    "### <font color='red'>  **Problem 5 (b) Solution** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWZPn3nnJ_lU"
   },
   "outputs": [],
   "source": [
    "## for problem 5 (b)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def feature_reshape(digit):\n",
    "  \"\"\"function reshapes a digit training example into a form acceptable for use with\n",
    "  clustering classes\n",
    "  \n",
    "  Parameters:\n",
    "  -----------\n",
    "  digit: ndarray of shape (1, num_of_features)\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  reshaped_digit: ndarray of shape (num_of_features, 1)\n",
    "  \"\"\"\n",
    "\n",
    "  return ##TODO\n",
    "\n",
    "\n",
    "#-----------------Don't change anything below------------------------#\n",
    "  \n",
    "\n",
    "# load data and extract a digit example\n",
    "X, _ = load_digits(return_X_y=True)\n",
    "digit = X[8].reshape(1, -1)\n",
    "\n",
    "# number of mixtures/clusters\n",
    "k = 2\n",
    "\n",
    "reshaped_digit = feature_reshape(digit)\n",
    "\n",
    "# cluster with model of choice\n",
    "model_1 = MyGMMs(reshaped_digit, k=k, num_iter=20)\n",
    "model_2 = MyKMeans(reshaped_digit, k=k, num_iter=20)\n",
    "y_pred_1 = model_1.fit_predict()\n",
    "y_pred_2 = model_2.fit_predict()\n",
    "\n",
    "# visualize results\n",
    "segmented_im1_1 = y_pred_1.reshape(8, 8)\n",
    "segmented_im1_2 = y_pred_2.reshape(8, 8)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,9))\n",
    "ax1.imshow(image_reshape(digit))\n",
    "ax1.set_title('Original')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax2.imshow(segmented_im1_1)\n",
    "ax2.set_title('Segmented By GMMs')\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax3.imshow(segmented_im1_2)\n",
    "ax3.set_title('Segmented by Kmeans')\n",
    "ax3.set_xticks([])\n",
    "ax3.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLgNIXf036e5"
   },
   "source": [
    "### <font color='red'>  **Problem 5 (c) Solution** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avFrwYBlOT2O"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1ZAvUJktJ0aojeXWJnuxuYqHlLslCZ9T-\n",
    "!gdown --id 1Qh2HppgVSAniVqWxRsbJ7cZFhUpRdUpI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siLWpLRAOVDH"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "im1 = imageio.imread('img1.png') # use k=3, background labels as 0, rectangle labels as 1 and triangle labels as 2\n",
    "im2 = imageio.imread('img2.png') # use k=2, background labels as 0, dog labels as 1\n",
    "\n",
    "##TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPW0Vnd0yqNu"
   },
   "outputs": [],
   "source": [
    "## make sure img1 has shape (50, 50) and img2 has shape (100, 100)\n",
    "result_img1 = ##TODO\n",
    "result_img2 = ##TODO\n",
    "\n",
    "\n",
    "## -----------------swap labels if needed------------------------------##\n",
    "## Here is the template code for you, if you need to swap label 0 and 1\n",
    "\n",
    "# result_img1[result_img1==0] = -1\n",
    "# result_img1[result_img1==1] = 0\n",
    "# result_img1[result_img1==-1] = 1\n",
    "\n",
    "#-----------------Don't change anything below------------------------#\n",
    "## After running this cell, you should be able to download sumission.csv file on the lefe side bar.\n",
    "\n",
    "cat_data = np.concatenate((result_img1.reshape(-1, 1), result_img2.reshape(-1, 1)), axis=0)\n",
    "np.savetxt('/content/submission.csv', np.concatenate((np.arange(12500).reshape(-1, 1), cat_data), axis=1), delimiter=',', header=\"Id,Category\", comments='')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECE4803-Sp22-HW4-Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
