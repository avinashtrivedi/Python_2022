{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# added for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "# import math\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from numpy import asarray\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform TLM Data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_data(df_resampled: pd.DataFrame):\n",
    "    \"\"\"A preprocessing function that creates a feature dataframe from the\n",
    "    raw measurement data and adds a column for an exogenous variable\n",
    "    which represents whether a given day is a business day.\n",
    "    This is done by\n",
    "    (1) Calculating the difference in volume between successive days,\n",
    "    (2) Setting the differences greater than 2 gallons equal to nan,\n",
    "    (3) calculating the median consumption\n",
    "    (4) Filling nan values with the median\n",
    "    (5) Computing a negative cumulative summation\n",
    "    (6) Applying a cumulative maximum\n",
    "    (7) Appending a column to the dataframe that represents a bus. day bool\n",
    "\n",
    "    :param df_resampled: A dataframe of resampled raw measurement data\n",
    "    :type df_resampled: pd.DataFrame\n",
    "    :returns: A dataframe of consumption features\n",
    "    \"\"\"\n",
    "    df_differences = df_resampled.diff().copy()\n",
    "    df_differences[df_differences > 2] = np.nan\n",
    "    median_consumption = df_differences.median()\n",
    "    df_differences.fillna(median_consumption)\n",
    "    df_features = (-df_differences.cumsum()).cummax()\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create TLM consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tlm_consumption(tlm_data: pd.DataFrame):\n",
    "    \"\"\"A function which generates the consumption for the TLM base data\n",
    "    params: tlm_data : A dataframe containing the TLM measurement readings\n",
    "    returns dataframe with consumption values for the TLM base data\n",
    "    \"\"\"\n",
    "    tlm_data_pivot = tlm_data.pivot(index=\"MeasurementDate\", columns=\"TankID\")\n",
    "    tlm_data_pivot.index = tlm_data_pivot.index.astype(\"M8[ns]\")\n",
    "    df_resampled = tlm_data_pivot.resample(\"D\").mean().interpolate()\n",
    "    df_features = _transform_data(df_resampled).diff()\n",
    "    median_consumption = df_features.median()\n",
    "    df_features.fillna(median_consumption, inplace=True)\n",
    "    df_features.columns = df_features.columns.droplevel(0)\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert tlm consumption from timeseries datset to a supervised leardning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "            data: Sequence of observations as a list or NumPy array.\n",
    "            n_in: Number of lag observations as input (X).\n",
    "            n_out: Number of observations as output (y).\n",
    "            dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "            Pandas DataFrame of series framed for supervised learning.\n",
    "            Dataframe has columns var(t-x) to var(t+y).\n",
    "            x,y are periods in training and test dataset respectively.\n",
    "            Column var(t-x) to var(t-1) represents input data (training data)\n",
    "            Columns var(t) to var(t+y) represent output data (testing data)\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit an xgboost model and make a one step prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_forecast(train, testX):\n",
    "    # transform list into array\n",
    "    train = asarray(train)\n",
    "    # split into input and output columns\n",
    "    trainX, trainy = train[:, :-1], train[:, -1]\n",
    "    # fit model\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=1000,n_jobs =-1)\n",
    "    model.fit(trainX, trainy)\n",
    "    # make a one-step prediction\n",
    "    yhat = model.predict(asarray([testX]))\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkforward validation for the XGboost model (Train and test the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(data, n_test):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        # fit model on history and make a prediction\n",
    "        yhat = xgboost_forecast(history, testX)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        # summarize progress\n",
    "        print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "    # estimate prediction error\n",
    "    error = mean_absolute_error(test[:, -1], predictions)\n",
    "    return error, test[:, -1], predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split a univariate dataset into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, n_test):\n",
    "    return data.iloc[:-n_test, ].values, data.iloc[-n_test:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Forecasts using XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_learning_model(tlm_consumption: pd.DataFrame,\n",
    "                              n_in: int,\n",
    "                              horizon_window: list = [42, 28],\n",
    "                              n_out: int = 1,\n",
    "                              ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tlm_consumption : pd.DataFrame\n",
    "        Dataframe with consumption.\n",
    "    n_in : int\n",
    "        Number of days to train the model for future forecasting.\n",
    "    n_out : int, optional\n",
    "        Number of days in future to predict for in one iteration.\n",
    "        The default is 1.\n",
    "    Returns A dataframe containing the future forecast\n",
    "    \"\"\"\n",
    "    forecast_dates = pd.date_range(\n",
    "        start=max(tlm_consumption.index), periods=(n_in+1), freq=\"D\")[1:]\n",
    "    supervised_forecast = pd.DataFrame(forecast_dates)\n",
    "    supervised_forecast.columns = ['Forecast_Date']\n",
    "    for tanks in tlm_consumption.columns:\n",
    "        tanks = str(tanks)\n",
    "        # Train the Model\n",
    "        supervised_forecasting_dataset = _series_to_supervised(\n",
    "            tlm_consumption[tanks].to_list(), n_in=n_in, n_out=n_out)\n",
    "        train = asarray(supervised_forecasting_dataset)\n",
    "        trainX, trainy = train[:, :-1], train[:, -1]\n",
    "        # fit model\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror', n_estimators=1000,n_jobs =-1)\n",
    "        model.fit(trainX, trainy)\n",
    "        row = tlm_consumption[tanks][-n_in:].values.flatten()\n",
    "        rows_forecasted = []\n",
    "        rows_forecasting = row\n",
    "        prediction = []\n",
    "        for days in range(n_in):\n",
    "            if len(prediction) > 0:\n",
    "                rows_forecasting = rows_forecasting[1:]\n",
    "                rows_forecasting = np.append(rows_forecasting, prediction)\n",
    "                rows_forecasting[-n_in:].flatten()\n",
    "            # make a one-step prediction\n",
    "            prediction = model.predict(asarray([rows_forecasting]))\n",
    "            rows_forecasted.append(prediction[0])\n",
    "            # print(rows_forecasted)\n",
    "        forecast_values = pd.DataFrame(\n",
    "            {'Forecast_Date': forecast_dates, tanks: rows_forecasted})\n",
    "        supervised_forecast = pd.merge(supervised_forecast,\n",
    "                                       forecast_values,\n",
    "                                       how='left',\n",
    "                                       left_on='Forecast_Date',\n",
    "                                       right_on='Forecast_Date')\n",
    "    return supervised_forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dates for test and train period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_training_dates(\n",
    "    t0: str = \"today\",\n",
    "    num_results: int = 1,\n",
    "    forecast_horizon: int = 42,\n",
    "    train_periods: int = 180,\n",
    "    skip: int = 1,\n",
    "    offset: int = 0,\n",
    "):\n",
    "    \"\"\"Generate start and end dates for training and test periods.\n",
    "\n",
    "    Args:\n",
    "      t0: The end date of collected data.In production, this should be \"today\".\n",
    "      In model training, this can be in the form of pandas accepted datetime\n",
    "      string. eg \"2020-08-31\"\n",
    "\n",
    "      num_results: The number of training and test periods. Number iterations to generate dates for\n",
    "\n",
    "      forecast_horizon: The forecast horizon in days\n",
    "\n",
    "      train_periods: The necessary training periods in days\n",
    "\n",
    "      skip:\n",
    "\n",
    "      offset: Shifts the start date\n",
    "\n",
    "    Returns:\n",
    "      A list of dictionaries of generated start and end dates for training\n",
    "      and test periods.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = pd.Timestamp(t0).normalize() - np.timedelta64(offset, \"D\")\n",
    "    t0 = t0 - pd.DateOffset(days=forecast_horizon)\n",
    "\n",
    "    check_points = pd.date_range(\n",
    "        end=t0, freq=str(skip) + \"D\", periods=num_results, closed=\"right\"\n",
    "    )\n",
    "\n",
    "    ts_cv_dates = [\n",
    "        {\n",
    "            \"start_train\": np.datetime64(\n",
    "                point - pd.DateOffset(days=train_periods - 1), \"D\"\n",
    "            ),\n",
    "            \"end_train\": np.datetime64(point - pd.DateOffset(days=1), \"D\"),\n",
    "            \"start_test\": np.datetime64(point - pd.DateOffset(days=0), \"D\"),\n",
    "            \"end_test\": np.datetime64(\n",
    "                point + pd.DateOffset(days=forecast_horizon), \"D\"\n",
    "            ),\n",
    "        }\n",
    "        for point in check_points\n",
    "    ]\n",
    "\n",
    "    return ts_cv_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# Update path to files appropriately\n",
    "tlm_base = pd.read_parquet('tlm_base.parquet')\n",
    "tank_context = pd.read_csv('tank_context.csv')\n",
    "hom_base = pd.read_parquet('hom_base.parquet')\n",
    "\n",
    "# Check Data types\n",
    "tlm_base.dtypes\n",
    "hom_base.dtypes\n",
    "tank_context.dtypes\n",
    "\n",
    "# Change Data Types for Tank ID to Str\n",
    "tlm_base.TankID = tlm_base.TankID.astype(str)\n",
    "tank_context.TankID = tank_context.TankID.astype(str)\n",
    "\n",
    "# Filter TLM base data for tanks in tank context\n",
    "tlm_base_tall = pd.merge(tlm_base,\n",
    "                         tank_context[['UID', 'TankID']].drop_duplicates(),\n",
    "                         left_on='TankID',\n",
    "                         right_on='TankID',\n",
    "                         how='inner')\n",
    "\n",
    "# Compute Consumption from TLM base data\n",
    "tlm_consumption = _create_tlm_consumption(tlm_data=tlm_base_tall)\n",
    "\n",
    "# Choose a tank for verifying function output\n",
    "supervised_forecasting = _series_to_supervised(\n",
    "    tlm_consumption['10017'].tolist(), n_in=42, n_out=42)\n",
    "\n",
    "# Train and test the model\n",
    "mae, y, yhat = walk_forward_validation(supervised_forecasting, 42)\n",
    "# Print MAPE score of the forecasti\n",
    "print('MAE: %.3f' % mae)\n",
    "\n",
    "# Testing for code output in prediction\n",
    "train = asarray(supervised_forecasting)\n",
    "trainX, trainy = train[:, :-1], train[:, -1]\n",
    "# fit model\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=1000,n_jobs=-1)\n",
    "model.fit(trainX, trainy)\n",
    "# construct an input for a new preduction\n",
    "row = tlm_consumption['10017'][-120:].values.flatten()\n",
    "prediction = model.predict(asarray([row]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through dates and generate forecasts for validation. This is to be converted into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast(date,num_results,forecast_horizon,offset):\n",
    "    \n",
    "    main_dir = 'Onedrive/doc/....'\n",
    "    \n",
    "    date_function_output = _get_training_dates(\n",
    "    t0= date,\n",
    "    num_results= num_results, # 18,\n",
    "    forecast_horizon= forecast_horizon, #42 - 1,\n",
    "    offset=offset#0,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    tlm_base = pd.read_parquet('tlm_base.parquet')\n",
    "    tank_context = pd.read_csv('tank_context.csv')\n",
    "    hom_base = pd.read_parquet('hom_base.parquet')\n",
    "\n",
    "    # Change Data Types for Tank ID to Str\n",
    "    tlm_base.TankID = tlm_base.TankID.astype(str)\n",
    "    tank_context.TankID = tank_context.TankID.astype(str)\n",
    "\n",
    "    tlm_base_tall = pd.merge(tlm_base,\n",
    "                             tank_context[['UID', 'TankID']].drop_duplicates(),\n",
    "                             left_on='TankID',\n",
    "                             right_on='TankID',\n",
    "                             how='inner')\n",
    "\n",
    "    # Compute Consumption from TLM base data\n",
    "    tlm_consumption = _create_tlm_consumption(tlm_data=tlm_base_tall)\n",
    "    \n",
    "    # tqdm added for progress bar\n",
    "    for j in tqdm(range(len(date_function_output))):\n",
    "    #     i = i + 1\n",
    "    #     print(i)\n",
    "        start_train = [x[\"start_train\"] for x in date_function_output][j]\n",
    "        end_train = [x[\"end_train\"] for x in date_function_output][j]\n",
    "        start_test = [x[\"start_test\"] for x in date_function_output][j]\n",
    "        end_test = [x[\"end_test\"] for x in date_function_output][j]\n",
    "        tlm_consumption_train = tlm_consumption[(tlm_consumption.index <= end_train) & (\n",
    "            tlm_consumption.index >= start_train)].copy()\n",
    "        tlm_actual_consumption = tlm_consumption[(tlm_consumption.index <= end_test) & (\n",
    "            tlm_consumption.index >= start_test)].copy()\n",
    "        xgboost_forecast_values = supervised_learning_model(\n",
    "            tlm_consumption=tlm_consumption_train,\n",
    "            n_in=42,\n",
    "        )\n",
    "        try:\n",
    "            # Uncomment and update path in next line\n",
    "            os.mkdir(\"XgBoost Model Validation Forecasts\")\n",
    "        except Exception:\n",
    "            print('Folder already exists')\n",
    "        date_for_file_name = str(end_train)\n",
    "        year = date_for_file_name.split(\"-\")[0]\n",
    "        month = date_for_file_name.split(\"-\")[1]\n",
    "        day = date_for_file_name.split(\"-\")[2]\n",
    "        \n",
    "        x = os.path.join(main_dir,'XgBoost Model Validation Forecasts')\n",
    "        y = os.path.join(x,str(year))\n",
    "        z = os.path.join(y,str(month))\n",
    "        dir_path = os.path.join(y,str(day))\n",
    "        \n",
    "        \n",
    "#         dir_path = \"XgBoost Model Validation Forecasts//\" + \\\n",
    "#             str(year) + '//' + str(month) + '//' + str(day)\n",
    "        \n",
    "        file_path = os.path.join(dir_path,'XGBoost Model Validation Forecast.csv')\n",
    "        file_path_actual = os.path.join(dir_path,'Actual Consumption.csv')\n",
    "        \n",
    "#         file_path = dir_path + \"//XGBoost Model Validation Forecast.csv\"\n",
    "\n",
    "#         file_path_actual = dir_path + \"//Actual Consumption.csv\"\n",
    "        try:\n",
    "            # Uncomment and update paths in next line\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            xgboost_forecast_values.to_csv(file_path, index=False)\n",
    "            tlm_actual_consumption.to_csv(file_path_actual)\n",
    "        except Exception:\n",
    "            xgboost_forecast_values.to_csv(file_path, index=False)\n",
    "            tlm_actual_consumption.to_csv(file_path_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_forecast('2022-01-27',18,42 - 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
