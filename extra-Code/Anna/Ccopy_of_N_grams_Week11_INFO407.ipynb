{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNxwHPHIFC_W"
   },
   "source": [
    "## **Programming with Python for Data Science**\n",
    "\n",
    "\n",
    "###**Lesson: N-grams** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZItgK3xMHeHC"
   },
   "source": [
    "## **Notebook Preparation for Lesson in 1•2 steps:**\n",
    "Each lesson will start with a similar template:  \n",
    "1. **save** the notebook to your google drive (copy to drive)<br/> ![](https://drive.google.com/uc?export=view&id=1NXb8jeYRc1yNCTz_duZdEpkm8uwRjOW5)\n",
    " \n",
    "2. **update** the NET_ID to be your netID (no need to include @illinois.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFNjvEMpHeHC"
   },
   "outputs": [],
   "source": [
    "LESSON_ID = 'p4ds:ds:ngrams'   # keep this as is\n",
    "NET_ID    = 'salonis3' # CHANGE_ME to your netID (keep the quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOBcS-X_SQ16"
   },
   "source": [
    "#**Lesson N-grams**\n",
    "###**More than one**\n",
    "You have now done the data science pipeline process from gathering raw data, transforming it, to creating a simple visualization. This first round is usually called **exploratory** analysis. For you, it was mostly about learning the nuances of Python and understanding the different pipeline stages. We purposely required that you do most of the hard work rather than use libraries to do it. It's critical that you, the data scientist, are competent enough with your programming skills that you don't feel trapped because some library doesn't do exactly what you need it to do -- they usually don't. Your programming (and critical thinking) skills are much more valuable than your ability to read documentation. This class is purposely designed not to be a recipe class. Once you have mastered the basic programming concepts, incorporating new libraries into your workflows is the easy part.\n",
    "\n",
    "###**Recap**\n",
    "Right now we have a pipeline that will produce a list of the most frequently used words (top_n) in a body of text. This by itself could be useful when comparing different authors' writing styles, genres, music lyrics, etc. The 'technical' term for what we have built is a **term-frequency table**.\n",
    "\n",
    "Term frequency measures how frequently a term occurs in a document. Since documents are different in length, it is possible that a term would appear much more in long documents than in shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms or tokens in the document) as a way of normalization. We will visit this concept again when we learn about tf-idf (term frequency inverse document frequency) which is used to evaluate how important a word is to a document. It helps identify which terms 'separate' the documents from each other.\n",
    "\n",
    "The term frequency table is also the input to tools that build word clouds where the most frequently used words are displayed in a larger font with bolder colors. This simple technique gets a lot of exposure because it communicates the idea quickly and is more interesting to look at (vs our bar chart).\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=17kXPzpK52XyTAetAUNnp99HzuUOJL8Dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0mGQNl-SQ1-"
   },
   "source": [
    "#**A BoW Model**\n",
    "\n",
    "When your text analysis drops the context (the order, structure, how the word is used) in which the words were used (like in the above word cloud), it's typically called a bag of words model. This model is very useful to extract features where the analysis is only concerned with whether or not a word was used. The bag of words is simplified to involve only the following:\n",
    "\n",
    "* A vocabulary of words (usually normalized to contain only letters, ignoring case)\n",
    "* A measure of presence of known words (e.g. a count)\n",
    "\n",
    "In this model, we say the feature is each count of the word.\n",
    "\n",
    "###**Improvements**\n",
    "In our second time around we are not only going to make some improvements to each step of the current process by using more powerful techniques and additional analysis but also to offload some of the coding to libraries. We can incrementally improve our simple model of analysis by making small, but important adjustments.\n",
    "\n",
    "###**N-grams**\n",
    "What you have accomplished (without perhaps knowing it), is that you built a language model based on single words called uni-grams. You essentially have a distribution model over single word usage. This concept can be expanded to include co-occurring words (words that follow each other) called bi-grams, sequences of three words that occur together (tri-grams)-- and so on. N-grams are basically a set of co-occurring words within a given window. For example, using the following sentence:\n",
    "```\n",
    "We went to a clump of bushes, and Tom made everybody swear to keep the secret, and then showed them a hole in the hill, right in the thickest part of the bushes.\n",
    "```\n",
    "We can show the bigrams (the variable N, is usually used and for bigrams, N == 2) for the first few windows:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=19iq3UaZD8szna0cydnAF6p7lxE0Fy0fb)\n",
    "\n",
    "Now if you do that and treat each pair as a 'word', you once again have a distribution over pairs of words. Using these pairs of \"grams\" along with using the same count analysis (unique keys in a dictionary), we get the following (top 5) results:\n",
    "\n",
    "```\n",
    "('in the',     2)\n",
    "('we went',    1)\n",
    "('went to',    1)\n",
    "('to clump',   1)\n",
    "('clump of',   1)\n",
    "```\n",
    "\n",
    "These pairs can be very useful at extracting information from a text (as you will see). The N-gram method is very powerful at modeling language with applications in optical character recognition (OCR -- if a word is hard to decode, you can use probabilities based on the previous word or following word, text generation that your predictive keyboard may use (e.g. You are super nice to see if you have any chance of that the first one¹), spell checkers, and so on. You can even create N-grams over sliding windows of characters (not just the words) to build distributions of likely letter/character pairings and use that to help guess what language is being typed by a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjebNTG6_j9H"
   },
   "source": [
    "###**Before you go, you should know:**\n",
    "\n",
    "\n",
    "* the meaning of term frequency table\n",
    "\n",
    "\n",
    "* what an ngram is and why it's useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj9YLVQ0SQ2Q"
   },
   "source": [
    "#**Lesson Assignment:  Ngrams**\n",
    "We will build a simple pipeline that does the following:\n",
    "\n",
    "* reads in the contents of Huckleberry Finn\n",
    "* tokenizes the contents into 'words'\n",
    "* builds a list of all bi-grams\n",
    "* find the most common bi-grams in the text\n",
    "\n",
    "The above pipeline will be used in other lessons and assignments. It's not only important to pass the tests for this lesson, but to understand the workflow. We will use this to eventually find characters in literature. Be sure to **TEST** each function before moving from one stage to the next. Do NOT attempt to write all the code and then 'run the tests'. You should build your own tests and make sure you understand each function before moving to the next one.\n",
    "\n",
    "###**Step 1: Read Text**\n",
    "Create a function read_text(filename) that opens the file and returns its contents. Don't hardcode the filename, use the parameter. Be sure to confirm you are reading all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KNz9l4AJSQ2V"
   },
   "outputs": [],
   "source": [
    "def read_text(filename):\n",
    "  # opens filename and returns its contents\n",
    "    with open(filename) as fp:\n",
    "        data  = fp.read()\n",
    "    return data\n",
    "\n",
    "def read_test():\n",
    "    path = 'sample.txt'\n",
    "    text = read_text(path)\n",
    "    print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWDndIqvBz1I"
   },
   "source": [
    "###**Step 2: Tokenizing**\n",
    "Now that we have seen regular expressions, we will use them to extract the tokens for this analysis. The one issue to discuss is how to handle apostrophes. Look at the following passage that shows some of the various usages of apostrophes:\n",
    "```\n",
    "With my sisters' 'ax' we are gonna 'chop off her legs!' I'm afraid; 'fraid I am. I was scout'n' for a place to take the table to shorten it for the kiddos.\n",
    "```\n",
    "\n",
    "The apostrophe can mark contractions (i.e. omission of letters), possession, the marking of plurals (e.g mind your p's and q's), quoting words within a quotation, marking of irony, and more (https://www.grammarbook.com/punctuation/apostro.asp). Even using an NLP (Natural Language Processing) toolkit to mark the different cases (which would require a lot of computation) will not promise error free results. In an attempt to strike a balance, we will use the following two rules for parsing words and dealing with apostrophes:\n",
    "\n",
    "###**Rule 1: Use the following regular expression to tokenize the text:**\n",
    "```\n",
    "['A-Za-z0-9]+-?['A-Za-z0-9]+\n",
    "```\n",
    "Note that this regular expression will\n",
    "\n",
    "* skip single letter words (and numbers)\n",
    "* not match double hyphenated words (Aunt--Poly) (it will be two matches)\n",
    "* keep single hyphenated words (e.g. iron-will)\n",
    "* include the apostrophe in all of its possible uses.\n",
    "\n",
    "Make sure you understand why the given regular expression has those limitations. Be sure you can read and understand that regular expression.\n",
    "\n",
    "###**Rule 2: normalize the tokens**\n",
    "Then for each of the returned tokens (from using the regular expression)\n",
    "\n",
    "* strip off any leading and trailing apostrophes\n",
    "* keep the internal ones\n",
    "* do NOT change the case of the word\n",
    "* you will need to use the powerful string methods you learned in the bootcamp\n",
    "\n",
    "Note that these rules keep contractions together (e.g. ain't, can't). And quoted words (e.g. 'food' ) will be the same as the non quoted version.\n",
    "\n",
    "Use the two rules above to implement the following split_text_into_tokens function. It must return a list of tokens (in the same order as the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5QAfKljHSQ2Y"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def split_text_into_tokens(text):\n",
    "    pattern = r\"['A-Za-z0-9]+-?['A-Za-z0-9]+\"\n",
    "    regex = re.compile(pattern)\n",
    "    return regex.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text   = read_text('sample.txt')\n",
    "# tokens = split_text_into_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otLi7rgUCfBP"
   },
   "source": [
    "Be sure your two functions work together. Print out tokens, if necessary.\n",
    "```\n",
    "text   = read_text('sample.txt')\n",
    "tokens = split_text_into_tokens(text)\n",
    "```\n",
    "\n",
    "###**Step 3: Bi-grams**\n",
    "Create a function named bi_grams(tokens) which returns a list of tuples. This function moves a sliding window of size 2 over tokens (a list), and creates a new list of bigrams tuples: each tuple has two elements, both of them tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3BHqunZNSQ2m"
   },
   "outputs": [],
   "source": [
    "def bi_grams(tokens):\n",
    "    lst = []\n",
    "    for i in range(len(tokens)-1):\n",
    "        lst.append(tuple(tokens[i:i+2]))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWP29lKxCwX_"
   },
   "source": [
    "You must build this algorithm. Do NOT use any library or the function zip to help you. Those will come in due time. It's more important to figure out how to do this (i.e. algorithm development) using the constructs of Python.\n",
    "\n",
    "Hint: you will need to use a loop.\n",
    "\n",
    "Your bi_grams function should work similarly to this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vpsbyAXqSQ2p"
   },
   "outputs": [],
   "source": [
    "def test_bigrams():\n",
    "    text = '''\n",
    "    it was the best of times\n",
    "    it was the worst of times\n",
    "    it was the age of wisdom\n",
    "    it was the age of foolishness\n",
    "    '''\n",
    "    tokens = text.split()\n",
    "    grams  = bi_grams(tokens)\n",
    "    print(grams)\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it', 'was'), ('was', 'the'), ('the', 'best'), ('best', 'of'), ('of', 'times'), ('times', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'worst'), ('worst', 'of'), ('of', 'times'), ('times', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'age'), ('age', 'of'), ('of', 'wisdom'), ('wisdom', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'age'), ('age', 'of'), ('of', 'foolishness')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('it', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'best'),\n",
       " ('best', 'of'),\n",
       " ('of', 'times'),\n",
       " ('times', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'worst'),\n",
       " ('worst', 'of'),\n",
       " ('of', 'times'),\n",
       " ('times', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', 'wisdom'),\n",
       " ('wisdom', 'it'),\n",
       " ('it', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', 'foolishness')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DP7FlRLTC5z4"
   },
   "source": [
    "\n",
    "**The following would be the output:**\n",
    "\n",
    "```[('it', 'was'), ('was', 'the'),('the', 'best'), ('best', 'of'), ('of', 'times'), ('times', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'worst'), ('worst', 'of'), ('of', 'times'), ('times', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'age'), ('age', 'of'), ('of', 'wisdom'), ('wisdom', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'age'), ('age', 'of'), ('of', 'foolishness')]```\n",
    "\n",
    "###**Step 4: Top N**\n",
    "For calculating top N lists, much like the previous lesson, we can leverage the Python library collections. It contains many handy utility functions. Please [read](https://docs.python.org/3.6/library/collections.html) the documentation.\n",
    "\n",
    "You can use (but not required) the Counter class. The following are two very handy ways to use the Counter class:\n",
    "```\n",
    "import collections\n",
    "words   = \"you are the one you are fun\".split()\n",
    "counter = collections.Counter(words)\n",
    "print(counter.most_common(10))\n",
    "```\n",
    "If it is necessary to use the count map within a loop, you can add the values one by one as well:\n",
    "\n",
    "```\n",
    "import collections\n",
    "words   = \"you are the one you are fun\".split()\n",
    "counter = collections.Counter()\n",
    "for w in words:\n",
    "  counter[w] += 1\n",
    "print(counter.most_common(10))\n",
    "```\n",
    "\n",
    "Create a function named top_n(tokens, n) which returns a list of tuples where each tuple contains the word followed by its count. The count is the number of times the token (a bi-gram) occurs in tokens. The parameter n is used to get the n most occurring tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rhDb2idLSQ2u"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def top_n(tokens, n):\n",
    "    counter = collections.Counter(tokens)\n",
    "    tokens = sorted(counter.items(),key = lambda x: x[1],reverse=True)[:n]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it', 'was'), ('was', 'the'), ('the', 'best'), ('best', 'of'), ('of', 'times'), ('times', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'worst'), ('worst', 'of'), ('of', 'times'), ('times', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'age'), ('age', 'of'), ('of', 'wisdom'), ('wisdom', 'it'), ('it', 'was'), ('was', 'the'), ('the', 'age'), ('age', 'of'), ('of', 'foolishness')]\n",
      "[(('it', 'was'), 4), (('was', 'the'), 4), (('of', 'times'), 2)]\n"
     ]
    }
   ],
   "source": [
    "def test_topn():\n",
    "  grams = test_bigrams()\n",
    "  top   = top_n(grams, 3)\n",
    "  print(top)\n",
    "test_topn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Q0pTSwGV0-"
   },
   "source": [
    "After this is done, the following should work:\n",
    "\n",
    "```\n",
    "def test_topn():\n",
    "  grams = test_bigrams()\n",
    "  top   = top_n(grams, 3)\n",
    "  print(top)\n",
    "```\n",
    "\n",
    "The output should match the following:\n",
    "\n",
    "```\n",
    "[(('it', 'was'), 4), (('was', 'the'), 4), (('of', 'times'), 2)]\n",
    "```\n",
    "\n",
    "###**Testing Huck**\n",
    "Before you submit, test your process by processing the text to the first chapter of Huckleberry Finn (data.txt)\n",
    "\n",
    "The top 5 bi-grams are:\n",
    "```\n",
    "  bi-grams    Count\n",
    "('in', 'the'),  421\n",
    "('of', 'the'),  333\n",
    "('and', 'the'), 310\n",
    "('it', 'was'),  290\n",
    "('to', 'the'),  233\n",
    "```\n",
    "\n",
    "That's not too useful. We will fix that in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in', 'the'), 421),\n",
       " (('of', 'the'), 333),\n",
       " (('and', 'the'), 310),\n",
       " (('it', 'was'), 290),\n",
       " (('to', 'the'), 233)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text   = read_text('data.txt')\n",
    "tokens = split_text_into_tokens(text)\n",
    "grams  = bi_grams(tokens)\n",
    "top_n(grams,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlXN7B6lSQ2w"
   },
   "source": [
    "##**Submission**\n",
    "\n",
    "After implementing all the functions and testing them please download the notebook as \"solution.py\" and submit to gradescope under \"Week11:DS:ngrams\" assignment tab and Moodle.\n",
    "\n",
    "**NOTES**\n",
    "\n",
    "* Be sure to use the function names and parameter names as given. \n",
    "* DONOT use your own function or parameter names. \n",
    "* Your file MUST be named \"solution.py\". \n",
    "* Comment out any lines of code and/or function calls to those functions that produce errors. If your solution has errors, then you have to work on them but if there were any errors in the examples/exercies then comment them before submitting to Gradescope.\n",
    "* Grading cannot be performed if any of these are violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oW9nawCIOc7"
   },
   "source": [
    "¹That is literally the sentence that was created by selecting the suggested word after typing You."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of N-grams_Week11_INFO407",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "story": {
   "auth_token": "i2-50D3GtwjjW1_LZfZE6fbVDLJyyBbRqG5R3ob2Y4s=",
   "authorship_tag": "AB",
   "chapters": 17,
   "name": "N-grams",
   "parser": {},
   "root": "https://github.com/habermanUIUC/CodeStories-lessons/blob/main/lessons/p4ds/ds/ngrams",
   "tag": "p4ds:ds:ngrams"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
