{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0y5yY_xUYib"
   },
   "source": [
    "## **Programming with Python for Data Science**\n",
    "\n",
    "###**Project: Finding Characters** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCI-8agAUYjI"
   },
   "source": [
    "## **Notebook Preparation for Lesson in 1•2 steps:**\n",
    "Each lesson will start with a similar template:  \n",
    "1. **save** the notebook to your google drive (copy to drive)<br/> ![](https://drive.google.com/uc?export=view&id=1NXb8jeYRc1yNCTz_duZdEpkm8uwRjOW5)\n",
    " \n",
    "2. **update** the NET_ID to be your netID (no need to include @illinois.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPU_mNz0UYjL"
   },
   "outputs": [],
   "source": [
    "LESSON_ID = 'p4ds:project:finding_characters'   # keep this as is\n",
    "NET_ID    = 'salonis3' # CHANGE_ME to your netID (keep the quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBQm3IKzUYjT"
   },
   "source": [
    "##**Lesson: Finding Characters**\n",
    "One of the goals of the Cliff Note Generator was to generate a list of characters in a novel. We can actually use our current skill set and include the techniques discussed in the nGrams lesson to extract (with a good level of accuracy) the main characters of a novel.\n",
    "\n",
    "We will also make some improvements with some of the parsing, cleaning, and preparation of the data. It would be best to read this entire lesson before doing any coding. Also note that this lesson is a bit different in that you will be responsible for more of the code writing. What is being specified is a minimum. We highly recommend that you decompose any complex processes into multiple functions.\n",
    "\n",
    "###**Preparation**\n",
    "Before doing anything, read through the entire set of directions first. You will get a sense of the restrictions and overall goals.\n",
    "\n",
    "###**Step 1**\n",
    "Fill in the functions from the previous lessons (ngrams and stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_dHP_RnUYjX"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# from Ngrams and Stopwords Lessons\n",
    "#\n",
    "\n",
    "# copy & paste your code\n",
    "# from ngrams lesson\n",
    "def read_text(filename):\n",
    "  # opens filename and returns its contents\n",
    "  return \"\"\n",
    "\n",
    "def split_text_into_tokens(text):\n",
    "  return [] \n",
    "  \n",
    "def bi_grams(tokens):\n",
    "  return []\n",
    "\n",
    "def top_n(tokens, n):\n",
    "  return []\n",
    "\n",
    "# from stopwords lesson\n",
    "def remove_stop_words(tokens, stoplist):\n",
    "  return []\n",
    "\n",
    "def load_stop_words(filename):\n",
    "  # read filename (stopwords.txt)\n",
    "  # and return a list of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REVW3_qH8AYL"
   },
   "source": [
    "###**Step 2 : Test your code.**\n",
    "The following should now work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAxYRDeyUYkI"
   },
   "outputs": [],
   "source": [
    "def demo_test():\n",
    "\n",
    "   text = read_text('huck.txt')\n",
    "   stop = load_stop_words('stopwords.txt')\n",
    "   \n",
    "   tokens  = split_text_into_tokens(text)\n",
    "   cleaned = remove_stop_words(tokens, stop)\n",
    "   grams = bi_grams(cleaned)\n",
    "   \n",
    "   print(top_n(grams, 10))\n",
    "\n",
    "demo_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVSvfSwNBCZ3"
   },
   "source": [
    "You should see the following:\n",
    "```\n",
    "[(('old', 'man'), 49), (('Mary', 'Jane'), 41), (('Tom', 'Sawyer'), 40), \n",
    "(('Aunt', 'Sally'), 39), (('pretty', 'soon'), 37), \n",
    "(('never', 'see'), 33), (('ever', 'see'), 29), (('Jim', 'said'), 28), \n",
    "(('every', 'time'), 25), (('come', 'along'), 24)]\n",
    "```\n",
    "\n",
    "Note how this compares to the output when we didn't account for the case of the stopword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vMEG9LzUYkR"
   },
   "source": [
    "###**Finding the Characters**\n",
    "With this machinery in place, we are ready to find characters in a novel (I hope you are reading this with great anticipation) using different strategies. Each of the strategies has a function to implement that strategy.\n",
    "\n",
    "####**Method #1**\n",
    "One attribute (or feature) of the text we are analyzing is that proper nouns are capitalized. Let’s capitalize on this and find all single words in the text whose first character is an uppercase letter and the word is **not** a stop word.\n",
    "\n",
    "Create and define the function find_characters_v1(text, stoplist, top):\n",
    "\n",
    "* Tokenize and clean the text using the function split_text_into_tokens\n",
    "* Filter the tokens so it has no stop words in it (regardless of case). The parameter stoplist is the array returned from load_stop_words\n",
    "* Create a new list of tokens (keep the order) of words that are capitalized. You can test the first character of the token.\n",
    "* Return the top words as a list of tuples (the first element is the word, the second is the count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAFHEtV9UYkf"
   },
   "outputs": [],
   "source": [
    "def find_characters_v1(text, stoplist, top):\n",
    "  return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MSV70s753PO"
   },
   "source": [
    "For Huck Finn, you should get the following (the output is formatted for clarity):\n",
    "```\n",
    "text = read_text('huck.txt')\n",
    "stop = load_stop_words('stopwords.txt')\n",
    "v1 = find_characters_v1(text,stop, 15)\n",
    "print(v1)\n",
    "```\n",
    "You should see:\n",
    "```\n",
    "('Jim', 341), \n",
    "('Well', 318), \n",
    "('Tom', 217), \n",
    "('Huck', 70), \n",
    "('Yes', 68), \n",
    "('Oh', 65), \n",
    "('Miss', 63), \n",
    "('Mary', 60), \n",
    "('Aunt', 53), \n",
    "('Now', 53), \n",
    "('Sally', 46), \n",
    "('CHAPTER', 43), \n",
    "('Sawyer', 43), \n",
    "('Jane', 43), \n",
    "('Buck', 38)\n",
    "```\n",
    "\n",
    "Notice with this very simple method we found 8 characters in the top 15 (those in bold). You also found an Aunt and a Miss too. You might be inclined to start fiddling with the stop-words. The one you could add is 'CHAPTER' and 'Well' -- the interjection, since we know that word does not provide much content in this context. But as we mentioned in the stop words lesson, that's a dangerous game, since other novels might include some of these:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1BztBBMwk5FSLxTy37naTUzeODVwg12kd)\n",
    "\n",
    "###**Method #2**\n",
    "Another feature of characters in a novel is that many of them have two names (Tom Sawyer, Aunt Polly, etc).\n",
    "\n",
    "Create and define the following function:\n",
    "```\n",
    "find_characters_v2(text, stoplist, top)\n",
    "```\n",
    "* Tokenize and clean the text using the function split_text_into_tokens\n",
    "* Convert the list of tokens into a list of bigrams (using your bi_grams method)\n",
    "* Filter out all bigrams to keep only the ones where both words are capitalized (just the first character)\n",
    "* Neither word should (either lower or upper) be in stoplist\n",
    "* Remember stoplist could be an empty list\n",
    "* Return the top bigrams as a list of tuples: The first element is the bigram tuple, the second is the count\n",
    "\n",
    "\n",
    "Note that we are **not** removing the stopwords from the text. We are now using the stopwords to make decisions on the text. The stopwords lesson has more details on this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaIjrT2fUYkh"
   },
   "outputs": [],
   "source": [
    "def find_characters_v2(text, stoplist, top):\n",
    "  return []\n",
    "\n",
    "\n",
    "v2  = find_characters_v2(text, [], 15)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VxqqepO7YOp"
   },
   "source": [
    "\n",
    "With the text of Huckleberry Finn, the following is the output with stopwords being the empty list:\n",
    "\n",
    "```\n",
    "(('Mary', 'Jane'), 41), \n",
    "(('Tom', 'Sawyer'), 40), \n",
    "(('Aunt', 'Sally'), 39), \n",
    "(('Miss', 'Watson'), 20), \n",
    "(('Miss', 'Mary'), 19), \n",
    "(('Mars', 'Tom'), 16), \n",
    "(('Huck', 'Finn'), 15), \n",
    "(('Uncle', 'Silas'), 15), \n",
    "(('Aunt', 'Polly'), 11), \n",
    "(('Judge', 'Thatcher'), 10), \n",
    "(('But', 'Tom'), 9),\n",
    "(('Ben', 'Rogers'), 8), \n",
    "(('So', 'Tom'), 9),\n",
    "(('St', 'Louis'), 7), \n",
    "(('Miss', 'Sophia'), 7)\n",
    "```\n",
    "\n",
    "That found 11 characters in the top 15 bigrams frequency table. This method is pretty good and the method didn't even consider stop words. What happens if you consider stop words?\n",
    "\n",
    "Note: in order to match these outputs, use the collections.Counter class. Otherwise, it's possible that your version of sorting will handle those tuples with equal counts differently (unstable sorting).\n",
    "\n",
    "###**Titles, a short diversion**\n",
    "Another feature of characters is that many of them have a title (also called honorifics) precede them (Dr. Mr. Mrs. Miss. Ms. Rev. Prof. Sir. etc). We will look for bi-grams that have these titles. However, we will **not** hard code the titles (we won't specify which titles to look for). We will let the data tell us what the 'titles' are.\n",
    "\n",
    "Here's the process to use to self discover titles:\n",
    "\n",
    "* Let's define a title as a capital letter followed by 1 to 3 lower case letters followed by a period. This is not perfect, but it captures a good majority of them.\n",
    "* Create a list named title_tokens whose text matches the above criteria (hint: use regular expressions) for example:\n",
    "title_tokens = regex1.findall(text)\n",
    "* Now we need to remove words that might have ended a sentence with those same title characteristics (e.g. Tom. Bill. Pat. Etc. ). These names could have been in a sentence like \"Please go Tom.\" Tom is **not** a title, but it would have been found by our definition.\n",
    "* Use the same definition for titles (above) but instead of ending with a period, the token must end with whitespace. The idea is that hopefully somewhere in the text the same name will appear but without a period. It’s very likely that you would encounter 'Tom' somewhere in the text without a period, but it’s unlikely that Mr., Mrs., Dr., etc would appear without a period. Let's call this list pseudo_titles.\n",
    "pseudo_titles = regex2.findall(text)\n",
    "* The set of titles is essentially the first list of tokens, title_tokens with all the tokens in the second set (pseudo_titles) removed. For example, the first list might have 'Dr.', 'Tom.' and 'Mr.' in it and the second set might have 'Tom' and 'Ted' in it. The final title list would be ['Dr', 'Mr'].\n",
    "\n",
    "Name your function get_titles that encapsulates the above logic; it should return a list of titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75aGHITJUYkq"
   },
   "outputs": [],
   "source": [
    "def get_titles(txt):\n",
    "   return [] # see process above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G96djC28uaB"
   },
   "source": [
    "Once you have get_titles working, the following should work:\n",
    "```\n",
    "text = read_text('huck.txt')\n",
    "titles = get_titles(text)\n",
    "print(sorted(titles))\n",
    "```\n",
    "You should get 7 computed titles in Huckleberry Finn:\n",
    "```\n",
    "['Col', 'Dr', 'Mr', 'Mrs', 'Otto', 'Rev', 'St']\n",
    "```\n",
    "\n",
    "Do not move forward until this is working.\n",
    "\n",
    "###**Method #3**\n",
    "Create and define the following function\n",
    "\n",
    "find_characters_v3(text, stoplist, top)\n",
    "\n",
    "* Tokenize and clean the text\n",
    "* Convert the list of tokens into a list of bigrams\n",
    "* Filter out all bigrams such that the first word in the bigram is a title and the second word is capitalized (hint: use the output of get_titles) **and** the second word (either lower or upper) should not be in stoplist\n",
    "* Return the top bigrams as a list of tuples: the first element is the bigram tuple, the second is the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bx1iMbPgUYkt"
   },
   "outputs": [],
   "source": [
    "def find_characters_v3(text, stoplist, top):\n",
    "  return []\n",
    "  \n",
    "\n",
    "text = read_text('huck.txt')\n",
    "stop = load_stop_words('stopwords.txt')\n",
    "v3 = find_characters_v3(text, stop, 15)\n",
    "print(v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Txbnnlm9OJw"
   },
   "source": [
    "For Huck Finn, you should get the following:\n",
    "```\n",
    "(('St', 'Louis'), 7), \n",
    "(('Mr', 'Lothrop's'), 6), \n",
    "(('Mrs', 'Phelps'), 4), \n",
    "(('St', 'Petersburg'), 3), \n",
    "(('Dr', 'Robinson'), 3), \n",
    "(('Mr', 'Garrick'), 2), \n",
    "(('Mr', 'Kean'), 2), \n",
    "(('Mr', 'Wilks'), 2),\n",
    "(('Mr', 'Mark'), 1), \n",
    "(('Mrs', 'Judith'), 1),\n",
    "(('Mr', 'Parker'), 1),\n",
    "(('Dr', 'Gunn's'), 1), \n",
    "(('Col', 'Grangerford'), 1), \n",
    "(('Dr', 'Armand'), 1), \n",
    "(('St', 'Jacques'), 1)\n",
    "```\n",
    "\n",
    "Clearly, that yields a lot of good information. Although looking at the counts, none of them are that prominent. We also found a few places as well as people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpkT3QwyUYkv"
   },
   "source": [
    "###**Machine Learning?**\n",
    "You may have heard of (and used) the NLTK Python library that’s a popular choice for processing text. These libraries include models that were built by processing large amounts of text. We will use both the NLTK and SpaCy NLP libraries to do something similar in another lesson. However, these libraries have models built from using large data sets to extract entities (called NER for named entity recognition). These entities include organizations, people, places, money.\n",
    "\n",
    "The models that were built essentially learned what features (like capitalization or title words) were important when analyzing text and came up with a model that attempts to do the same thing we did here. However, we hard coded the rules (use bigrams, remove stop words, look for capital letters, etc). This is sometimes referred to as a rule-based system. The analysis is built on manually crafted rules.\n",
    "\n",
    "In machine learning (sometimes referred to as an automatic system), some of the algorithms essentially learn what features are important (or can learn how much weight to apply to each feature) to build a model and then uses the model to classify tokens as named entities. The biggest issue is that these models could be built with a very different text source (e.g. journal articles or twitter feed) than what you are processing. Also the models themselves require a large set of resources (memory, cpu) that you may not have available. What you built in this lesson is efficient, fast and fairly accurate.\n",
    "\n",
    "In the follow-on course, you'll be able to build your own text-based models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffqu0iuJUYkx"
   },
   "source": [
    "##**Submission**\n",
    "\n",
    "After implementing all the functions and testing them please download the notebook as \"solution.py\" and submit to gradescope under \"Week13:Project:Finding_Characters\" assignment tab and Moodle.\n",
    "\n",
    "**NOTES**\n",
    "\n",
    "* Be sure to use the function names and parameter names as given. \n",
    "* DONOT use your own function or parameter names. \n",
    "* Your file MUST be named \"solution.py\". \n",
    "* Comment out any lines of code and/or function calls to those functions that produce errors. If your solution has errors, then you have to work on them but if there were any errors in the examples/exercies then comment them before submitting to Gradescope.\n",
    "* Grading cannot be performed if any of these are violated."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Finding Characters_Week13_INFO407",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "story": {
   "auth_token": "I6IJmwnzGX6taC_dLccYtmImNk92Zij6lJx1WS9PGM0=",
   "authorship_tag": "AB",
   "chapters": 18,
   "name": "Finding Characters",
   "parser": {},
   "root": "https://github.com/habermanUIUC/CodeStories-lessons/blob/main/lessons/p4ds/cc/finding_characters",
   "tag": "p4ds:cc:finding_characters"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
