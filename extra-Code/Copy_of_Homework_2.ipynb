{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtbfXH5tUzDE"
   },
   "source": [
    "## Homework 2, part II. Decision Trees\n",
    "\n",
    "We  will  use  a  dataset  of  1298  “fake  news”  headlines  (which  mostly  include \n",
    "headlines  of  articles  classified  as  biased,  etc.)  and  1968  “real”  news  headlines, \n",
    "where the “fake news” headlines are from https://www.kaggle.com/mrisdal/fake-news/data and “real news” headlines are from https://www.kaggle.com/therohk/million-headlines.  The  data  were  cleaned  by  removing  words  from  fake  news \n",
    "titles  that  are  not  a  part  of  the  headline,  removing  special  characters  from  the \n",
    "headlines,  and  restricting  real  news  headlines  to  those  after  October  2016 \n",
    "containing the word “trump”. The cleaned-up data are available as clean_real.txt \n",
    "and clean_fake.txt in the google colab file. \n",
    "\n",
    "Each headline appears as a single line in the data file. You will build a decision \n",
    "tree to classify real vs. fake news headlines. Instead of coding the decision trees \n",
    "yourself,  you  will  do  what  we  normally  do  in  practice  —  use  an  existing \n",
    "implementation.  You  should  use  the  DecisionTreeClassifier  included  in  sklearn. \n",
    "Note that figuring out how to use this implementation is a part of the \n",
    "assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXN70srFUZK_"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRraqZGq8US3"
   },
   "outputs": [],
   "source": [
    "#These two lines allow you to import the necessary data for this tutorial\n",
    "#You can open the links to see the content of those files if you are curious\n",
    "! wget https://raw.githubusercontent.com/carrasqu/datah2/master/data/clean_fake.txt \n",
    "! wget https://raw.githubusercontent.com/carrasqu/datah2/master/data/clean_real.txt\n",
    "\n",
    "# This function performs a data split (training, validation, test sets)\n",
    "def split_data(X, y, train_size=0.7, val_size=0.15):\n",
    "    total_data = X.shape[0] # This line allows to get the dimension of the first axis of X, which is the total number of data points\n",
    "    train_size = int(train_size * total_data)\n",
    "    val_size = int(val_size * total_data)\n",
    "    test_size = total_data - train_size - val_size\n",
    "\n",
    "    all_indices = np.random.permutation(np.arange(total_data)) #This line is used to randomize the indices of X and y before splitting into train, validation and test sets\n",
    "    train_indices = all_indices[:train_size]\n",
    "    val_indices = all_indices[train_size:train_size + val_size]\n",
    "    test_indices = all_indices[train_size+val_size:]\n",
    "\n",
    "    train_X, train_y = X[train_indices], y[train_indices]\n",
    "    val_X, val_y = X[val_indices], y[val_indices]\n",
    "    test_X, test_y = X[test_indices], y[test_indices]\n",
    "\n",
    "    #The output of this function below is a python dictionnay. For instance, to access the train data, you need to access data['train'], if the output of this function is called \"data\"\n",
    "    #More details about python dictionaries can be found on this link: https://realpython.com/python-dicts/\n",
    "    return {\n",
    "        'train': (train_X, train_y),\n",
    "        'val':  (val_X, val_y),\n",
    "        'test': (test_X, test_y)\n",
    "    }\n",
    "\n",
    "# This function loads, processes (with CountVectorizer, read and understand the documentation https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html )\n",
    "# and splits the data from the downloaded files \n",
    "def load_data(paths):\n",
    "    vec = CountVectorizer(input='content')\n",
    "    lines = []\n",
    "    counts = []\n",
    "    for p in paths: # This loop is used to read the files in each path. More details reading files can be found here: https://www.w3schools.com/python/python_file_open.asp\n",
    "        with open(p) as f:\n",
    "            file_lines = f.readlines()\n",
    "        counts.append(len(file_lines))\n",
    "        lines.extend([l.strip() for l in file_lines])\n",
    "\n",
    "    vec.fit(lines) #more details about \"fit\" are provided here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit\n",
    "    data_matrix = vec.transform(lines).toarray() #more details about \"transform\" can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform\n",
    "    y = np.concatenate((np.zeros(counts[0]), np.ones(counts[1]))) #Labels 0 are given for real data and Labels 1 for fake data. np.concatenate is used to merge the labels in one array before splitting the data in the next line.\n",
    "    return split_data(data_matrix, y), vec.get_feature_names_out()\n",
    "\n",
    "data, feature_names = load_data(['/content/clean_real.txt', '/content/clean_fake.txt'])    \n",
    "\n",
    "!rm clean* # this is to delete the data from the google colab after we downloaded them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUFOswd2CQce"
   },
   "source": [
    "## 2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv5opHLva2Oi"
   },
   "outputs": [],
   "source": [
    "# Extract the dimensionality of the feature vectors, the number of datapoints in the training, validation and test sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhzMFXRwCVh_"
   },
   "source": [
    "## 2.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bZzz-Yr8xgT"
   },
   "outputs": [],
   "source": [
    "# Complete a function to compute the accuracy of a given model on input data X\n",
    "# and label t\n",
    "# You can get some inspiration from the code of Homework 1\n",
    "def get_acc(model, X, t):\n",
    "    '''\n",
    "     Complete the code here\n",
    "    '''\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qCIimj0EKei"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Complete a function that defines and trains decision trees on different depths and split criterion on the data\n",
    "# store the model, the training accuracy and validation accuracy in the dict out \n",
    "# You can take a look at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# You can also read the user-guide https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "def select_model(depths, data, criterion ):\n",
    "    out = {}\n",
    "    for d in depths:\n",
    "        print('Evaluating on depth {}'.format(d))\n",
    "        out[d] = {}\n",
    "        '''\n",
    "        Your definition of the decision tree model goes here:\n",
    "        tree = ....\n",
    "        fit your decision tree here (you can use tree.fit, see documentation for more details)\n",
    "         \n",
    "        '''\n",
    "        out[d]['val'] = get_acc(tree, *data['val'])\n",
    "        out[d]['train'] = get_acc(tree, *data['train'])\n",
    "        out[d]['model'] = tree\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fw8yOeVPC_gb"
   },
   "source": [
    "## 2.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--BbC4R7H4WK"
   },
   "outputs": [],
   "source": [
    "# Code to train the models on multiple depths and two split criteria \n",
    "\n",
    "# train the models with the information gain criterion\n",
    "\n",
    "depths = [] # the depths you want to explore go in the depths list \n",
    "\n",
    "res_entropy = select_model(depths,data, \"entropy\") # training models with different depths using information gain\n",
    "\n",
    "# looping over the different models and accuracies to find the optimal model according to its validation accuracy\n",
    "best_d_entropy = None\n",
    "best_acc_entropy = 0\n",
    "\n",
    "for d in res_entropy:\n",
    "    val_acc = res_entropy[d]['val']\n",
    "    print(\"Depth: {}   Train: {}    Val: {}\".format(d, res_entropy[d]['train'], val_acc))\n",
    "    if val_acc  > best_acc_entropy:\n",
    "        best_d_entropy = d\n",
    "        best_acc_entropy = val_acc\n",
    "\n",
    "# train the models with the gini impurity criterion \n",
    "\n",
    "res_gini = select_model(depths,data,\"gini\") # training models with different depths using gini impurity \n",
    "\n",
    "# looping over the different models and accuracies to find the optimal model according to its validation accuracy\n",
    "best_d_gini = None\n",
    "best_acc_gini = 0\n",
    "\n",
    "for d in res_gini:\n",
    "    val_acc = res_gini[d]['val']\n",
    "    print(\"Depth: {}   Train: {}    Val: {}\".format(d, res_gini[d]['train'], val_acc))\n",
    "    if val_acc  > best_acc_gini:\n",
    "        best_d_gini = d\n",
    "        best_acc_gini = val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJCta3qSq3T4"
   },
   "outputs": [],
   "source": [
    "# Compute and report the test accuracy of the best model here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv3v5XD-DEja"
   },
   "source": [
    "## 2.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nvvxxq7XIBaB"
   },
   "outputs": [],
   "source": [
    "# visualize the two first two layers of the tree here if doing it by code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Homework 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
