{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web content text tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits titles from web articles into verbs, adjectives, adverbs, superlatives and named entities. Outputs the results to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to ignore the warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==2\n",
      "  Using cached spacy-2.0.0.tar.gz (13.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\avitr\\appdata\\roaming\\python\\python38\\site-packages (from spacy==2) (1.21.6)\n",
      "Collecting murmurhash<0.29,>=0.28\n",
      "  Using cached murmurhash-0.28.0.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cymem<1.32,>=1.30\n",
      "  Using cached cymem-1.31.2.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting preshed<2.0.0,>=1.0.0\n",
      "  Using cached preshed-1.0.1.tar.gz (112 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting thinc<6.11.0,>=6.10.0\n",
      "  Using cached thinc-6.10.3.tar.gz (1.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting plac<1.0.0,>=0.9.6\n",
      "  Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from spacy==2) (1.16.0)\n",
      "Collecting pathlib\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: ujson>=1.35 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from spacy==2) (4.0.2)\n",
      "Collecting dill<0.3,>=0.2\n",
      "  Using cached dill-0.2.9-py3-none-any.whl\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from spacy==2) (2.27.1)\n",
      "Collecting regex==2017.4.5\n",
      "  Using cached regex-2017.04.05.tar.gz (601 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy<5.0.0,>=4.4.2\n",
      "  Using cached ftfy-4.4.3-py3-none-any.whl\n",
      "Collecting msgpack-python\n",
      "  Using cached msgpack_python-0.5.6-cp38-cp38-win_amd64.whl\n",
      "Collecting msgpack-numpy\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy==2) (0.2.5)\n",
      "Requirement already satisfied: html5lib in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy==2) (1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2) (2021.10.8)\n",
      "Collecting msgpack<1.0.0,>=0.5.6\n",
      "  Using cached msgpack-0.6.2-cp38-cp38-win_amd64.whl\n",
      "Collecting cytoolz<0.10,>=0.9.0\n",
      "  Using cached cytoolz-0.9.0.1.tar.gz (443 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.0->spacy==2) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.0->spacy==2) (4.63.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.0->spacy==2) (0.11.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.10.0->thinc<6.11.0,>=6.10.0->spacy==2) (0.4.4)\n",
      "Requirement already satisfied: webencodings in c:\\users\\avitr\\anaconda3\\lib\\site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy==2) (0.5.1)\n",
      "Building wheels for collected packages: spacy, regex, cymem, murmurhash, preshed, thinc, cytoolz\n",
      "  Building wheel for spacy (setup.py): started\n",
      "  Building wheel for spacy (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for spacy\n",
      "  Building wheel for regex (setup.py): started\n",
      "  Building wheel for regex (setup.py): finished with status 'done'\n",
      "  Created wheel for regex: filename=regex-2017.4.5-cp38-cp38-win_amd64.whl size=247796 sha256=17079f4d8991c3d9749f871440c454c2213f7ad1dca2ed399d424ec518c37f3d\n",
      "  Stored in directory: c:\\users\\avitr\\appdata\\local\\pip\\cache\\wheels\\45\\6d\\d9\\1c9b861321c9240122cb967b734a80545c9f465be4fcb16f19\n",
      "  Building wheel for cymem (setup.py): started\n",
      "  Building wheel for cymem (setup.py): finished with status 'done'\n",
      "  Created wheel for cymem: filename=cymem-1.31.2-cp38-cp38-win_amd64.whl size=25362 sha256=a7ac31f3ad45c4645825b31c067036ccb3f9645cbc7c3c8862cb81094652be4b\n",
      "  Stored in directory: c:\\users\\avitr\\appdata\\local\\pip\\cache\\wheels\\7d\\79\\cb\\1a676ab9c2ea1be46f7d4ce7c687dac4aac9d45ee9935ed6e3\n",
      "  Building wheel for murmurhash (setup.py): started\n",
      "  Building wheel for murmurhash (setup.py): finished with status 'done'\n",
      "  Created wheel for murmurhash: filename=murmurhash-0.28.0-cp38-cp38-win_amd64.whl size=14624 sha256=c7a4de2712bc9e8aeb5175153f154d0bea6a3879e823193743d2926be742ce60\n",
      "  Stored in directory: c:\\users\\avitr\\appdata\\local\\pip\\cache\\wheels\\c4\\94\\18\\da70ce5a85c38612b346994f18e348e28e8c6006f6b2daf784\n",
      "  Building wheel for preshed (setup.py): started\n",
      "  Building wheel for preshed (setup.py): finished with status 'done'\n",
      "  Created wheel for preshed: filename=preshed-1.0.1-cp38-cp38-win_amd64.whl size=79698 sha256=6a5d5ee2430dfe28981d6f45e48d8beb46101442d63bbcf84ab642e8f637f46c\n",
      "  Stored in directory: c:\\users\\avitr\\appdata\\local\\pip\\cache\\wheels\\49\\19\\ca\\7c61cfaa11914a9f562cd0d1f3ee745c049d1eef08d458e995\n",
      "  Building wheel for thinc (setup.py): started\n",
      "  Building wheel for thinc (setup.py): finished with status 'done'\n",
      "  Created wheel for thinc: filename=thinc-6.10.3-cp38-cp38-win_amd64.whl size=1904588 sha256=280ca0b4b0c6c49d9992867129bb5ef1d894397de932d8adf9765b10043e2d25\n",
      "  Stored in directory: c:\\users\\avitr\\appdata\\local\\pip\\cache\\wheels\\fb\\8a\\f0\\814ec96cace85b0afd73928e7c6396f270d6a61443c9b1ff57\n",
      "  Building wheel for cytoolz (setup.py): started\n",
      "  Building wheel for cytoolz (setup.py): finished with status 'done'\n",
      "  Created wheel for cytoolz: filename=cytoolz-0.9.0.1-cp38-cp38-win_amd64.whl size=378974 sha256=8299a7fa23add23e450e674f73915ca1ab0aaca174d6b0ae56cc3c3582975c3a\n",
      "  Stored in directory: c:\\users\\avitr\\appdata\\local\\pip\\cache\\wheels\\6f\\49\\e5\\c1170703a39737d0b37581805c6d5b8cb99a430814e52c7a07\n",
      "Successfully built regex cymem murmurhash preshed thinc cytoolz\n",
      "Failed to build spacy\n",
      "Installing collected packages: regex, plac, pathlib, murmurhash, msgpack-python, msgpack, cymem, preshed, msgpack-numpy, dill, cytoolz, thinc, ftfy, spacy\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.8.3\n",
      "    Uninstalling regex-2021.8.3:\n",
      "      Successfully uninstalled regex-2021.8.3\n",
      "  Attempting uninstall: plac\n",
      "    Found existing installation: plac 1.3.5\n",
      "    Uninstalling plac-1.3.5:\n",
      "      Successfully uninstalled plac-1.3.5\n",
      "  Attempting uninstall: murmurhash\n",
      "    Found existing installation: murmurhash 1.0.5\n",
      "    Uninstalling murmurhash-1.0.5:\n",
      "      Successfully uninstalled murmurhash-1.0.5\n",
      "  Attempting uninstall: msgpack\n",
      "    Found existing installation: msgpack 1.0.2\n",
      "    Uninstalling msgpack-1.0.2:\n",
      "      Successfully uninstalled msgpack-1.0.2\n",
      "  Attempting uninstall: cymem\n",
      "    Found existing installation: cymem 2.0.5\n",
      "    Uninstalling cymem-2.0.5:\n",
      "      Successfully uninstalled cymem-2.0.5\n",
      "  Attempting uninstall: preshed\n",
      "    Found existing installation: preshed 3.0.5\n",
      "    Uninstalling preshed-3.0.5:\n",
      "      Successfully uninstalled preshed-3.0.5\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: cytoolz\n",
      "    Found existing installation: cytoolz 0.11.0\n",
      "    Uninstalling cytoolz-0.11.0:\n",
      "      Successfully uninstalled cytoolz-0.11.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.15\n",
      "    Uninstalling thinc-8.0.15:\n",
      "      Successfully uninstalled thinc-8.0.15\n",
      "  Attempting uninstall: ftfy\n",
      "    Found existing installation: ftfy 6.1.1\n",
      "    Uninstalling ftfy-6.1.1:\n",
      "      Successfully uninstalled ftfy-6.1.1\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.2.3\n",
      "    Uninstalling spacy-3.2.3:\n",
      "      Successfully uninstalled spacy-3.2.3\n",
      "  Running setup.py install for spacy: started\n",
      "  Running setup.py install for spacy: finished with status 'error'\n",
      "  Rolling back uninstall of spacy\n",
      "  Moving to c:\\users\\avitr\\anaconda3\\lib\\site-packages\\spacy-3.2.3.dist-info\\\n",
      "   from C:\\Users\\avitr\\anaconda3\\Lib\\site-packages\\~pacy-3.2.3.dist-info\n",
      "  Moving to c:\\users\\avitr\\anaconda3\\lib\\site-packages\\spacy\\\n",
      "   from C:\\Users\\avitr\\anaconda3\\Lib\\site-packages\\~-acy\n",
      "  Moving to c:\\users\\avitr\\anaconda3\\scripts\\spacy.exe\n",
      "   from C:\\Users\\avitr\\AppData\\Local\\Temp\\pip-uninstall-lii0amqo\\spacy.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [536 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\about.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\compat.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\glossary.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\language.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\scorer.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\util.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\_ml.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\__main__.py -> build\\lib.win-amd64-3.8\\spacy\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\convert.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\download.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\evaluate.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\info.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\link.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\package.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\profile.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\train.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\validate.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\vocab.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  copying spacy\\cli\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\cli\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\data\n",
      "  copying spacy\\data\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\data\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\displacy\n",
      "  copying spacy\\displacy\\render.py -> build\\lib.win-amd64-3.8\\spacy\\displacy\n",
      "  copying spacy\\displacy\\templates.py -> build\\lib.win-amd64-3.8\\spacy\\displacy\n",
      "  copying spacy\\displacy\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\displacy\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\char_classes.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\entity_rules.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\norm_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\punctuation.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  copying spacy\\lang\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  copying spacy\\tests\\conftest.py -> build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  copying spacy\\tests\\test_matcher.py -> build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  copying spacy\\tests\\test_misc.py -> build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  copying spacy\\tests\\test_pickles.py -> build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  copying spacy\\tests\\test_underscore.py -> build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  copying spacy\\tests\\util.py -> build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  copying spacy\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\printers.py -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\underscore.py -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\cli\\converters\n",
      "  copying spacy\\cli\\converters\\conllu2json.py -> build\\lib.win-amd64-3.8\\spacy\\cli\\converters\n",
      "  copying spacy\\cli\\converters\\conll_ner2json.py -> build\\lib.win-amd64-3.8\\spacy\\cli\\converters\n",
      "  copying spacy\\cli\\converters\\iob2json.py -> build\\lib.win-amd64-3.8\\spacy\\cli\\converters\n",
      "  copying spacy\\cli\\converters\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\cli\\converters\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  copying spacy\\lang\\bn\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  copying spacy\\lang\\bn\\morph_rules.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  copying spacy\\lang\\bn\\punctuation.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  copying spacy\\lang\\bn\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  copying spacy\\lang\\bn\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  copying spacy\\lang\\bn\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  copying spacy\\lang\\bn\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\bn\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\da\n",
      "  copying spacy\\lang\\da\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\da\n",
      "  copying spacy\\lang\\da\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\da\n",
      "  copying spacy\\lang\\da\\morph_rules.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\da\n",
      "  copying spacy\\lang\\da\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\da\n",
      "  copying spacy\\lang\\da\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\da\n",
      "  copying spacy\\lang\\da\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\da\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\norm_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\punctuation.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\syntax_iterators.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  copying spacy\\lang\\de\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\de\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\morph_rules.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\norm_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\syntax_iterators.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  copying spacy\\lang\\en\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  copying spacy\\lang\\es\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  copying spacy\\lang\\es\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  copying spacy\\lang\\es\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  copying spacy\\lang\\es\\syntax_iterators.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  copying spacy\\lang\\es\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  copying spacy\\lang\\es\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  copying spacy\\lang\\es\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\es\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\fi\n",
      "  copying spacy\\lang\\fi\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fi\n",
      "  copying spacy\\lang\\fi\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fi\n",
      "  copying spacy\\lang\\fi\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fi\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\punctuation.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\syntax_iterators.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\_tokenizer_exceptions_list.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  copying spacy\\lang\\fr\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\fr\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\ga\n",
      "  copying spacy\\lang\\ga\\irish_morphology_helpers.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ga\n",
      "  copying spacy\\lang\\ga\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ga\n",
      "  copying spacy\\lang\\ga\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ga\n",
      "  copying spacy\\lang\\ga\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ga\n",
      "  copying spacy\\lang\\ga\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ga\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\he\n",
      "  copying spacy\\lang\\he\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\he\n",
      "  copying spacy\\lang\\he\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\he\n",
      "  copying spacy\\lang\\he\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\he\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\hi\n",
      "  copying spacy\\lang\\hi\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hi\n",
      "  copying spacy\\lang\\hi\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hi\n",
      "  copying spacy\\lang\\hi\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hi\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\hr\n",
      "  copying spacy\\lang\\hr\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hr\n",
      "  copying spacy\\lang\\hr\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hr\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\hu\n",
      "  copying spacy\\lang\\hu\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hu\n",
      "  copying spacy\\lang\\hu\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hu\n",
      "  copying spacy\\lang\\hu\\punctuation.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hu\n",
      "  copying spacy\\lang\\hu\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hu\n",
      "  copying spacy\\lang\\hu\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hu\n",
      "  copying spacy\\lang\\hu\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\hu\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\norm_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\punctuation.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\syntax_iterators.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\_tokenizer_exceptions_list.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  copying spacy\\lang\\id\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\id\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\it\n",
      "  copying spacy\\lang\\it\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\it\n",
      "  copying spacy\\lang\\it\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\it\n",
      "  copying spacy\\lang\\it\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\it\n",
      "  copying spacy\\lang\\it\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\it\n",
      "  copying spacy\\lang\\it\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\it\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\ja\n",
      "  copying spacy\\lang\\ja\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ja\n",
      "  copying spacy\\lang\\ja\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ja\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\nb\n",
      "  copying spacy\\lang\\nb\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nb\n",
      "  copying spacy\\lang\\nb\\morph_rules.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nb\n",
      "  copying spacy\\lang\\nb\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nb\n",
      "  copying spacy\\lang\\nb\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nb\n",
      "  copying spacy\\lang\\nb\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nb\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\nl\n",
      "  copying spacy\\lang\\nl\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nl\n",
      "  copying spacy\\lang\\nl\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nl\n",
      "  copying spacy\\lang\\nl\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nl\n",
      "  copying spacy\\lang\\nl\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\nl\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\pl\n",
      "  copying spacy\\lang\\pl\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pl\n",
      "  copying spacy\\lang\\pl\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pl\n",
      "  copying spacy\\lang\\pl\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pl\n",
      "  copying spacy\\lang\\pl\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pl\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  copying spacy\\lang\\pt\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  copying spacy\\lang\\pt\\lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  copying spacy\\lang\\pt\\lex_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  copying spacy\\lang\\pt\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  copying spacy\\lang\\pt\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  copying spacy\\lang\\pt\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  copying spacy\\lang\\pt\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\pt\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\ro\n",
      "  copying spacy\\lang\\ro\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ro\n",
      "  copying spacy\\lang\\ro\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ro\n",
      "  copying spacy\\lang\\ro\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\ro\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\sv\n",
      "  copying spacy\\lang\\sv\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\sv\n",
      "  copying spacy\\lang\\sv\\morph_rules.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\sv\n",
      "  copying spacy\\lang\\sv\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\sv\n",
      "  copying spacy\\lang\\sv\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\sv\n",
      "  copying spacy\\lang\\sv\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\sv\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\th\n",
      "  copying spacy\\lang\\th\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\th\n",
      "  copying spacy\\lang\\th\\tag_map.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\th\n",
      "  copying spacy\\lang\\th\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\th\n",
      "  copying spacy\\lang\\th\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\th\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\tr\n",
      "  copying spacy\\lang\\tr\\stop_words.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\tr\n",
      "  copying spacy\\lang\\tr\\tokenizer_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\tr\n",
      "  copying spacy\\lang\\tr\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\tr\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\xx\n",
      "  copying spacy\\lang\\xx\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\xx\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\zh\n",
      "  copying spacy\\lang\\zh\\examples.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\zh\n",
      "  copying spacy\\lang\\zh\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\zh\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\lookup.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_adjectives.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_adjectives_irreg.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_adverbs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_adverbs_irreg.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_lemma_rules.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_nouns.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_nouns_irreg.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_verbs.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\_verbs_irreg.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  copying spacy\\lang\\en\\lemmatizer\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\en\\lemmatizer\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\lang\\sv\\lemmatizer\n",
      "  copying spacy\\lang\\sv\\lemmatizer\\lookup.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\sv\\lemmatizer\n",
      "  copying spacy\\lang\\sv\\lemmatizer\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\lang\\sv\\lemmatizer\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_add_entities.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_array.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_creation.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_doc_api.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_pickle_doc.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_span.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_span_merge.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\test_token_api.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  copying spacy\\tests\\doc\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\doc\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\gold\n",
      "  copying spacy\\tests\\gold\\test_biluo.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\gold\n",
      "  copying spacy\\tests\\gold\\test_lev_align.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\gold\n",
      "  copying spacy\\tests\\gold\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\gold\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\n",
      "  copying spacy\\tests\\lang\\test_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\n",
      "  copying spacy\\tests\\lang\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_add_label.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_beam_parse.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_ner.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_neural_parser.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_nn_beam.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_nonproj.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_parse.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_parse_navigate.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_preset_sbd.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_space_attachment.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\test_to_from_bytes_disk.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  copying spacy\\tests\\parser\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\parser\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\pipeline\n",
      "  copying spacy\\tests\\pipeline\\test_pipe_methods.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\pipeline\n",
      "  copying spacy\\tests\\pipeline\\test_textcat.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\pipeline\n",
      "  copying spacy\\tests\\pipeline\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\pipeline\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue118.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1242.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1250.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1253.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1257.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1305.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1375.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1380.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1387.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1434.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1450.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue1488.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue242.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue309.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue351.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue360.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue361.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue401.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue429.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue514.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue54.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue587.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue588.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue589.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue590.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue595.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue599.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue600.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue615.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue686.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue693.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue704.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue717.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue719.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue736.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue740.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue743.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue744.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue758.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue759.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue768.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue775.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue781.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue792.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue801.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue805.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue834.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue850.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue852.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue859.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue886.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue891.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue903.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue910.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue912.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue957.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue995.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\test_issue999.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  copying spacy\\tests\\regression\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\regression\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialization.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_empty_model.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_extension_attrs.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_parser_ner.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_stringstore.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_tagger.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_tensorizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\test_serialize_vocab.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  copying spacy\\tests\\serialize\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\serialize\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\stringstore\n",
      "  copying spacy\\tests\\stringstore\\test_freeze_string_store.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\stringstore\n",
      "  copying spacy\\tests\\stringstore\\test_stringstore.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\stringstore\n",
      "  copying spacy\\tests\\stringstore\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\stringstore\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n",
      "  copying spacy\\tests\\tokenizer\\test_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  copying spacy\\tests\\tokenizer\\test_naughty_strings.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n",
      "  copying spacy\\tests\\tokenizer\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n",
      "  copying spacy\\tests\\tokenizer\\test_urls.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n",
      "  copying spacy\\tests\\tokenizer\\test_whitespace.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n",
      "  copying spacy\\tests\\tokenizer\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\vectors\n",
      "  copying spacy\\tests\\vectors\\test_similarity.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\vectors\n",
      "  copying spacy\\tests\\vectors\\test_vectors.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\vectors\n",
      "  copying spacy\\tests\\vectors\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\vectors\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\vocab\n",
      "  copying spacy\\tests\\vocab\\test_add_vectors.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\vocab\n",
      "  copying spacy\\tests\\vocab\\test_lexeme.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\vocab\n",
      "  copying spacy\\tests\\vocab\\test_vocab_api.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\vocab\n",
      "  copying spacy\\tests\\vocab\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\vocab\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\bn\n",
      "  copying spacy\\tests\\lang\\bn\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\bn\n",
      "  copying spacy\\tests\\lang\\bn\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\bn\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\da\n",
      "  copying spacy\\tests\\lang\\da\\test_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\da\n",
      "  copying spacy\\tests\\lang\\da\\test_text.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\da\n",
      "  copying spacy\\tests\\lang\\da\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\da\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  copying spacy\\tests\\lang\\de\\test_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  copying spacy\\tests\\lang\\de\\test_lemma.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  copying spacy\\tests\\lang\\de\\test_models.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  copying spacy\\tests\\lang\\de\\test_parser.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  copying spacy\\tests\\lang\\de\\test_prefix_suffix_infix.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  copying spacy\\tests\\lang\\de\\test_text.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  copying spacy\\tests\\lang\\de\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\de\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_customized_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_indices.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_lemmatizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_models.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_ner.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_noun_chunks.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_parser.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_prefix_suffix_infix.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_punct.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_sbd.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_tagger.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\test_text.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  copying spacy\\tests\\lang\\en\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\en\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\es\n",
      "  copying spacy\\tests\\lang\\es\\test_exception.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\es\n",
      "  copying spacy\\tests\\lang\\es\\test_text.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\es\n",
      "  copying spacy\\tests\\lang\\es\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\es\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fi\n",
      "  copying spacy\\tests\\lang\\fi\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fi\n",
      "  copying spacy\\tests\\lang\\fi\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fi\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fr\n",
      "  copying spacy\\tests\\lang\\fr\\test_exceptions.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fr\n",
      "  copying spacy\\tests\\lang\\fr\\test_lemmatization.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fr\n",
      "  copying spacy\\tests\\lang\\fr\\test_text.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fr\n",
      "  copying spacy\\tests\\lang\\fr\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\fr\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\ga\n",
      "  copying spacy\\tests\\lang\\ga\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\ga\n",
      "  copying spacy\\tests\\lang\\ga\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\ga\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\he\n",
      "  copying spacy\\tests\\lang\\he\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\he\n",
      "  copying spacy\\tests\\lang\\he\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\he\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\hu\n",
      "  copying spacy\\tests\\lang\\hu\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\hu\n",
      "  copying spacy\\tests\\lang\\hu\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\hu\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\id\n",
      "  copying spacy\\tests\\lang\\id\\test_prefix_suffix_infix.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\id\n",
      "  copying spacy\\tests\\lang\\id\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\id\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\ja\n",
      "  copying spacy\\tests\\lang\\ja\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\ja\n",
      "  copying spacy\\tests\\lang\\ja\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\ja\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\nb\n",
      "  copying spacy\\tests\\lang\\nb\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\nb\n",
      "  copying spacy\\tests\\lang\\nb\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\nb\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\sv\n",
      "  copying spacy\\tests\\lang\\sv\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\sv\n",
      "  copying spacy\\tests\\lang\\sv\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\sv\n",
      "  creating build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\th\n",
      "  copying spacy\\tests\\lang\\th\\test_tokenizer.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\th\n",
      "  copying spacy\\tests\\lang\\th\\__init__.py -> build\\lib.win-amd64-3.8\\spacy\\tests\\lang\\th\n",
      "  copying spacy\\attrs.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\gold.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\lexeme.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\matcher.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\morphology.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\parts_of_speech.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\pipeline.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\strings.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\symbols.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\tokenizer.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\typedefs.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\vectors.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\vocab.pyx -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\attrs.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\gold.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\lexeme.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\morphology.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\parts_of_speech.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\pipeline.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\strings.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\structs.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\symbols.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\tokenizer.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\typedefs.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\vocab.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\__init__.pxd -> build\\lib.win-amd64-3.8\\spacy\n",
      "  copying spacy\\syntax\\arc_eager.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\ner.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\nn_parser.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\nonproj.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\stateclass.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\transition_system.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\_beam_utils.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\_state.pyx -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\arc_eager.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\ner.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\nn_parser.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\nonproj.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\stateclass.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\transition_system.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\_state.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\syntax\\__init__.pxd -> build\\lib.win-amd64-3.8\\spacy\\syntax\n",
      "  copying spacy\\tokens\\doc.pyx -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\span.pyx -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\token.pyx -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\doc.pxd -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\span.pxd -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\token.pxd -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tokens\\__init__.pxd -> build\\lib.win-amd64-3.8\\spacy\\tokens\n",
      "  copying spacy\\tests\\tokenizer\\sun.txt -> build\\lib.win-amd64-3.8\\spacy\\tests\\tokenizer\n",
      "  running build_ext\n",
      "  building 'spacy.parts_of_speech' extension\n",
      "  creating build\\temp.win-amd64-3.8\n",
      "  creating build\\temp.win-amd64-3.8\\Release\n",
      "  creating build\\temp.win-amd64-3.8\\Release\\spacy\n",
      "  C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\\Users\\avitr\\anaconda3\\include -IC:\\Users\\avitr\\AppData\\Local\\Temp\\pip-install-cmxma65l\\spacy_2628beb2709d4dd2a659b629d8399f53\\include -IC:\\Users\\avitr\\anaconda3\\include -IC:\\Users\\avitr\\anaconda3\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" /EHsc /Tpspacy/parts_of_speech.cpp /Fobuild\\temp.win-amd64-3.8\\Release\\spacy/parts_of_speech.obj /Ox /EHsc\n",
      "  parts_of_speech.cpp\n",
      "  spacy/parts_of_speech.cpp(3837): warning C4996: '_typeobject::tp_print': deprecated in 3.8\n",
      "  C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\\Users\\avitr\\anaconda3\\libs /LIBPATH:C:\\Users\\avitr\\anaconda3\\PCbuild\\amd64 \"/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\lib\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\\\lib\\10.0.19041.0\\\\um\\x64\" /EXPORT:PyInit_parts_of_speech build\\temp.win-amd64-3.8\\Release\\spacy/parts_of_speech.obj /OUT:build\\lib.win-amd64-3.8\\spacy\\parts_of_speech.cp38-win_amd64.pyd /IMPLIB:build\\temp.win-amd64-3.8\\Release\\spacy\\parts_of_speech.cp38-win_amd64.lib\n",
      "     Creating library build\\temp.win-amd64-3.8\\Release\\spacy\\parts_of_speech.cp38-win_amd64.lib and object build\\temp.win-amd64-3.8\\Release\\spacy\\parts_of_speech.cp38-win_amd64.exp\n",
      "  Generating code\n",
      "  Finished generating code\n",
      "  building 'spacy.strings' extension\n",
      "  C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\\Users\\avitr\\anaconda3\\include -IC:\\Users\\avitr\\AppData\\Local\\Temp\\pip-install-cmxma65l\\spacy_2628beb2709d4dd2a659b629d8399f53\\include -IC:\\Users\\avitr\\anaconda3\\include -IC:\\Users\\avitr\\anaconda3\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" /EHsc /Tpspacy/strings.cpp /Fobuild\\temp.win-amd64-3.8\\Release\\spacy/strings.obj /Ox /EHsc\n",
      "  strings.cpp\n",
      "  spacy/strings.cpp(1617): warning C4244: 'argument': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  spacy/strings.cpp(2183): warning C4244: '=': conversion from 'long' to 'unsigned char', possible loss of data\n",
      "  spacy/strings.cpp(2761): warning C4244: 'argument': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  spacy/strings.cpp(3103): warning C4244: 'argument': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  spacy/strings.cpp(3114): warning C4244: 'argument': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  spacy/strings.cpp(3493): warning C4244: 'argument': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  spacy/strings.cpp(3508): warning C4018: '<': signed/unsigned mismatch\n",
      "  spacy/strings.cpp(5381): warning C4244: 'argument': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  spacy/strings.cpp(6136): warning C4996: '_typeobject::tp_print': deprecated in 3.8\n",
      "  spacy/strings.cpp(6191): warning C4996: '_typeobject::tp_print': deprecated in 3.8\n",
      "  spacy/strings.cpp(7128): error C2039: 'exc_type': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7129): error C2039: 'exc_value': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7130): error C2039: 'exc_traceback': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7137): error C2039: 'exc_type': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7138): error C2039: 'exc_value': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7139): error C2039: 'exc_traceback': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7140): error C2039: 'exc_type': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7141): error C2039: 'exc_value': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7142): error C2039: 'exc_traceback': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7187): error C2039: 'exc_type': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7188): error C2039: 'exc_value': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7189): error C2039: 'exc_traceback': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7190): error C2039: 'exc_type': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7191): error C2039: 'exc_value': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(7192): error C2039: 'exc_traceback': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(8361): error C2039: 'exc_type': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(8362): error C2039: 'exc_value': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(8363): error C2039: 'exc_traceback': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(8364): error C2039: 'exc_type': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(8365): error C2039: 'exc_value': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  spacy/strings.cpp(8366): error C2039: 'exc_traceback': is not a member of '_ts'\n",
      "  C:\\Users\\avitr\\anaconda3\\include\\cpython/pystate.h(51): note: see declaration of '_ts'\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.32.31326\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for spacy\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for spacy did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [9 lines of output]\n",
      "  running install\n",
      "  running build\n",
      "  running build_py\n",
      "  running build_ext\n",
      "  building 'spacy.parts_of_speech' extension\n",
      "  C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\\Users\\avitr\\anaconda3\\include -IC:\\Users\\avitr\\AppData\\Local\\Temp\\pip-install-cmxma65l\\spacy_2628beb2709d4dd2a659b629d8399f53\\include -IC:\\Users\\avitr\\anaconda3\\include -IC:\\Users\\avitr\\anaconda3\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" /EHsc /Tpspacy/parts_of_speech.cpp /Fobuild\\temp.win-amd64-3.8\\Release\\spacy/parts_of_speech.obj /Ox /EHsc\n",
      "  parts_of_speech.cpp\n",
      "  c1xx: fatal error C1083: Cannot open source file: 'spacy/parts_of_speech.cpp': No such file or directory\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.32.31326\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  WARNING: No metadata found in c:\\users\\avitr\\anaconda3\\lib\\site-packages\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "spacy\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\avitr\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename  = 'data/source/find-keywords.xlsx'\n",
    "output_filename = 'data/find-keywords-nouns.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the articles and display a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article count:  593\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Url</th>\n",
       "      <th>Title</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ParaDocs provides our clients with the highest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AGNITY Global Inc., (AGNITY) is a global provi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FlashCo has grown to be one of the largest man...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PAC Worldwide is a global leader in the manufa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>DynexÂ® Technologies, Inc. is an original pion...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Auris Surgical Robotics, Inc. is a technology ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sense Corp powers insight-driven organizations...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>We provide quality software and uncompromising...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Driven by the ever-changing needs of our clien...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Americo Manufacturing Company, headquartered i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Url                                              Title  Unnamed: 2  \\\n",
       "0  NaN  ParaDocs provides our clients with the highest...         NaN   \n",
       "1  NaN  AGNITY Global Inc., (AGNITY) is a global provi...         NaN   \n",
       "2  NaN  FlashCo has grown to be one of the largest man...         NaN   \n",
       "3  NaN  PAC Worldwide is a global leader in the manufa...         NaN   \n",
       "4  NaN  DynexÂ® Technologies, Inc. is an original pion...         NaN   \n",
       "5  NaN  Auris Surgical Robotics, Inc. is a technology ...         NaN   \n",
       "6  NaN  Sense Corp powers insight-driven organizations...         NaN   \n",
       "7  NaN  We provide quality software and uncompromising...         NaN   \n",
       "8  NaN  Driven by the ever-changing needs of our clien...         NaN   \n",
       "9  NaN  Americo Manufacturing Company, headquartered i...         NaN   \n",
       "\n",
       "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Abstract  \n",
       "0         NaN         NaN         NaN       NaN  \n",
       "1         NaN         NaN         NaN       NaN  \n",
       "2         NaN         NaN         NaN       NaN  \n",
       "3         NaN         NaN         NaN       NaN  \n",
       "4         NaN         NaN         NaN       NaN  \n",
       "5         NaN         NaN         NaN       NaN  \n",
       "6         NaN         NaN         NaN       NaN  \n",
       "7         NaN         NaN         NaN       NaN  \n",
       "8         NaN         NaN         NaN       NaN  \n",
       "9         NaN         NaN         NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(input_filename)\n",
    "print('Article count: ', len(df))\n",
    "# For testing\n",
    "# df = df.head(10)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spacy, a Natural Language Processing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prefer_gpu' from 'thinc.api' (C:\\Users\\avitr\\anaconda3\\lib\\site-packages\\thinc\\api.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# proper english model needs to be loaded.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# nlp = spacy.load('en_core_web_sm')\u001b[39;00m\n\u001b[0;32m      5\u001b[0m spacy\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'prefer_gpu' from 'thinc.api' (C:\\Users\\avitr\\anaconda3\\lib\\site-packages\\thinc\\api.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# proper english model needs to be loaded.\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(texts):\n",
    "    docs = [doc for doc in nlp.pipe(texts, batch_size=500)]\n",
    "    return docs\n",
    "\n",
    "def to_text(tokens):\n",
    "    return next(map(lambda token: token.orth_, tokens), '')\n",
    "\n",
    "def filter_first_punct(noun_chunks):\n",
    "    noun_chunks = list(noun_chunks)\n",
    "    if len(noun_chunks) > 0:\n",
    "        print('ROOT', noun_chunks[0].sent[noun_chunks[0].start])\n",
    "    return []\n",
    "\n",
    "def get_nouns(sentences):\n",
    "    return [to_text(docs.noun_chunks) for docs in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the titles and abstracts. Token types available [here](https://spacy.io/docs/usage/pos-tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docs'] = tokenize_text(df['Title'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['docs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(article_docs):\n",
    "    # print('Domain:       ', url)\n",
    "    print('Title:     ', article_docs)\n",
    "    print('-------------')\n",
    "    print('Words:     ', list(map(lambda word: word, article_docs)))\n",
    "    print('Lemma:     ', list(map(lambda word: word.lemma_, article_docs)))\n",
    "    print('Types:     ', list(map(lambda word: word.pos_, article_docs)))\n",
    "    print('Tags:      ', list(map(lambda word: word.tag_, article_docs)))\n",
    "    print('>')\n",
    "    print('Nouns:     ', list(filter(lambda word: word.pos_ == 'NOUN' or word.tag_ == 'NNP' or word.tag_ == 'NNPS', article_docs)))\n",
    "    print('Nouns sentences (chunks):     ', get_nouns(article_docs.sents))\n",
    "    print('Noun chunks:', list(article_docs.noun_chunks))\n",
    "    print('Noun chunks +1 words:', list(filter(lambda chunk: len(str(chunk).split(' ')) >= 2, list(article_docs.noun_chunks))))\n",
    "    print('Verbs:     ', list(filter(lambda word: word.pos_ == 'VERB', article_docs)))\n",
    "    print('Verbs Lemma:', list(map(lambda word: word.lemma_, filter(lambda word: word.pos_ == 'VERB', article_docs))))\n",
    "    print('Adjectives:', list(filter(lambda word: word.pos_ == 'ADJ', article_docs)))\n",
    "    print('Adjs Lemma:', list(map(lambda word: word.lemma_, filter(lambda word: word.pos_ == 'ADJ', article_docs))))\n",
    "    print('Adverbs:   ', list(filter(lambda word: word.pos_ == 'ADV', article_docs)))\n",
    "    print('Adverbs Lemma:', list(map(lambda word: word.lemma_, filter(lambda word: word.pos_ == 'ADV', article_docs))))\n",
    "    print('Superlatives:', list(filter(lambda word: word.tag_ == 'JJS' or word.tag_ == 'RBS', article_docs)))\n",
    "    print('Entities:  ', list(map(lambda entity: (entity, entity.label_), article_docs.ents)))\n",
    "    \n",
    "def df_url_docs(id, docs_field = 'docs'):\n",
    "    return df[docs_field][id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually check that the correct data types have been identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:      ParaDocs provides our clients with the highest level of emergency medical standby. Professionalism, discretion, and clinical excellence are our core values. From large public concerts to intimate private dinners we can provide an array of medical services to fit your needs. We provide only the most highly trained EMTâ€™s, paramedics, doctors, or ambulances services.\n",
      "-------------\n",
      "Words:      [ParaDocs, provides, our, clients, with, the, highest, level, of, emergency, medical, standby, ., Professionalism, ,, discretion, ,, and, clinical, excellence, are, our, core, values, ., From, large, public, concerts, to, intimate, private, dinners, we, can, provide, an, array, of, medical, services, to, fit, your, needs, ., We, provide, only, the, most, highly, trained, EMTâ€, ™, s, ,, paramedics, ,, doctors, ,, or, ambulances, services, .]\n",
      "Lemma:      ['ParaDocs', 'provide', 'our', 'client', 'with', 'the', 'high', 'level', 'of', 'emergency', 'medical', 'standby', '.', 'professionalism', ',', 'discretion', ',', 'and', 'clinical', 'excellence', 'be', 'our', 'core', 'value', '.', 'from', 'large', 'public', 'concert', 'to', 'intimate', 'private', 'dinner', 'we', 'can', 'provide', 'an', 'array', 'of', 'medical', 'service', 'to', 'fit', 'your', 'need', '.', 'we', 'provide', 'only', 'the', 'most', 'highly', 'train', 'EMTâ€', '™', 's', ',', 'paramedic', ',', 'doctor', ',', 'or', 'ambulance', 'service', '.']\n",
      "Types:      ['PROPN', 'VERB', 'PRON', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'CCONJ', 'ADJ', 'NOUN', 'AUX', 'PRON', 'NOUN', 'NOUN', 'PUNCT', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'PART', 'VERB', 'ADJ', 'NOUN', 'PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PART', 'VERB', 'PRON', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'ADV', 'DET', 'ADV', 'ADV', 'VERB', 'PROPN', 'PROPN', 'PART', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'CCONJ', 'VERB', 'NOUN', 'PUNCT']\n",
      "Tags:       ['NNP', 'VBZ', 'PRP$', 'NNS', 'IN', 'DT', 'JJS', 'NN', 'IN', 'NN', 'JJ', 'NN', '.', 'NN', ',', 'NN', ',', 'CC', 'JJ', 'NN', 'VBP', 'PRP$', 'NN', 'NNS', '.', 'IN', 'JJ', 'JJ', 'NNS', 'TO', 'VB', 'JJ', 'NNS', 'PRP', 'MD', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'TO', 'VB', 'PRP$', 'NNS', '.', 'PRP', 'VBP', 'RB', 'DT', 'RBS', 'RB', 'VBN', 'NNP', 'NNPS', 'POS', ',', 'NNS', ',', 'NNS', ',', 'CC', 'VBZ', 'NNS', '.']\n",
      ">\n",
      "Nouns:      [ParaDocs, clients, level, emergency, standby, Professionalism, discretion, excellence, core, values, concerts, dinners, array, services, needs, EMTâ€, ™, paramedics, doctors, services]\n",
      "Nouns sentences (chunks):      ['ParaDocs', 'Professionalism', 'large public concerts', 'We']\n",
      "Noun chunks: [ParaDocs, our clients, the highest level, emergency medical standby, Professionalism, discretion, clinical excellence, our core values, large public concerts, private dinners, we, an array, medical services, your needs, We, only the most highly trained EMTâ€, ™, paramedics, doctors, ambulances services]\n",
      "Noun chunks +1 words: [our clients, the highest level, emergency medical standby, clinical excellence, our core values, large public concerts, private dinners, an array, medical services, your needs, only the most highly trained EMTâ€, ambulances services]\n",
      "Verbs:      [provides, intimate, provide, fit, provide, trained, ambulances]\n",
      "Verbs Lemma: ['provide', 'intimate', 'provide', 'fit', 'provide', 'train', 'ambulance']\n",
      "Adjectives: [highest, medical, clinical, large, public, private, medical]\n",
      "Adjs Lemma: ['high', 'medical', 'clinical', 'large', 'public', 'private', 'medical']\n",
      "Adverbs:    [only, most, highly]\n",
      "Adverbs Lemma: ['only', 'most', 'highly']\n",
      "Superlatives: [highest, most]\n",
      "Entities:   [(ParaDocs, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "print_tokens(df_url_docs(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through every article and applies f() to it. \n",
    "# Then applies token_extractor() to convert from a Token to a string.\n",
    "# Finally, concatenates the tokens of a single type with commas\n",
    "def filter_bad_excel_strings(tokens_string):\n",
    "    tokens_string = re.sub('[\\000-\\010]|[\\013-\\014]|[\\016-\\037]', '', tokens_string)\n",
    "    if tokens_string.startswith(\"=\"):\n",
    "        return tokens_string[1:]\n",
    "    elif tokens_string.startswith(\"- \"):\n",
    "        return tokens_string[2:]\n",
    "    else:\n",
    "        return tokens_string\n",
    "\n",
    "def map_articles(token_extractor, f, articles):\n",
    "    def map_article(article):\n",
    "        tokens_string = \",\".join(map(token_extractor, f(article)))\n",
    "        # Replace excel invalid chars\n",
    "        tokens_strings = filter_bad_excel_strings(tokens_string)\n",
    "        return tokens_string\n",
    "    return list(map(map_article, articles))\n",
    "\n",
    "def make_excel_df(docs_column_name = 'docs'):\n",
    "    df_excel = pd.DataFrame()\n",
    "    docs = df[docs_column_name]\n",
    "    df_excel['Title'] = df['Title']\n",
    "    df_excel['Nouns'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'NOUN' or word.tag_ == 'NNP' or word.tag_ == 'NNPS', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Noun Chunks (1)'] = map_articles(lambda token: token, \n",
    "                                     lambda doc : get_nouns(doc.sents), \n",
    "                                     docs)\n",
    "    df_excel['Noun Chunks (2)'] = map_articles(\n",
    "                                     lambda chunk: str(chunk),\n",
    "                                     lambda doc : list(doc.noun_chunks), \n",
    "                                     docs)\n",
    "    df_excel['Noun Chunks (3) +1 words'] = map_articles(\n",
    "                                     lambda chunk: str(chunk),\n",
    "                                     lambda doc : list(filter(lambda chunk: len(str(chunk).split(' ')) >= 2, list(doc.noun_chunks))), \n",
    "                                     docs)\n",
    "    #df_excel['Noun Chunks +1 words (3)'] = map_articles(lambda token: token, \n",
    "    #                                 list(filter(lambda chunk: len(str(chunk).split(' ')) >= 2, list(article_docs.noun_chunks))))\n",
    "    df_excel['Verbs'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'VERB', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Verbs Lemma'] = map_articles(lambda token: token.lemma_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'VERB', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adjectives'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADJ', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adjectives Lemma'] = map_articles(lambda token: token.lemma_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADJ', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adverbs'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADV', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Adverbs Lemma'] = map_articles(lambda token: token.lemma_, \n",
    "                                     lambda sentence : filter(lambda word: word.pos_ == 'ADV', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Superlatives'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.tag_ == 'JJS' or word.tag_ == 'RBS', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Superlatives Lemma'] = map_articles(lambda token: token.orth_, \n",
    "                                     lambda sentence : filter(lambda word: word.tag_ == 'JJS' or word.tag_ == 'RBS', sentence), \n",
    "                                     docs)\n",
    "    df_excel['Entities'] = map_articles(lambda ent: ent.orth_, \n",
    "                                     lambda sentence : sentence.ents, \n",
    "                                     docs)\n",
    "    return df_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Noun Chunks (1)</th>\n",
       "      <th>Noun Chunks (2)</th>\n",
       "      <th>Noun Chunks (3) +1 words</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Verbs Lemma</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Adjectives Lemma</th>\n",
       "      <th>Adverbs</th>\n",
       "      <th>Adverbs Lemma</th>\n",
       "      <th>Superlatives</th>\n",
       "      <th>Superlatives Lemma</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ParaDocs provides our clients with the highest...</td>\n",
       "      <td>ParaDocs,clients,level,emergency,standby,Profe...</td>\n",
       "      <td>ParaDocs,Professionalism,large public concerts,We</td>\n",
       "      <td>ParaDocs,our clients,the highest level,emergen...</td>\n",
       "      <td>our clients,the highest level,emergency medica...</td>\n",
       "      <td>provides,intimate,provide,fit,provide,trained,...</td>\n",
       "      <td>provide,intimate,provide,fit,provide,train,amb...</td>\n",
       "      <td>highest,medical,clinical,large,public,private,...</td>\n",
       "      <td>high,medical,clinical,large,public,private,med...</td>\n",
       "      <td>only,most,highly</td>\n",
       "      <td>only,most,highly</td>\n",
       "      <td>highest,most</td>\n",
       "      <td>highest,most</td>\n",
       "      <td>ParaDocs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGNITY Global Inc., (AGNITY) is a global provi...</td>\n",
       "      <td>AGNITY,Global,Inc.,AGNITY,provider,Intelligent...</td>\n",
       "      <td>AGNITY Global Inc.,™s products leverage,All AG...</td>\n",
       "      <td>AGNITY Global Inc.,AGNITY,a global provider,In...</td>\n",
       "      <td>AGNITY Global Inc.,a global provider,Intellige...</td>\n",
       "      <td>™,enable,transform,become,capitalize,powered,l...</td>\n",
       "      <td>™,enable,transform,become,capitalize,power,lea...</td>\n",
       "      <td>global,real,workplace,competitive,new,secured,...</td>\n",
       "      <td>global,real,workplace,competitive,new,secured,...</td>\n",
       "      <td>more</td>\n",
       "      <td>more</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AGNITY Global Inc.,Intelligent Business Commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FlashCo has grown to be one of the largest man...</td>\n",
       "      <td>FlashCo,manufacturers,roof,flashings,accessori...</td>\n",
       "      <td>FlashCo,We,FlashCo,Our designs,FlashCo,FlashCo...</td>\n",
       "      <td>FlashCo,the largest manufacturers,roof flashin...</td>\n",
       "      <td>the largest manufacturers,roof flashings,North...</td>\n",
       "      <td>grown,be,providing,designed,made,consider,regi...</td>\n",
       "      <td>grow,be,provide,design,make,consider,register,...</td>\n",
       "      <td>largest,committed,lasting,available,significan...</td>\n",
       "      <td>large,committed,lasting,available,significant,...</td>\n",
       "      <td>best,longest,strongly,when</td>\n",
       "      <td>well,long,strongly,when</td>\n",
       "      <td>largest,best,longest,best,best,most,highest,ne...</td>\n",
       "      <td>largest,best,longest,best,best,most,highest,ne...</td>\n",
       "      <td>FlashCo,North America,FlashCo,FlashCo,FlashCo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAC Worldwide is a global leader in the manufa...</td>\n",
       "      <td>PAC,Worldwide,leader,manufacturing,distributio...</td>\n",
       "      <td>PAC Worldwide,Our products,We</td>\n",
       "      <td>PAC Worldwide,a global leader,the manufacturin...</td>\n",
       "      <td>PAC Worldwide,a global leader,the manufacturin...</td>\n",
       "      <td>branded,include,provide,branded,receive</td>\n",
       "      <td>brand,include,provide,brand,receive</td>\n",
       "      <td>global,protective,flat,poly,highest,lasting,po...</td>\n",
       "      <td>global,protective,flat,poly,high,lasting,positive</td>\n",
       "      <td>now</td>\n",
       "      <td>now</td>\n",
       "      <td>highest</td>\n",
       "      <td>highest</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DynexÂ® Technologies, Inc. is an original pion...</td>\n",
       "      <td>®,Technologies,Inc.,pioneer,technology,staff,p...</td>\n",
       "      <td>® Technologies,Our talented, multidisciplinary...</td>\n",
       "      <td>® Technologies,Inc.,an original pioneer,microp...</td>\n",
       "      <td>® Technologies,an original pioneer,microplate ...</td>\n",
       "      <td>include,manufacturing,deliver,cutting,meet,imp...</td>\n",
       "      <td>include,manufacture,deliver,cut,meet,improve,e...</td>\n",
       "      <td>original,microplate,talented,multidisciplinary...</td>\n",
       "      <td>original,microplate,talented,multidisciplinary...</td>\n",
       "      <td>approximately,worldwide,most,ultimately,also,w...</td>\n",
       "      <td>approximately,worldwide,most,ultimately,also,w...</td>\n",
       "      <td>most</td>\n",
       "      <td>most</td>\n",
       "      <td>Technologies, Inc.,approximately 100,DSXÂ,DS2Â...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  ParaDocs provides our clients with the highest...   \n",
       "1  AGNITY Global Inc., (AGNITY) is a global provi...   \n",
       "2  FlashCo has grown to be one of the largest man...   \n",
       "3  PAC Worldwide is a global leader in the manufa...   \n",
       "4  DynexÂ® Technologies, Inc. is an original pion...   \n",
       "\n",
       "                                               Nouns  \\\n",
       "0  ParaDocs,clients,level,emergency,standby,Profe...   \n",
       "1  AGNITY,Global,Inc.,AGNITY,provider,Intelligent...   \n",
       "2  FlashCo,manufacturers,roof,flashings,accessori...   \n",
       "3  PAC,Worldwide,leader,manufacturing,distributio...   \n",
       "4  ®,Technologies,Inc.,pioneer,technology,staff,p...   \n",
       "\n",
       "                                     Noun Chunks (1)  \\\n",
       "0  ParaDocs,Professionalism,large public concerts,We   \n",
       "1  AGNITY Global Inc.,™s products leverage,All AG...   \n",
       "2  FlashCo,We,FlashCo,Our designs,FlashCo,FlashCo...   \n",
       "3                      PAC Worldwide,Our products,We   \n",
       "4  ® Technologies,Our talented, multidisciplinary...   \n",
       "\n",
       "                                     Noun Chunks (2)  \\\n",
       "0  ParaDocs,our clients,the highest level,emergen...   \n",
       "1  AGNITY Global Inc.,AGNITY,a global provider,In...   \n",
       "2  FlashCo,the largest manufacturers,roof flashin...   \n",
       "3  PAC Worldwide,a global leader,the manufacturin...   \n",
       "4  ® Technologies,Inc.,an original pioneer,microp...   \n",
       "\n",
       "                            Noun Chunks (3) +1 words  \\\n",
       "0  our clients,the highest level,emergency medica...   \n",
       "1  AGNITY Global Inc.,a global provider,Intellige...   \n",
       "2  the largest manufacturers,roof flashings,North...   \n",
       "3  PAC Worldwide,a global leader,the manufacturin...   \n",
       "4  ® Technologies,an original pioneer,microplate ...   \n",
       "\n",
       "                                               Verbs  \\\n",
       "0  provides,intimate,provide,fit,provide,trained,...   \n",
       "1  ™,enable,transform,become,capitalize,powered,l...   \n",
       "2  grown,be,providing,designed,made,consider,regi...   \n",
       "3            branded,include,provide,branded,receive   \n",
       "4  include,manufacturing,deliver,cutting,meet,imp...   \n",
       "\n",
       "                                         Verbs Lemma  \\\n",
       "0  provide,intimate,provide,fit,provide,train,amb...   \n",
       "1  ™,enable,transform,become,capitalize,power,lea...   \n",
       "2  grow,be,provide,design,make,consider,register,...   \n",
       "3                brand,include,provide,brand,receive   \n",
       "4  include,manufacture,deliver,cut,meet,improve,e...   \n",
       "\n",
       "                                          Adjectives  \\\n",
       "0  highest,medical,clinical,large,public,private,...   \n",
       "1  global,real,workplace,competitive,new,secured,...   \n",
       "2  largest,committed,lasting,available,significan...   \n",
       "3  global,protective,flat,poly,highest,lasting,po...   \n",
       "4  original,microplate,talented,multidisciplinary...   \n",
       "\n",
       "                                    Adjectives Lemma  \\\n",
       "0  high,medical,clinical,large,public,private,med...   \n",
       "1  global,real,workplace,competitive,new,secured,...   \n",
       "2  large,committed,lasting,available,significant,...   \n",
       "3  global,protective,flat,poly,high,lasting,positive   \n",
       "4  original,microplate,talented,multidisciplinary...   \n",
       "\n",
       "                                             Adverbs  \\\n",
       "0                                   only,most,highly   \n",
       "1                                               more   \n",
       "2                         best,longest,strongly,when   \n",
       "3                                                now   \n",
       "4  approximately,worldwide,most,ultimately,also,w...   \n",
       "\n",
       "                                       Adverbs Lemma  \\\n",
       "0                                   only,most,highly   \n",
       "1                                               more   \n",
       "2                            well,long,strongly,when   \n",
       "3                                                now   \n",
       "4  approximately,worldwide,most,ultimately,also,w...   \n",
       "\n",
       "                                        Superlatives  \\\n",
       "0                                       highest,most   \n",
       "1                                                      \n",
       "2  largest,best,longest,best,best,most,highest,ne...   \n",
       "3                                            highest   \n",
       "4                                               most   \n",
       "\n",
       "                                  Superlatives Lemma  \\\n",
       "0                                       highest,most   \n",
       "1                                                      \n",
       "2  largest,best,longest,best,best,most,highest,ne...   \n",
       "3                                            highest   \n",
       "4                                               most   \n",
       "\n",
       "                                            Entities  \n",
       "0                                           ParaDocs  \n",
       "1  AGNITY Global Inc.,Intelligent Business Commun...  \n",
       "2  FlashCo,North America,FlashCo,FlashCo,FlashCo,...  \n",
       "3                                                     \n",
       "4  Technologies, Inc.,approximately 100,DSXÂ,DS2Â...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_excel_titles = make_excel_df()\n",
    "df_excel_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(output_filename)\n",
    "df_excel_titles.to_excel(writer,'Titles')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
