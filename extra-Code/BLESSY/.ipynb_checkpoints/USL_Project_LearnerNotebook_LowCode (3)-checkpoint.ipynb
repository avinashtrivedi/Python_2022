{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Trade&Ahead\n",
    "\n",
    "**Marks: 60**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "The stock market has consistently proven to be a good place to invest in and save for the future. There are a lot of compelling reasons to invest in stocks. It can help in fighting inflation, create wealth, and also provides some tax benefits. Good steady returns on investments over a long period of time can also grow a lot more than seems possible. Also, thanks to the power of compound interest, the earlier one starts investing, the larger the corpus one can have for retirement. Overall, investing in stocks can help meet life's financial aspirations.\n",
    "\n",
    "It is important to maintain a diversified portfolio when investing in stocks in order to maximise earnings under any market condition. Having a diversified portfolio tends to yield higher returns and face lower risk by tempering potential losses when the market is down. It is often easy to get lost in a sea of financial metrics to analyze while determining the worth of a stock, and doing the same for a multitude of stocks to identify the right picks for an individual can be a tedious task. By doing a cluster analysis, one can identify stocks that exhibit similar characteristics and ones which exhibit minimum correlation. This will help investors better analyze stocks across different market segments and help protect against risks that could make the portfolio vulnerable to losses.\n",
    "\n",
    "\n",
    "## Objective\n",
    "\n",
    "Trade&Ahead is a financial consultancy firm who provide their customers with personalized investment strategies. They have hired you as a Data Scientist and provided you with data comprising stock price and some financial indicators for a few companies listed under the New York Stock Exchange. They have assigned you the tasks of analyzing the data, grouping the stocks based on the attributes provided, and sharing insights about the characteristics of each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "- Ticker Symbol: An abbreviation used to uniquely identify publicly traded shares of a particular stock on a particular stock market\n",
    "- Company: Name of the company\n",
    "- GICS Sector: The specific economic sector assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations\n",
    "- GICS Sub Industry: The specific sub-industry group assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations\n",
    "- Current Price: Current stock price in dollars\n",
    "- Price Change: Percentage change in the stock price in 13 weeks\n",
    "- Volatility: Standard deviation of the stock price over the past 13 weeks\n",
    "- ROE: A measure of financial performance calculated by dividing net income by shareholders' equity (shareholders' equity is equal to a company's assets minus its debt)\n",
    "- Cash Ratio: The ratio of a  company's total reserves of cash and cash equivalents to its total current liabilities\n",
    "- Net Cash Flow: The difference between a company's cash inflows and outflows (in dollars)\n",
    "- Net Income: Revenues minus expenses, interest, and taxes (in dollars)\n",
    "- Earnings Per Share: Company's net profit divided by the number of common shares it has outstanding (in dollars)\n",
    "- Estimated Shares Outstanding: Company's stock currently held by all its shareholders\n",
    "- P/E Ratio: Ratio of the company's current stock price to the earnings per share \n",
    "- P/B Ratio: Ratio of the company's stock price per share by its book value per share (book value of a company is the net difference between that company's total assets and total liabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM29eMvG9MQ6"
   },
   "source": [
    "## Let's start coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to help with reading and manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='darkgrid')\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "# to scale the data using z-score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to compute distances\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "# to perform k-means clustering and compute silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# to visualize the elbow curve and silhouette scores\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete the code to import the data\n",
    "data = pd.read_csv('stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking shape of the data\n",
    "print(f\"There are {'______'} rows and {'______'} columns.\") ## Complete the code to get the shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's view a sample of the data\n",
    "data.sample(n=10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the column names and datatypes\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the data to another variable to avoid any changes to original data\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WapzauPrhmV7",
    "outputId": "011488d0-725d-420a-eee1-e6bed30758d8"
   },
   "outputs": [],
   "source": [
    "# checking for duplicate values\n",
    "df.'_______' ## Complete the code to get total number of duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values in the data\n",
    "df.'_______' ## Complete the code to check the missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhPuzWO7hmV8"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW4-5-GUhmV8"
   },
   "source": [
    "**Let's check the statistical summary of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "\n",
    "\n",
    "def histogram_boxplot(df, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to the show density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=df, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=df, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=df, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        df[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        df[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Current Price`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df, 'Current Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Price Change`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'Price Change'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Volatility`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'Volatility'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ROE`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'ROE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Cash Ratio`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'Cash Ratio'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Net Cash Flow`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'Net Cash Flow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Net Income`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'Net Income'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Earnings Per Share`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'Earnings Per Share'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Estimated Shares Outstanding`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'Estimated Shares Outstanding'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`P/E Ratio`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'P/E Ratio'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`P/B Ratio`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'P/B Ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(df, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(df[feature])  # length of the column\n",
    "    count = df[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=df,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=df[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`GICS Sector`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(df, 'GICS Sector', perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`GICS Sub Industry`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot('_______')  ## Complete the code to create a labelled barplot for 'GICS Sub Industry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga_huJrnhmWE"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpJxawjAhmWE",
    "outputId": "78740200-cf4f-430e-d006-baf16bfb7a78"
   },
   "outputs": [],
   "source": [
    "# correlation check\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(\n",
    "    df.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check the stocks of which economic sector have seen the maximum price increase on average.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(data=df, x='___', y='___', ci=False)  ## Complete the code to choose the right variables\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cash ratio provides a measure of a company's ability to cover its short-term obligations using only cash and cash equivalents. Let's see how the average cash ratio varies across economic sectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(data=df, x='___', y='___', ci=False)  ## Complete the code to choose the right variables\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P/E ratios can help determine the relative value of a company's shares as they signify the amount of money an investor is willing to invest in a single share of a company per dollar of its earnings. Let's see how the P/E ratio varies, on average, across economic sectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(data=df, x='___', y='___', ci=False)  ## Complete the code to choose the right variables\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Volatility accounts for the fluctuation in the stock price. A stock with high volatility will witness sharper price changes, making it a riskier investment. Let's see how volatility varies, on average, across economic sectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(data=df, x='___', y='___', ci=False)  ## Complete the code to choose the right variables\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Check\n",
    "\n",
    "- Let's plot the boxplots of all numerical columns to check for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "for i, variable in enumerate(numeric_columns):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.boxplot(df[variable], whis=1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "- Let's scale the data before we proceed with clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data before clustering\n",
    "scaler = StandardScaler()\n",
    "subset = \"___\"  ## Complete the code to scale the data\n",
    "subset_scaled = scaler.fit_transform(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe of the scaled data\n",
    "subset_scaled_df = pd.DataFrame(subset_scaled, columns=subset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_df = subset_scaled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = range(1, 15)\n",
    "meanDistortions = []\n",
    "\n",
    "for k in clusters:\n",
    "    model = KMeans(n_clusters=k, random_state=1)\n",
    "    model.fit(subset_scaled_df)\n",
    "    prediction = model.predict(k_means_df)\n",
    "    distortion = (\n",
    "        sum(np.min(cdist(k_means_df, model.cluster_centers_, \"euclidean\"), axis=1))\n",
    "        / k_means_df.shape[0]\n",
    "    )\n",
    "\n",
    "    meanDistortions.append(distortion)\n",
    "\n",
    "    print(\"Number of Clusters:\", k, \"\\tAverage Distortion:\", distortion)\n",
    "\n",
    "plt.plot(clusters, meanDistortions, \"bx-\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Average Distortion\")\n",
    "plt.title(\"Selecting k with the Elbow Method\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(random_state=1)\n",
    "visualizer = KElbowVisualizer(model, k=(1, 15), timings=True)\n",
    "visualizer.fit(k_means_df)  # fit the data to the visualizer\n",
    "visualizer.show()  # finalize and render figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check the silhouette scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score = []\n",
    "cluster_list = range(2, 15)\n",
    "for n_clusters in cluster_list:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    preds = clusterer.fit_predict((subset_scaled_df))\n",
    "    score = silhouette_score(k_means_df, preds)\n",
    "    sil_score.append(score)\n",
    "    print(\"For n_clusters = {}, the silhouette score is {})\".format(n_clusters, score))\n",
    "\n",
    "plt.plot(cluster_list, sil_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(random_state=1)\n",
    "visualizer = KElbowVisualizer(model, k=(2, 15), metric=\"silhouette\", timings=True)\n",
    "visualizer.fit(k_means_df)  # fit the data to the visualizer\n",
    "visualizer.show()  # finalize and render figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans('___', random_state=1))  ## Complete the code to visualize the silhouette scores for certain number of clusters\n",
    "visualizer.fit(k_means_df)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final K-means model\n",
    "kmeans = KMeans(n_clusters='___', random_state=1)  ## Complete the code to choose the number of clusters\n",
    "kmeans.fit(k_means_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the original data\n",
    "df1 = df.copy()\n",
    "\n",
    "# adding kmeans cluster labels to the original and scaled dataframes\n",
    "k_means_df[\"KM_segments\"] = kmeans.labels_\n",
    "df1[\"KM_segments\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_cluster_profile = df1.groupby(\"___\").mean()  ## Complete the code to groupby the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_cluster_profile[\"count_in_each_segment\"] = (\n",
    "    df1.groupby(\"___\")[\"Security\"].count().values  ## Complete the code to groupby the cluster labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Complete the code to print the companies in each cluster\n",
    "for cl in df1[\"___\"].unique():\n",
    "    print(\"In cluster {}, the following companies are present:\".format(cl))\n",
    "    print(df1[df1[\"___\"] == cl][\"Security\"].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby([\"KM_segments\", \"GICS Sector\"])['Security'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(20, 20))\n",
    "counter = 0\n",
    "\n",
    "for ii in range(3):\n",
    "    for jj in range(4):\n",
    "        if counter < 11:\n",
    "            sns.boxplot(\n",
    "                ax=axes[ii][jj],\n",
    "                data=df1,\n",
    "                y=df1.columns[4+counter],\n",
    "                x=\"KM_segments\",\n",
    "            )\n",
    "            counter = counter + 1\n",
    "\n",
    "fig.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df = subset_scaled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of distance metrics\n",
    "distance_metrics = ['___'] ## Complete the code to add distance metrics\n",
    "\n",
    "# list of linkage methods\n",
    "linkage_methods = ['___'] ## Complete the code to add linkages\n",
    "\n",
    "high_cophenet_corr = 0\n",
    "high_dm_lm = [0, 0]\n",
    "\n",
    "for dm in distance_metrics:\n",
    "    for lm in linkage_methods:\n",
    "        Z = linkage(hc_df, metric=dm, method=lm)\n",
    "        c, coph_dists = cophenet(Z, pdist(hc_df))\n",
    "        print(\n",
    "            \"Cophenetic correlation for {} distance and {} linkage is {}.\".format(\n",
    "                dm.capitalize(), lm, c\n",
    "            )\n",
    "        )\n",
    "        if high_cophenet_corr < c:\n",
    "            high_cophenet_corr = c\n",
    "            high_dm_lm[0] = dm\n",
    "            high_dm_lm[1] = lm\n",
    "            \n",
    "# printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
    "print('*'*100)\n",
    "print(\n",
    "    \"Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.\".format(\n",
    "        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's explore different linkage methods with Euclidean distance only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of linkage methods\n",
    "linkage_methods = ['___'] ## Complete the code to add linkages\n",
    "\n",
    "high_cophenet_corr = 0\n",
    "high_dm_lm = [0, 0]\n",
    "\n",
    "for lm in linkage_methods:\n",
    "    Z = linkage(hc_df, metric=\"euclidean\", method=lm)\n",
    "    c, coph_dists = cophenet(Z, pdist(hc_df))\n",
    "    print(\"Cophenetic correlation for {} linkage is {}.\".format(lm, c))\n",
    "    if high_cophenet_corr < c:\n",
    "        high_cophenet_corr = c\n",
    "        high_dm_lm[0] = \"euclidean\"\n",
    "        high_dm_lm[1] = lm\n",
    "        \n",
    "# printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
    "print('*'*100)\n",
    "print(\n",
    "    \"Highest cophenetic correlation is {}, which is obtained with {} linkage.\".format(\n",
    "        high_cophenet_corr, high_dm_lm[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's view the dendrograms for the different linkage methods with Euclidean distance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of linkage methods\n",
    "linkage_methods = ['___'] ## Complete the code to add linkages\n",
    "\n",
    "# lists to save results of cophenetic correlation calculation\n",
    "compare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n",
    "compare = []\n",
    "\n",
    "# to create a subplot image\n",
    "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))\n",
    "\n",
    "# We will enumerate through the list of linkage methods above\n",
    "# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    Z = linkage(hc_df, metric=\"euclidean\", method=method)\n",
    "\n",
    "    dendrogram(Z, ax=axs[i])\n",
    "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n",
    "\n",
    "    coph_corr, coph_dist = cophenet(Z, pdist(hc_df))\n",
    "    axs[i].annotate(\n",
    "        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n",
    "        (0.80, 0.80),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )\n",
    "\n",
    "    compare.append([method, coph_corr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and print a dataframe to compare cophenetic correlations for different linkage methods\n",
    "df_cc = pd.DataFrame(compare, columns=compare_cols)\n",
    "df_cc = df_cc.sort_values(by=\"Cophenetic Coefficient\")\n",
    "df_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCmodel = AgglomerativeClustering(n_clusters='___', affinity='___', linkage='___')  ## Complete the code to define the hierarchical clustering model\n",
    "HCmodel.fit(hc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the original data\n",
    "df2 = df.copy()\n",
    "\n",
    "# adding hierarchical cluster labels to the original and scaled dataframes\n",
    "hc_df[\"HC_segments\"] = HCmodel.labels_\n",
    "df2[\"HC_segments\"] = HCmodel.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_cluster_profile = df2.groupby(\"___\").mean()  ## Complete the code to groupby the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_cluster_profile[\"count_in_each_segment\"] = (\n",
    "    df2.groupby(\"___\")[\"Security\"].count().values  ## Complete the code to groupby the cluster labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Complete the code to print the companies in each cluster\n",
    "for cl in df2[\"____\"].unique():\n",
    "    print(\"In cluster {}, the following companies are present:\".format(cl))\n",
    "    print(df2[df2[\"___\"] == cl][\"Security\"].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby([\"HC_segments\", \"GICS Sector\"])['Security'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(20, 20))\n",
    "counter = 0\n",
    "\n",
    "for ii in range(3):\n",
    "    for jj in range(4):\n",
    "        if counter < 11:\n",
    "            sns.boxplot(\n",
    "                ax=axes[ii][jj],\n",
    "                data=df2,\n",
    "                y=df2.columns[4+counter],\n",
    "                x=\"HC_segments\",\n",
    "            )\n",
    "            counter = counter + 1\n",
    "\n",
    "fig.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means vs Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You compare several things, like:\n",
    "- Which clustering technique took less time for execution?\n",
    "- Which clustering technique gave you more distinct clusters, or are they the same?\n",
    "- How many observations are there in the similar clusters of both algorithms?\n",
    "- How many clusters are obtained as the appropriate number of clusters from both algorithms?\n",
    "\n",
    "You can also mention any differences or similarities you obtained in the cluster profiles from both the clustering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxrEHj3ihmWO"
   },
   "source": [
    "## Actionable Insights and Recommendations\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
