{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <Center> Homework 5: Classification </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment needs the following two data files:\n",
    "- train.csv: dataset for training\n",
    "- test.csv: dataset for testing. \n",
    "    \n",
    "Both of them have samples in the following format. The `text` column contains documents and the `label` column gives the sentiment of each document.\n",
    "\n",
    "|label | text |\n",
    "|------|------|\n",
    "|1|  I must admit that I'm addicted to \"Version 2.0...|\n",
    "|0|  I think it's such a shame that an enormous tal...|\n",
    "|1|  The Sunsout No Room at The Inn Puzzle has oddl...|\n",
    "|...|...|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Classification\n",
    "\n",
    "- Define a function `create_model(train_docs, train_y, test_docs, test_y, model_type='svm', stop_words='english', min_df = 1, print_result = True, algorithm_para=1.0`), where\n",
    "\n",
    "    - `train_docs`: is a list of documents for training\n",
    "    - `train_y`: is the ground-truth labels of training documents\n",
    "    - `test_docs`: is a list of documents for test\n",
    "    - `test_y`: is the ground-truth labels of test documents\n",
    "    - `model_type`: two options: `nb` (Multinomial Naive Bayes) or `svm` (Linear SVM)\n",
    "    - `stop_words`: indicate whether stop words should be removed. The default value is 'english', i.e. remove English stopwords.\n",
    "    - `min_df`: only word with document frequency above this threshold can be included. The default is 1. \n",
    "    - `print_result`: controls whether to show classification report or plots. The default is True.\n",
    "    - `algorithm_para`: the hyperparameter of algorithms, here refers to C in SVM and alpha in NB. Both C and alpha have default values of 1.0. Please check sklearn documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "\n",
    "- This function does the following:\n",
    "    - Fit a `TfidfVectorizer` using `train_docs` with options `stop_words, min_df` as specified in the function inputs. Extract features from `train_docs` using the fitted `TfidfVectorizer`.\n",
    "    - Build `linear SVM` or `Multinomial Naive Bayes` model as specified by `model_type` with `algorithm_para` setting. Train the model using the extracted features and `train_y`. \n",
    "    - Transform `test_docs` by the fitted `TfidfVectorizer` (hint: use function `transform` not `fit_transform`).\n",
    "    - Predict the labels for `test_docs`. If `print_result` is True, print the classification report.\n",
    "    - Calculate the AUC score and PRC score (or Average Precision) for class 1 on the test dataset. If `print_result` is True, plot the ROC and PRC curves. **Hint**: \n",
    "        - `sklearn.svm.LinearSVM` does not provide `predict_proba` function. \n",
    "        - Instead, you can use its `decision_function` (see <a href = \"https://stackoverflow.com/questions/59227176/how-to-plot-roc-and-calculate-auc-for-binary-classifier-with-no-probabilities-s\">some referenc code</a>) \n",
    "        - Another option is to use `sklearn.svm.SVC` with `kernel='linear' and probability=False` (see <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\"> reference</a>)\n",
    "    - Return the AUC and PRC scores.\n",
    "\n",
    "    \n",
    "- Test your function with following cases:\n",
    "    - model_type='svm', stop_words = 'english', min_df = 1\n",
    "    - model_type='nb', stop_words = 'english', min_df = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avitr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "import string\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc,precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train = pd.read_csv(\"train5.csv\")\n",
    "test = pd.read_csv(\"test5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for Task 1\n",
    "\n",
    "def create_model(train_docs, train_y, test_docs, test_y, \\\n",
    "              model_type='svm', stop_words=None, min_df = 1, print_result = True, algorithm_para=1.0):\n",
    "    \n",
    "    \n",
    "    f = lambda x: ''.join([i for i in x if i not in string.punctuation]).lower()\n",
    "\n",
    "    train_docs = train_docs.apply(f)\n",
    "    test_docs = test_docs.apply(f)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words,min_df=min_df)\n",
    "\n",
    "    vectorizer.fit(train_docs)\n",
    "    train_docs = vectorizer.transform(train_docs)\n",
    "    train_docs = train_docs.toarray()\n",
    "\n",
    "    # model building\n",
    "\n",
    "    if model_type=='svm':\n",
    "        clf = svm.SVC(kernel='linear', probability=False,C=algorithm_para)\n",
    "    else:\n",
    "        clf = MultinomialNB(alpha=algorithm_para)\n",
    "\n",
    "    clf.fit(train_docs, train_y)\n",
    "    test_docs = vectorizer.transform(test_docs)\n",
    "    test_docs = test_docs.toarray()\n",
    "    y_pred = clf.predict(test_docs)\n",
    "    \n",
    "    if print_result:\n",
    "        print(classification_report(test_y,y_pred))\n",
    "        \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    if model_type=='svm':\n",
    "        y_score = clf.decision_function(test_docs)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(test_docs)\n",
    "        \n",
    "    if model_type=='svm':\n",
    "        for i in range(2):\n",
    "            fpr[i], tpr[i], _ = roc_curve(test_y, y_score)\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        lr_precision, lr_recall, _ = precision_recall_curve(test_y, y_score)\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            fpr[i], tpr[i], _ = roc_curve(test_y, y_score[:,1])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        lr_precision, lr_recall, _ = precision_recall_curve(test_y, y_score[:,1])\n",
    "\n",
    "    \n",
    "\n",
    "    auc_score = roc_auc[1]\n",
    "    prc_score = lr_precision.mean()\n",
    "            \n",
    "    print('AUC: {:.2f}% PRC: {:.2f}%'.format(auc_score*100,prc_score*100))\n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr[1],\n",
    "        tpr[1],\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=\"ROC curve (area = %0.2f)\" % roc_auc[1],\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_type} AUC')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{model_type} PRC')\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_score, prc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with 'svm'\n",
    "\n",
    "auc_score, prc_socre = create_model(train[\"text\"], train[\"label\"], test[\"text\"], test[\"label\"], \\\n",
    "          model_type='svm', stop_words = 'english', min_df = 1, print_result=True, algorithm_para=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with 'nb' \n",
    "\n",
    "auc_score, prc_score = create_model(train[\"text\"], train[\"label\"], test[\"text\"], test[\"label\"], \\\n",
    "                    model_type='nb', stop_words = 'english', min_df = 1, print_result=True, algorithm_para=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Search for best parameters \n",
    "\n",
    "From Task 1, you may find there are many possible ways to configure parameters. Next, let's use grid search to find the optimal parameters\n",
    "\n",
    "- Define a function `search_para(docs, y)` where `docs` are training documents and `y` is the ground-truth labels.\n",
    "- This function does the following:\n",
    "    - Create a pipleline which integrates `TfidfVectorizer` and `SVM` classifier \n",
    "    - Define the parameter ranges as follow:\n",
    "        - `stop_words': [None, 'english']`\n",
    "        - `min_df: [1,2,5]`\n",
    "        - `C: [0.2,0.5,0.8]`\n",
    "    - Set the scoring metric to \"f1_macro\"\n",
    "    - Use `GridSearchCV` with `5-fold cross validation` to find the best parameter values based on the training dataset. \n",
    "    - Print the best parameter values\n",
    "    \n",
    "- Call the function `svm_model` defined in Task 1 `with the best parameter values`. You will have to add the best C value to the original function, because you used default settings of the SVM classifier previously. Then please briefly answer the following:\n",
    "    - Compare with the model in Task 1, how is the performance improved on the test dataset?\n",
    "    - Why do you think the new parameter values help sentiment classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grade search function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def search_para(docs, y):\n",
    "    \n",
    "    objs = [   \n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", svm.SVC())\n",
    "    ]\n",
    "    \n",
    "    pipe = Pipeline(objs)\n",
    "\n",
    "    param_grid = { \n",
    "    'tfidf__stop_words': (None,'english'),# None\n",
    "    'tfidf__min_df': (1,2,5),\n",
    "    'clf__C': (0.2,0.5,0.8)\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipe,param_grid,cv= 5, n_jobs=-1, verbose=3)\n",
    "    grid_search.fit(docs, y)\n",
    "    best_parameters = grid_search.best_estimator_\n",
    "    print('best_parameters:',best_parameters)\n",
    "    # Add your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "best_parameters: Pipeline(steps=[('tfidf', TfidfVectorizer(min_df=5, stop_words='english')),\n",
      "                ('clf', SVC(C=0.8))])\n"
     ]
    }
   ],
   "source": [
    "search_para(train[\"text\"][:100], train[\"label\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function with the best parameters\n",
    "\n",
    "auc_score, prc_score = create_model(train[\"text\"], train[\"label\"], test[\"text\"], test[\"label\"], \\\n",
    "          model_type='svm', stop_words = None, min_df = 2, print_result=True, algorithm_para=0.2)\n",
    "\n",
    "# Please answer the questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. How many samples are enough? \n",
    "\n",
    "This task is to help you understand the impact of sample size on classifier performance. \n",
    "\n",
    "- Define a function `sample_size_impact(train_docs, train_y, test_docs, test_y)` with the input as defined above. \n",
    "    \n",
    "    \n",
    "- This function does the following:\n",
    "    - Starting with 500 samples, in each round you build a classifier with 500 more samples. i.e. in round 1, you use samples from 0:500, and in round 2, you use samples from 0:1000, …, until you use all samples. \n",
    "    - In each round, call the `create_model` function with `model_type='svm'` and then `model_type='nb'`. Keep other parameters as default. Record the AUC score of each model type and the sample size.\n",
    "    - For each model type, plot a line chart show the relationship between sample size and the AUC score. \n",
    "    - This function has no return.\n",
    "    \n",
    "    \n",
    "- Write your analysis on the following:\n",
    "    - How sample size affects each classifier’s performance? \n",
    "    - If it is expensive to collect and label samples, can you decide an optimal sample size with model performance and the cost of samples both considered? \n",
    "    - How is performance of SVM classifier compared with Naïve Bayes classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the impact of sample size\n",
    "\n",
    "def sample_size_impact(train_docs, train_y, test_docs, test_y):  \n",
    "    x = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    for i in (500,len(train_docs),500):\n",
    "        auc_score, prc_score = create_model(train_docs[:i], train_y[:i], test_docs[:i], test_y[:i],\n",
    "                                      model_type='svm', stop_words = None, \n",
    "                                      min_df = 2, print_result=True,algorithm_para=0.2)\n",
    "        \n",
    "        y1.append(auc_score)\n",
    "        auc_score, prc_score = create_model(train_docs[:i], train_y[:i], test_docs[:i], test_y[:i],\n",
    "                                      model_type='nb', stop_words = None, \n",
    "                                      min_df = 2, print_result=True,algorithm_para=0.2)\n",
    "        \n",
    "        y2.append(auc_score)\n",
    "        x.append(i)\n",
    "        \n",
    "    plt.plot(x,y1,label='svm')\n",
    "    plt.plot(x,y1,label='nb')\n",
    "    plt.xlabel('sample size')\n",
    "    plt.ylabel('AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "\n",
    "sample_size_impact(train[\"text\"], train[\"label\"], test[\"text\"], test[\"label\"])\n",
    "\n",
    "\n",
    "# Briefly answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (Bonus): How to improve sentiment classification?\n",
    "\n",
    "- Can you tune other parameters of TfidfVectorizer or SVM model to further improve the classification performance? Try to beat the model you get in Task 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
