{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T11:09:43.814890Z",
     "iopub.status.busy": "2022-11-30T11:09:43.814176Z",
     "iopub.status.idle": "2022-11-30T11:09:48.329512Z",
     "shell.execute_reply": "2022-11-30T11:09:48.328458Z",
     "shell.execute_reply.started": "2022-11-30T11:09:43.814831Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import spacy\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T11:09:48.332245Z",
     "iopub.status.busy": "2022-11-30T11:09:48.331810Z",
     "iopub.status.idle": "2022-11-30T11:09:48.381409Z",
     "shell.execute_reply": "2022-11-30T11:09:48.380469Z",
     "shell.execute_reply.started": "2022-11-30T11:09:48.332217Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hw2_train.csv', index_col=0)\n",
    "df.columns = [\"text\", \"labels\"]\n",
    "df_test=pd.read_csv(\"hw2_test.csv\",index_col = 0)\n",
    "df_test.columns = [\"text\"]\n",
    "disctionary_list = []\n",
    "for i, row in df.iterrows():\n",
    "    temp = {}\n",
    "    text_length = len(row['text'].split())\n",
    "    labels_length = len(row['labels'].split())\n",
    "    if(text_length == labels_length):\n",
    "        temp['text'] = row['text']\n",
    "        temp['labels'] = row['labels']\n",
    "        disctionary_list.append(temp)\n",
    "df= pd.DataFrame.from_dict(disctionary_list)\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T11:09:48.383527Z",
     "iopub.status.busy": "2022-11-30T11:09:48.383220Z",
     "iopub.status.idle": "2022-11-30T11:09:48.394787Z",
     "shell.execute_reply": "2022-11-30T11:09:48.393618Z",
     "shell.execute_reply.started": "2022-11-30T11:09:48.383493Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, random_state = 42, test_size = 0.2,shuffle = True)\n",
    "print(f' train data shape {df_train.shape}')\n",
    "print(f' validation data shape {df_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T11:09:48.397128Z",
     "iopub.status.busy": "2022-11-30T11:09:48.396126Z",
     "iopub.status.idle": "2022-11-30T11:14:04.940033Z",
     "shell.execute_reply": "2022-11-30T11:14:04.938492Z",
     "shell.execute_reply.started": "2022-11-30T11:09:48.397086Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T11:14:04.952839Z",
     "iopub.status.busy": "2022-11-30T11:14:04.949781Z",
     "iopub.status.idle": "2022-11-30T11:14:05.111171Z",
     "shell.execute_reply": "2022-11-30T11:14:05.109651Z",
     "shell.execute_reply.started": "2022-11-30T11:14:04.952789Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = list(embeddings_index.keys())\n",
    "embeddings = list(embeddings_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T11:14:05.113730Z",
     "iopub.status.busy": "2022-11-30T11:14:05.112937Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = np.array(vocab)\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.insert(vocab, 0, '<pad>')\n",
    "vocab = np.insert(vocab, 1, '<unk>')\n",
    "def add_unk_pad(embeddings):\n",
    "    pad = np.zeros((1,embeddings.shape[1]))\n",
    "    unk = np.mean(embeddings,axis = 0,keepdims = True)\n",
    "    embeddings = np.vstack((pad,unk,embeddings))\n",
    "    return embeddings\n",
    "\n",
    "add_unk_pad(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Embeddings Shape {embeddings.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word to Index and index to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: idx for idx, w in enumerate(vocab)}\n",
    "word2idx['<pad>'] = 0\n",
    "word2idx['<unk>'] = 1\n",
    "idx2word = {idx: w for idx, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag dictionary and assigning indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "        [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n",
    "\n",
    "labs_count = len(labels_to_ids)\n",
    "labs_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggingDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        self.text = self.data['text']\n",
    "        if 'labels' in self.data.columns:\n",
    "            self.labels = self.data['labels']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, \n",
    "                    idx: int):\n",
    "        text = self.text.iloc[idx]\n",
    "        encoded_text = self.text_encoder(text)\n",
    "        \n",
    "        if 'labels' in self.data.columns:\n",
    "            label = self.labels.iloc[idx]\n",
    "            encoded_label = self.labels_encoder(label)\n",
    "            return encoded_text, encoded_label \n",
    "        else:\n",
    "            return encoded_text\n",
    "\n",
    "    def get_tokens(self, \n",
    "                 text: str):\n",
    "        return [i.text for i in tokenizer(text)]\n",
    "    \n",
    "\n",
    "    def text_encoder(self, \n",
    "                    text):\n",
    "        list_of_texts = [word for word in self.get_tokens(text)]\n",
    "        vector = []\n",
    "        for word in list_of_texts:\n",
    "            if word in word2idx:\n",
    "                vector.append(word2idx[word])\n",
    "            else:\n",
    "                vector.append(1)\n",
    "        return vector\n",
    "    \n",
    "    def labels_encoder(self, \n",
    "                   label):\n",
    "        list_of_labels = [word for word in self.get_tokens(label)]\n",
    "        vector = [labels_to_ids[word] for word in list_of_labels]\n",
    "        return vector\n",
    "\n",
    "    \n",
    "training = TaggingDataset(df_train)\n",
    "validation = TaggingDataset(df_val)\n",
    "testing = TaggingDataset(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pad_sequence from torch to create a collate function\n",
    "def my_collate_fn(batch):\n",
    "    if isinstance(batch[0], list):\n",
    "        tensor_text = [torch.tensor(text) for text in batch]\n",
    "\n",
    "        lengths = [len(text) for text in batch]\n",
    "        lengths = torch.tensor(lengths)\n",
    "\n",
    "        pad_texts = pad_sequence(tensor_text, batch_first = True, padding_value = 0) #using pad_sequence from torch\n",
    "\n",
    "        return pad_texts, lengths\n",
    "\n",
    "    else:\n",
    "        texts, labels = zip(*batch)\n",
    "\n",
    "        tensor_text = [torch.tensor(text) for text in texts]\n",
    "        labels_tensor = [torch.tensor(label) for label in labels]\n",
    "\n",
    "        lengths = [len(text) for text in texts]\n",
    "        lengths = torch.tensor(lengths)\n",
    "\n",
    "        pad_texts = pad_sequence(tensor_text, batch_first = True, padding_value = 0)\n",
    "        labels_padded = pad_sequence(labels_tensor, batch_first = True, padding_value = 0)\n",
    "\n",
    "        return pad_texts, labels_padded, lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(training,batch_size = 50,                          shuffle = True, \n",
    "                          collate_fn = my_collate_fn)\n",
    "validation_loader = DataLoader(validation, batch_size = 50, \n",
    "                        shuffle = True, collate_fn = my_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(testing,batch_size = 1, \n",
    "                        shuffle = False, \n",
    "                        collate_fn = my_collate_fn)\n",
    "\n",
    "assert df_train.shape[0] == len(train_loader.dataset)\n",
    "assert df_val.shape[0] == len(validation_loader.dataset)\n",
    "assert df_test.shape[0] == len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float())\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = True)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_directions, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                x, \n",
    "                x_lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, \n",
    "                                                            x_lengths, \n",
    "                                                            batch_first = True, \n",
    "                                                            enforce_sorted = False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, \n",
    "                                                                  batch_first = True)\n",
    "        output = self.fc1(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(25)\n",
    "\n",
    "EMBEDDING_DIM = embeddings.shape[1]\n",
    "HIDDEN_DIM = 20\n",
    "OUTPUT_DIM = labs_count\n",
    "NUM_LAYERS = 3\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = LSTM(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            NUM_LAYERS, \n",
    "            BIDIRECTION, \n",
    "            DROPOUT).to(device)\n",
    "\n",
    "print('LSTM Model: ', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_squeez(my_list):\n",
    "    temp_list = [int(element) for element in my_list]\n",
    "    return temp_list  \n",
    "\n",
    "def slicing(my_list, slice_increment):\n",
    "    return [my_list[i : i + slice_increment] for i in range(0, len(my_list), slice_increment)]\n",
    "\n",
    "def idx_to_tags_conversion(lol, isTensor):\n",
    "    iob_list = []\n",
    "    for list_element in lol:\n",
    "        if (isTensor):\n",
    "            list_element = list_element.numpy() \n",
    "        iob = [ids_to_labels[index] for index in list_element]\n",
    "        iob_list.append(iob)\n",
    "    return iob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(loader, \n",
    "          model, \n",
    "          optimizer, \n",
    "          loss_fn):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y, lengths in pbar:\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "        y_pred = model(x, lengths)\n",
    "        print(y.shape, y_pred.shape)\n",
    "        \n",
    "\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1])\n",
    "        y = torch.flatten(y)\n",
    "        \n",
    "        print(y.shape, y_pred.shape)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()  \n",
    "        optimizer.step()          \n",
    "    return round((sum(losses) / len(losses)), 4)\n",
    "\n",
    "\n",
    "def evaluate(loader, \n",
    "             model, \n",
    "             loss_fn, \n",
    "             score_fn):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y, lengths in tqdm(loader):\n",
    "        y_pred = model(x, lengths)\n",
    "        \n",
    "        max_len = x.shape[1]\n",
    "              \n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1])\n",
    "        y = torch.flatten(y)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        tags_iob = idx_to_tags_conversion(slicing(y, max_len), True)\n",
    "\n",
    "        max_preds = y_pred.argmax(dim = 1, keepdim = True) \n",
    "        predictions_iob = idx_to_tags_conversion(slicing((list_squeez(max_preds)), max_len), False)\n",
    "    \n",
    "    score = score_fn(tags_iob, predictions_iob, scheme = IOB2)\n",
    "    return tags_iob, predictions_iob, round((sum(losses) / len(losses)), 4), round(score, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr = 0.01)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "score_fn = f1_score\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "f1_score_list = []\n",
    "n_epochs = 1\n",
    "best_acc = 0\n",
    "PATH = f'best-model.pt'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader, \n",
    "                     model, \n",
    "                     optimizer, \n",
    "                     loss_fn)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print('Train Loss: ', train_loss)\n",
    "    \n",
    "    tags, predictions, val_loss, accuracy = evaluate(validation_loader, \n",
    "                                                       model, \n",
    "                                                       loss_fn, \n",
    "                                                       score_fn)\n",
    "    val_loss_list.append(val_loss)\n",
    "    f1_score_list.append(accuracy)\n",
    "    print('Val Accuracy: ', accuracy)\n",
    "    print('Val Loss: ', val_loss)\n",
    "    \n",
    "\n",
    "    if accuracy > best_acc and accuracy > 0.70:\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LSTM(EMBEDDING_DIM, \n",
    "                   HIDDEN_DIM, \n",
    "                   OUTPUT_DIM, \n",
    "                   NUM_LAYERS, \n",
    "                   BIDIRECTION, \n",
    "                   DROPOUT).to(device)\n",
    "\n",
    "best_model.load_state_dict(torch.load(PATH))\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Predict Function\n",
    "def predict(loader, \n",
    "            model):\n",
    "    predictions = []\n",
    "    for x, lengths in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            y_pred = best_model.forward(x, lengths)\n",
    "            \n",
    "            max_len = x.shape[1]\n",
    "\n",
    "            # Convert y_pred to 2D Tensor\n",
    "            y_pred = y_pred.view(-1, y_pred.shape[-1])\n",
    "\n",
    "            max_preds = y_pred.argmax(dim = 1, keepdim = True) \n",
    "            predictions_iob = idx_to_tags_conversion(slicing((list_squeez(max_preds)), max_len), False)\n",
    "            predictions.append(predictions_iob)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predicted_tags = predict(test_loader, best_model)\n",
    "predicted_tags = np.array(predicted_tags)\n",
    "predicted_tags = predicted_tags.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce prediction and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_dict = []\n",
    "for i in range(len(predicted_tags)):\n",
    "    temp_dict = {}\n",
    "    temp_dict[\"ID\"] = i\n",
    "    temp_dict[\"IOB Slot tags\"] = ' '.join(predicted_tags[i])\n",
    "    list_of_dict.append(temp_dict)\n",
    "\n",
    "tags_df = pd.DataFrame.from_dict(list_of_dict)\n",
    "tags_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
