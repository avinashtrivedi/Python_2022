{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb446410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re  \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7970c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "test = pd.read_csv(r'../task1/pkv-results.csv')\n",
    "# dataset for documentId extraction\n",
    "ex=pd.read_csv(r'../task1/pkv_comp.csv', delimiter=\";\")\n",
    "test =test.applymap(lambda x: str(x).rstrip('.json'))\n",
    "test['file']=test['file'].astype(str).astype(int)\n",
    "ex=ex[['fileId','documentId']]\n",
    "# join the two datasets\n",
    "output=pd.merge(ex,test, left_on='fileId',right_on='file',how='outer')\n",
    "# if value in 'file' found in 'fileId', copy the value of 'documentId' into new column 'documentId_'\n",
    "output['documentId_']=output.loc[output.fileId==output.file, 'documentId_'] = output.documentId\n",
    "\n",
    "output.loc[(output['content'].isna()), ['fileId', 'documentId','documentId_']] = np.nan\n",
    "output=output.dropna( axis=0,how='all')\n",
    "output=output.drop_duplicates(keep='last').reset_index()\n",
    "output=output[['documentId_','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c6e4d7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['documentId_'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mdrop(res[res\u001b[38;5;241m.\u001b[39mrate_old \u001b[38;5;241m==\u001b[39m res\u001b[38;5;241m.\u001b[39mrate_new]\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     62\u001b[0m res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mdrop(res[res\u001b[38;5;241m.\u001b[39mrate_new \u001b[38;5;241m<\u001b[39m res\u001b[38;5;241m.\u001b[39mrate_old]\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m---> 63\u001b[0m \u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocumentId_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_of_birth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mincreased_at\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrate_old\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrate_new\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtariff_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincreased_at\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['documentId_'] not in index\""
     ]
    }
   ],
   "source": [
    "# test dataframe\n",
    "#output = pd.read_csv(r'../task1/pkv-results.csv')\n",
    "output=output.join(output['content'].apply(json.loads).apply(pd.Series))\n",
    "output=output.drop(labels=['content'],axis=1)\n",
    "output=output.applymap(lambda x: str(x).rstrip('.json'))\n",
    "#output['documentId_']=output['documentId_'].astype(str).astype(int)\n",
    "output=output.replace('na',np.nan)\n",
    "output[['documentId_',\n",
    "     #records for person1 with specific date\n",
    "    'person1','person1_date_of_birth','person1_increased_at',\n",
    "     #associated with 'person1','person1_date_of_birth','person1_increased_at'\n",
    "     'person1_rate_old1','person1_rate_new1','person1_tariff_code1',\n",
    "     'person1_rate_old2','person1_rate_new2','person1_tariff_code2',\n",
    "     'person1_rate_old3','person1_rate_new3','person1_tariff_code3',\n",
    "     'person1_rate_old4','person1_rate_new4','person1_tariff_code4',\n",
    "     'person1_rate_old5','person1_rate_new5','person1_tariff_code5',\n",
    "     'person1_rate_old6','person1_rate_new6','person1_tariff_code6',\n",
    "     'person1_rate_old7','person1_rate_new7','person1_tariff_code7',\n",
    "     'person1_rate_old8','person1_rate_new8','person1_tariff_code8',\n",
    "      #records for person2 with specific date\n",
    "    'person2','person2_date_of_birth','person2_increased_at',\n",
    "     #associated with 'person2','person2_date_of_birth','person2_increased_at'\n",
    "    'person2_rate_old1','person2_rate_new1','person2_tariff_code1',\n",
    "    'person2_rate_old2','person2_rate_new2','person2_tariff_code2',\n",
    "    'person2_rate_old3','person2_rate_new3','person2_tariff_code3',\n",
    "    'person2_rate_old4','person2_rate_new4','person2_tariff_code4',\n",
    "    'person2_rate_old5','person2_rate_new5','person2_tariff_code5',\n",
    "    'person2_rate_old6','person2_rate_new6','person2_tariff_code6',\n",
    "    'person2_rate_old7','person2_rate_new7','person2_tariff_code7',\n",
    "    'person2_rate_old8','person2_rate_new8','person2_tariff_code8',\n",
    "     #records for person3 with specific date\n",
    "    'person3','person3_date_of_birth','person3_increased_at',\n",
    "     #associated with 'person3','person3_date_of_birth','person3_increased_at'\n",
    "    'person3_rate_old1','person3_rate_new1','person3_tariff_code1',\n",
    "    'person3_rate_old2','person3_rate_new2','person3_tariff_code2',\n",
    "    'person3_rate_old3','person3_rate_new3','person3_tariff_code3',\n",
    "    'person3_rate_old4','person3_rate_new4','person3_tariff_code4',\n",
    "    'person3_rate_old5','person3_rate_new5','person3_tariff_code5',\n",
    "    'person3_rate_old6','person3_rate_new6','person3_tariff_code6',\n",
    "    'person3_rate_old7','person3_rate_new7','person3_tariff_code7',\n",
    "    'person3_rate_old8','person3_rate_new8','person3_tariff_code8'\n",
    "     \n",
    "    ]]\n",
    "\n",
    "\n",
    "output=output.rename(columns={\"person1\": \"person1_name\", \"person2\": \"person2_name\", \"person3\":\"person3_name\"})\n",
    "output.columns = output.columns.str.split(r\"(?<=\\w\\d[_])(?=\\w)\", expand = True)\n",
    "\n",
    "# TODO:\n",
    "# in the following step, the documentId_ value for the entries is lost\n",
    "# please make sure it's included for the respective record\n",
    "\n",
    "tmp = (output.unstack().unstack(level=1).reset_index(level=1, drop = True).rename_axis('person').reset_index())\n",
    "tmp=tmp.rename(columns={tmp.columns[1]: 'file'})\n",
    "tmp.loc[(tmp['name'].isna()), ['person', 'file']] = np.nan\n",
    "tmp=tmp.dropna( axis=0,how='all')\n",
    "tmp=tmp.reset_index()\n",
    "res = (pd.wide_to_long(tmp,stubnames=['rate_old','rate_new','tariff_code'],i=['file','name','date_of_birth', 'increased_at','person'],j='v',sep='').dropna().droplevel(-1).reset_index())\n",
    "res['rate_old']=pd.to_numeric(res['rate_old'], errors='coerce').astype('int64')\n",
    "res['rate_new']=pd.to_numeric(res['rate_new'], errors='coerce').astype('int64')\n",
    "res = res.drop(res[res.rate_old == res.rate_new].index)\n",
    "res = res.drop(res[res.rate_new < res.rate_old].index)\n",
    "res[['documentId_','name','date_of_birth','increased_at','rate_old','rate_new','tariff_code']].sort_values(['name','increased_at']).reset_index().drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403d52a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this dataframe is to be merged with above (horizontally)\n",
    "# on validation_name/name, validation_date_of_birth/date_of_birth and validation_increased_at/increased_at\n",
    "validation = pd.read_csv(r'../task1/pkv_comp copy.csv', delimiter=\";\")\n",
    "validation\n",
    "validation=validation.rename(columns={\"person\": \"validation_name\", \n",
    "                                      \"date_of_birth\": \"validation_date_of_birth\", \n",
    "                                      \"increased_at\":\"validation_increased_at\",\n",
    "                                      \"rate_old\": \"validation_rate_old\",\n",
    "                                      \"rate_new\": \"validation_rate_new\",\n",
    "                                      \"tariff_code\": \"validation_tariff_code\",\n",
    "                                      \"documentId\":\"validation_documentId\"\n",
    "                                     })\n",
    "validation[['validation_documentId','validation_name','validation_date_of_birth',\n",
    "            'validation_increased_at','validation_rate_old','validation_rate_new','validation_tariff_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f7f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
