{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining DocBin for training with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def massage_data(address):\n",
    "    # Replacing multiple commas and multiple whitespaces following commas with a single comma and whitespace\n",
    "    cleansed_address1=re.sub(r'(,)(?!\\s)',', ',address)\n",
    "    # Replacing new lines with comma\n",
    "    cleansed_address2=re.sub(r'(\\\\n)',', ',cleansed_address1)\n",
    "    # Replacing multiple spaces before and after a hyphen with a single whitespace before and after\n",
    "    cleansed_address3=re.sub(r'(?!\\s)(-)(?!\\s)',' - ',cleansed_address2)\n",
    "    # Removing period from the string\n",
    "    cleansed_address=re.sub(r'\\.','',cleansed_address3)\n",
    "    return cleansed_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_span(address=None,address_component=None,label=None):\n",
    "    if pd.isna(address_component) or str(address_component)=='nan':\n",
    "        # Don't do anything if the address or address component is not present\n",
    "        pass\n",
    "    else:\n",
    "        # Find the address component in the address and return start and end indices\n",
    "        address_component1=re.sub('\\.','',address_component)\n",
    "        address_component2=re.sub(r'(?!\\s)(-)(?!\\s)',' - ',address_component1)\n",
    "        span=re.search('\\\\b(?:'+address_component2+')\\\\b',address)\n",
    "        return (span.start(),span.end(),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_list(entity_list,entity):\n",
    "    # Method to extend list\n",
    "    if pd.isna(entity):\n",
    "        return entity_list\n",
    "    else:\n",
    "        entity_list.append(entity)\n",
    "        return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_spans(df,tag_list):\n",
    "    # Obtaining entity span and labels of the address components from the given data\n",
    "    # This data is used for training\n",
    "    df['Address']=df['Address'].apply(lambda x: massage_data(x))\n",
    "    df[\"BuildingTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['Building_Name'],label='BUILDING_NAME'),axis=1)\n",
    "    df[\"BuildingNoTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['Building_Number'],label='BUILDING_NO'),axis=1)\n",
    "    df[\"RecipientTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['Recipient'],label='RECIPIENT'),axis=1)\n",
    "    df[\"StreetNameTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['Street_Name'],label='STREET_NAME'),axis=1)\n",
    "    df[\"ZipCodeTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['Zip_Code'],label='ZIP_CODE'),axis=1)\n",
    "    df[\"CityTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['City'],label='CITY'),axis=1)\n",
    "    df[\"CountryTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['Country'],label='COUNTRY'),axis=1)\n",
    "    df[\"StateTag\"]=df.apply(lambda row:get_address_span(address=row['Address'],address_component=row['State'],label='STATE'),axis=1)\n",
    "    df['EmptySpan']=df.apply(lambda x: [], axis=1)\n",
    "\n",
    "    for i in tag_list:\n",
    "        df['EntitySpans']=df.apply(lambda row: extend_list(row['EmptySpan'],row[i]),axis=1)\n",
    "        df['EntitySpans']=df[['EntitySpans','Address']].apply(lambda x: (x[1], x[0]),axis=1)\n",
    "    return df['EntitySpans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_bin(training_data,nlp):\n",
    "    # DocBin is used for data storing in spaCy\n",
    "    # the DocBin will store the example documents\n",
    "    db = DocBin()\n",
    "    for text, annotations in training_data:\n",
    "        doc = nlp(text) #Construct a Doc object\n",
    "        ents = []\n",
    "        for start, end, label in annotations:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list=[\"BuildingTag\",\"BuildingNoTag\",\"RecipientTag\",\"StreetNameTag\",\"ZipCodeTag\",\"CityTag\",\"StateTag\",\"CountryTag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(filepath_or_buffer=\"us-train-dataset.csv\",sep=\",\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_spans= create_entity_spans(df_train.astype(str),tag_list)\n",
    "training_data= df_entity_spans.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin_train= get_doc_bin(training_data,nlp)\n",
    "doc_bin_train.to_disk(\"train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(filepath_or_buffer=\"us-test-dataset.csv\",sep=\",\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_spans= create_entity_spans(df_test.astype(str),tag_list)\n",
    "validation_data= df_entity_spans.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin_test= get_doc_bin(validation_data,nlp)\n",
    "doc_bin_test.to_disk(\"test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing and training with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Auto-filled config with all values\n",
      "✔ Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 19:01:57.012184: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2022-06-05 19:01:57.012698: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Created output directory: output\n",
      "ℹ Saving to output directory: output\n",
      "ℹ Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "✔ Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "ℹ Pipeline: ['ner']\n",
      "ℹ Initial learn rate: 0.001\n",
      "E    #       LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  --------  ------  ------  ------  ------\n",
      "  0       0     66.65    6.46    4.40   12.21    0.06\n",
      "  0      10    824.19    0.00    0.00    0.00    0.00\n",
      "  1      20    466.36   25.49   35.62   19.85    0.25\n",
      "  1      30    388.29   58.18   71.91   48.85    0.58\n",
      "  2      40    340.52   71.73   80.19   64.89    0.72\n",
      "  2      50    274.83   74.70   78.81   70.99    0.75\n",
      "  3      60    197.17   90.00   90.70   89.31    0.90\n",
      "  3      70     94.29   92.72   93.08   92.37    0.93\n",
      "  4      80     58.15   94.62   95.35   93.89    0.95\n",
      "  4      90     35.47   93.13   93.13   93.13    0.93\n",
      "  5     100     39.30   96.55   96.92   96.18    0.97\n",
      "  5     110     22.16   96.18   96.18   96.18    0.96\n",
      "  6     120     22.06   97.34   96.97   97.71    0.97\n",
      "  7     130      9.73   97.32   97.69   96.95    0.97\n",
      "  7     140     12.81   97.32   97.69   96.95    0.97\n",
      "  8     150      7.70   95.75   96.88   94.66    0.96\n",
      "  8     160      4.29   96.15   96.90   95.42    0.96\n",
      "  9     170      8.21   95.79   96.15   95.42    0.96\n",
      " 10     180      7.40   95.79   96.15   95.42    0.96\n",
      " 10     190      0.24   95.79   96.15   95.42    0.96\n",
      " 11     200      0.21   96.15   96.90   95.42    0.96\n",
      " 11     210      2.05   96.15   96.90   95.42    0.96\n",
      " 12     220      2.84   95.79   96.15   95.42    0.96\n",
      " 13     230      5.81   96.15   96.90   95.42    0.96\n",
      " 13     240      0.46   95.75   96.88   94.66    0.96\n",
      " 14     250      0.01   97.32   97.69   96.95    0.97\n",
      " 15     260      0.00   97.34   96.97   97.71    0.97\n",
      " 15     270      0.00   97.34   96.97   97.71    0.97\n",
      " 16     280      0.00   97.34   96.97   97.71    0.97\n",
      " 17     290      0.01   97.34   96.97   97.71    0.97\n",
      " 17     300      0.00   96.15   96.90   95.42    0.96\n",
      "✔ Saved pipeline to output directory\n",
      "output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 19:04:01.593217: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2022-06-05 19:04:01.593746: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-06-05 19:04:05,219] [INFO] Set up nlp object from config\n",
      "[2022-06-05 19:04:05,228] [INFO] Pipeline: ['ner']\n",
      "[2022-06-05 19:04:05,232] [INFO] Created vocabulary\n",
      "[2022-06-05 19:04:05,239] [INFO] Finished initializing nlp object\n",
      "[2022-06-05 19:04:05,661] [INFO] Initialized pipeline components: ['ner']\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --paths.train train.spacy --paths.dev test.spacy --output output --training.eval_frequency 10 --training.max_steps 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"output\\model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_list=[\"130 W BOSE ST STE 100, PARK RIDGE, IL, 60068, USA\",\n",
    "              \"8311 MCDONALD RD, HOUSTON, TX, 77053-4821, USA\",\n",
    "              \"PO Box 317, 4100 Hwy 20 E Ste 403, NICEVILLE, FL, 32578-5037, USA\",\n",
    "              \"C/O Elon Musk Innovations Inc, 1548 E Florida Avenue, Suite 209, TAMPA, FL, 33613, USA\",\n",
    "              \"Seven Edgeway Plaza, C/O Mac Dermott Inc, OAKBROOK TERRACE, IL, 60181, USA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address string -> 130 W BOSE ST STE 100, PARK RIDGE, IL, 60068, USA\n",
      "Parsed address -> [('130', 'BUILDING_NO'), ('W BOSE ST', 'STREET_NAME'), ('PARK RIDGE', 'CITY'), ('IL', 'STATE'), ('60068', 'ZIP_CODE'), ('USA', 'COUNTRY')]\n",
      "******\n",
      "Address string -> 8311 MCDONALD RD, HOUSTON, TX, 77053-4821, USA\n",
      "Parsed address -> [('8311', 'BUILDING_NO'), ('MCDONALD RD', 'STREET_NAME'), ('HOUSTON', 'CITY'), ('TX', 'STATE'), ('77053-4821', 'ZIP_CODE'), ('USA', 'COUNTRY')]\n",
      "******\n",
      "Address string -> PO Box 317, 4100 Hwy 20 E Ste 403, NICEVILLE, FL, 32578-5037, USA\n",
      "Parsed address -> [('4100 Hwy 20 E Ste', 'ZIP_CODE'), ('NICEVILLE', 'CITY'), ('FL', 'STATE'), ('32578-5037', 'ZIP_CODE'), ('USA', 'COUNTRY')]\n",
      "******\n",
      "Address string -> C/O Elon Musk Innovations Inc, 1548 E Florida Avenue, Suite 209, TAMPA, FL, 33613, USA\n",
      "Parsed address -> [('C/O Elon Musk Innovations Inc', 'RECIPIENT'), ('1548', 'BUILDING_NO'), ('E Florida Avenue', 'STREET_NAME'), ('TAMPA', 'CITY'), ('FL', 'STATE'), ('33613', 'ZIP_CODE'), ('USA', 'COUNTRY')]\n",
      "******\n",
      "Address string -> Seven Edgeway Plaza, C/O Mac Dermott Inc, OAKBROOK TERRACE, IL, 60181, USA\n",
      "Parsed address -> [('Seven Edgeway Plaza', 'STREET_NAME'), ('C/O Mac Dermott Inc', 'RECIPIENT'), ('OAKBROOK TERRACE', 'CITY'), ('IL', 'STATE'), ('60181', 'ZIP_CODE'), ('USA', 'COUNTRY')]\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "for address in address_list:\n",
    "    doc=nlp(address)\n",
    "    ent_list=[(ent.text, ent.label_) for ent in doc.ents]\n",
    "    print(\"Address string -> \"+address)\n",
    "    print(\"Parsed address -> \"+str(ent_list))\n",
    "    print(\"******\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a38351ae415a46c9357d181cbc0605f144b2a6ed92a3304311c05688b4cb644f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
