{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zGiXUGe1Ud6Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import re   # regex\n",
    "import nltk # text handling\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from textblob import TextBlob # WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZNbFsfa_00Od"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import seaborn as sns \n",
    "from sklearn import preprocessing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "import re\n",
    "import string\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkQG38Mns_Sm",
    "outputId": "7444369a-7b29-4f86-ae19-a8158004592c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avitr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wdfNHINEnE0k"
   },
   "outputs": [],
   "source": [
    "# To see the whole tweet text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zlt2wFw-AdrY"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pld5-njokxdI"
   },
   "outputs": [],
   "source": [
    "# load specific columns from the csv file\n",
    "#col_list = [\"created_at\", \"id\", \"in_reply_to_status_id\", \n",
    " #           \"in_reply_to_user_id\", \"text\", \"user_screen_name\"]\n",
    "\n",
    "tweets = pd.read_excel('Sentiment Analysis Training.xlsx')\n",
    "#\n",
    " #                 , usecols = col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-t6yBIZQmXwc",
    "outputId": "24c906de-365d-42c1-b457-ee8cd8607995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4r9XpVoiTLQD",
    "outputId": "2b62c030-d1f8-4fd4-8e22-ff878e93333a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9qVVKShBZ5Z"
   },
   "source": [
    "# Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oFXQ99ueoC1F"
   },
   "outputs": [],
   "source": [
    "mentions = re.compile('(@[a-zA-Z0-9_]{1,50})') # mentions regex @...\n",
    "urls = re.compile('http\\S+')         # links regex\n",
    "diacritics = re.compile('Ÿ∞ Ÿë Ÿé Ÿã Ÿè Ÿå Ÿê Ÿç ŸíŸÄ')       # Tashkeel regex\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "arabic_punctuations = '''`√∑√óÿõ<>()*&^%][ŸÄÿå/:\"ÿü.,'{}~¬¶+|!‚Äù‚Ä¶‚Äú‚ÄìŸÄ¬´¬ª'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(mentions, '', text)\n",
    "    return text\n",
    "\n",
    "# removing URLs\n",
    "def remove_urls(text):\n",
    "    # twitter converts all links to thier own domain t.co\n",
    "    text = re.sub(urls, '', text)\n",
    "    return text\n",
    "\n",
    "# removing tashkeel\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(r'[\\u064b-\\u065f]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # https://www.linkedin.com/pulse/extracting-twitter-data-pre-processing-sentiment-using-jayasekara\n",
    "    return re.sub(emoji_pattern, '', text)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # lemmatizes text\n",
    "    lemmatized_text = []\n",
    "\n",
    "    # set up arabic lemmatizer Farasa\n",
    "    url = 'https://farasa.qcri.org/webapi/lemmatization/'\n",
    "    api_key = \"lErIOPgmHZtflLMgIf\"\n",
    "\n",
    "    # set up english lemmatizer\n",
    "    eng_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    for word in text:\n",
    "        # Detect language to use proper lemmatizer\n",
    "        if TextBlob(word).detect_language() == 'en':\n",
    "            lemmatized_text.append(eng_lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            payload = {'text': word, 'api_key': api_key}\n",
    "            data = requests.post(url, data=payload)\n",
    "            lemmatized_text.append(json.loads(data.text))\n",
    "    return lemmatized_text\n",
    "\n",
    "def text_stemming(text):\n",
    "    return ISRIStemmer().suf32(text)\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    translator = str.maketrans(' ', ' ', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    text = re.sub(\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n",
    "    text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
    "    text = re.sub(\"ÿ§\", \"ÿ°\", text)\n",
    "    text = re.sub(\"ÿ¶\", \"ÿ°\", text)\n",
    "    text = re.sub(\"ÿ©\", \"Ÿá\", text)\n",
    "    text = re.sub(\"⁄Ø\", \"ŸÉ\", text)\n",
    "    return text\n",
    "    \n",
    "# remove hashtags marks but keep the words itself\n",
    "def normalize_hashtags(text):\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"_\", \" \", text)\n",
    "    text = re.sub(\"_\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1jefUK1oOrW",
    "outputId": "d31497b0-a9c9-447c-c543-bb4ed446505c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÿßŸäÿßŸáŸÜ', 'ÿµÿ®ÿ±', 'ÿ∫', 'ÿßŸäÿßÿ±', 'ÿ≠ÿßÿ°']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_stopwords = map(normalize_arabic, stopwords)\n",
    "normalized_stopwords = list(normalized_stopwords)\n",
    "normalized_stopwords = list(set(normalized_stopwords))\n",
    "normalized_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iXZawvM4sqfD"
   },
   "outputs": [],
   "source": [
    "# clean text\n",
    "def text_preprocessing(text):\n",
    "    text = remove_diacritics(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = normalize_hashtags(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = ' '.join(word for word in text.split() if word not in normalized_stopwords)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-v1YRoLqNAdT"
   },
   "outputs": [],
   "source": [
    "tweets['text']=tweets['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q12YEnowtE4c"
   },
   "outputs": [],
   "source": [
    "tweets['clean_tweet'] = tweets['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VaNcuYsLOJd"
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj6-qVOpdDbq"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "isdDOMjELyZa"
   },
   "outputs": [],
   "source": [
    "file_model = tweets.copy()\n",
    "file_model = file_model[file_model.clean_tweet.str.len()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÿßÿ®ÿ¥ÿ±ŸÉ',\n",
       " 'ÿ®ÿ™ÿØÿÆŸÑ',\n",
       " 'ÿßŸÑŸäŸàŸÖ',\n",
       " 'ÿßŸÑÿπÿßÿ¥ÿ±',\n",
       " 'ÿπÿ¥ÿßŸÜ',\n",
       " 'Ÿäÿ±ÿØŸàÿß',\n",
       " 'ŸäÿÆÿØŸÖŸàŸÜŸÉ',\n",
       " 'ŸäÿßÿÆŸä',\n",
       " 'ÿ™ÿ≠ÿ≥',\n",
       " 'stc',\n",
       " 'ŸÖÿßŸäÿØÿ±ŸàŸÜ',\n",
       " 'ÿ±ÿ°ŸäŸá',\n",
       " 'Ÿ¢Ÿ†Ÿ£Ÿ†',\n",
       " 'ÿßŸÑÿ≠ŸäŸÜ',\n",
       " 'ŸàŸÜŸÅÿ≥',\n",
       " 'ÿßŸÑÿßÿÆÿ∑ÿßÿ°',\n",
       " 'ÿπÿ¥ÿ±ÿßÿ™',\n",
       " 'ÿßŸÑÿ≥ŸÜŸäŸÜ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_model['clean_tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6Gzyyoew7yI3"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from time import time \n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mAnNWDIiG0l2"
   },
   "outputs": [],
   "source": [
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:21:02: collecting all words and their counts\n",
      "INFO - 21:21:02: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 21:21:02: collected 1839 word types from a corpus of 1239 words (unigram + bigrams) and 125 sentences\n",
      "INFO - 21:21:02: using 1839 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 21:21:02: source_vocab length 1839\n",
      "INFO - 21:21:02: Phraser built with 55 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ÿßÿ®ÿ¥ÿ±ŸÉ', 'Ÿ£ÿÆÿ∑Ÿàÿ∑', 'ŸÜŸÇŸÑÿ™Ÿáÿß', 'Ÿàÿ™Ÿàÿ®Ÿá', 'ŸàŸÖÿßÿπÿßÿØ', 'ÿ±ÿ¨ÿπŸá']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine common expressions from 2 words that are repeated in the data\n",
    "sent = [row for row in file_model.clean_tweet]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1] # print the first tweet after the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [ÿßÿ®ÿ¥ÿ±ŸÉ, ÿ®ÿ™ÿØÿÆŸÑ, ÿßŸÑŸäŸàŸÖ, ÿßŸÑÿπÿßÿ¥ÿ±, ÿπÿ¥ÿßŸÜ, Ÿäÿ±ÿØŸàÿß, ŸäÿÆÿØŸÖŸàŸÜŸÉ, ŸäÿßÿÆŸä, ÿ™ÿ≠ÿ≥, stc, ŸÖÿßŸäÿØÿ±ŸàŸÜ, ÿ±ÿ°ŸäŸá, Ÿ¢Ÿ†Ÿ£Ÿ†, ÿßŸÑÿ≠ŸäŸÜ, ŸàŸÜŸÅÿ≥, ÿßŸÑÿßÿÆÿ∑ÿßÿ°, ÿπÿ¥ÿ±ÿßÿ™, ÿßŸÑÿ≥ŸÜŸäŸÜ]\n",
       "1                                                                                        [ÿßÿ®ÿ¥ÿ±ŸÉ, Ÿ£ÿÆÿ∑Ÿàÿ∑, ŸÜŸÇŸÑÿ™Ÿáÿß, Ÿàÿ™Ÿàÿ®Ÿá, ŸàŸÖÿßÿπÿßÿØ, ÿ±ÿ¨ÿπŸá]\n",
       "2                                                                [ÿßÿÆÿ∞ÿ™Ÿá, ÿ±ŸÇŸÖŸä, ÿßŸÑÿ´ÿßŸÜŸä, ŸÅÿ±ÿπ, ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ, ŸÖŸàŸÑ, ÿßŸÑŸÖŸàÿ∂Ÿàÿπ, ÿØŸÇÿßŸäŸÇ, ÿ¥ŸÉÿ±ÿß, stc]\n",
       "3                                                                                                                   [ŸäÿπŸàÿ∂ŸÜŸä, stc, ü•∫]\n",
       "4                                            [ÿ™ÿ±ÿß, ŸÖÿßŸäŸÜŸÅÿπ, ŸÖÿπŸáŸÖ, ÿßŸÜŸÉ, ÿ™ÿ±Ÿàÿ≠, ŸÑÿßŸÇÿ±ÿ®, ŸÅÿ±ÿπ, Ÿàÿ™ŸÅÿßŸáŸÖ, ŸÖÿπŸáŸÖ, ŸàŸÇŸÑŸáŸÖ, ÿ®ÿ≠ŸàŸÑ, ÿπŸÜŸÉŸÖ, ÿ¥ŸàŸÅ, Ÿäÿ±ÿ¨ÿπŸà]\n",
       "                                                                   ...                                                              \n",
       "128                                                                                         [ŸäÿßÿÆŸä, ŸÑŸäŸá, ŸÖÿßÿ™ÿ±ÿØŸàŸÜ, Ÿàÿ™ÿÆŸÑÿµŸàŸÜ, ŸÖÿ¥ŸÉŸÑÿ™Ÿä, ü§å]\n",
       "129                                                                      [ÿÆŸÑÿßÿµ, ŸÅÿπŸÑÿ™Ÿá, ÿ®ÿ∑ÿ±ŸäŸÇŸá, ÿ´ÿßŸÜŸäŸá, mystc, Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá, ÿ®ÿßŸÑÿ±ŸÖÿ≤, ÿßŸÑŸäÿØŸàŸä]\n",
       "130                                                                           [ÿßŸÑÿÆŸäÿ±, ÿßŸÑÿ∫Ÿäÿ™, ÿ±ŸÇŸÖŸä, ÿßŸÑŸÖŸÅŸàÿ™ÿ±, ŸàŸÜÿ≤ŸÑÿ™, ŸÖÿØŸäŸàÿßŸÜŸäŸá, ÿ≥ŸÑÿßŸÖÿßÿ™]\n",
       "131                                                                                                                 [ÿ≥Ÿàÿß, ÿ®Ÿàÿ≥ÿ™, ÿ®ŸÑÿ≥]\n",
       "132                                                                                            [ÿ™ŸÅÿπŸäŸÑ, ŸÖŸÉÿßŸÑŸÖÿßÿ™, ÿ≥Ÿàÿß, ŸÖÿ≠ÿØŸàÿØ, ÿ≠ŸÇ, ÿ¥Ÿáÿ±]\n",
       "Name: clean_tweet, Length: 125, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_model['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcŸàÿ¥</th>\n",
       "      <th>...</th>\n",
       "      <th>Ÿ°Ÿ®</th>\n",
       "      <th>Ÿ¢Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ†Ÿ£Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ§</th>\n",
       "      <th>Ÿ¢Ÿ©</th>\n",
       "      <th>Ÿ£ÿÆÿ∑Ÿàÿ∑</th>\n",
       "      <th>Ÿ£Ÿ†Ÿ°Ÿ¢</th>\n",
       "      <th>Ÿ§ÿ≥ÿßÿπÿßÿ™</th>\n",
       "      <th>Ÿ•Ÿ†</th>\n",
       "      <th>Ÿ®ÿ≥ŸÜŸàÿßÿ™</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10   13   5g   90  iphone   my  mystc  stc  stcpay  stcŸàÿ¥  ...   Ÿ°Ÿ®   Ÿ¢Ÿ†  \\\n",
       "0  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "\n",
       "   Ÿ¢Ÿ†Ÿ£Ÿ†   Ÿ¢Ÿ§   Ÿ¢Ÿ©  Ÿ£ÿÆÿ∑Ÿàÿ∑  Ÿ£Ÿ†Ÿ°Ÿ¢  Ÿ§ÿ≥ÿßÿπÿßÿ™   Ÿ•Ÿ†  Ÿ®ÿ≥ŸÜŸàÿßÿ™  \n",
       "0   1.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "1   0.0  0.0  0.0    1.0   0.0     0.0  0.0     0.0  \n",
       "2   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "3   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "4   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "\n",
       "[5 rows x 810 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =10000)\n",
    "\n",
    "cleaned_tweets = file_model['clean_tweet'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# unigram = word_vectorizer.fit(file_model['clean_tweet'])\n",
    "unigramdataGet= word_vectorizer.fit_transform(cleaned_tweets)\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emTSCfqhGYvh",
    "outputId": "ee46ebfb-9eb9-4bea-ec0b-e547ab318f35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:09:01: collecting all words and their counts\n",
      "INFO - 22:09:01: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:09:01: collected 833 word types from a corpus of 1141 raw words and 125 sentences\n",
      "INFO - 22:09:01: Loading a fresh vocabulary\n",
      "INFO - 22:09:01: effective_min_count=3 retains 61 unique words (7% of original 833, drops 772)\n",
      "INFO - 22:09:01: effective_min_count=3 leaves 244 word corpus (21% of original 1141, drops 897)\n",
      "INFO - 22:09:01: deleting the raw counts dictionary of 833 items\n",
      "INFO - 22:09:01: sample=0.001 downsamples 61 most-common words\n",
      "INFO - 22:09:01: downsampling leaves estimated 74 word corpus (30.4% of prior 244)\n",
      "INFO - 22:09:01: estimated required memory for 61 words and 100 dimensions: 79300 bytes\n",
      "INFO - 22:09:01: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=3,\n",
    "                     min_alpha=0.0007, \n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MirvAxX3V_Qz",
    "outputId": "112f7d81-6d98-4fb8-db42-f76369d575bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:09:27: training model with 7 workers on 61 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 1 : training on 1141 raw words (77 effective words) took 0.0s, 15555 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 2 : training on 1141 raw words (81 effective words) took 0.0s, 14335 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 3 : training on 1141 raw words (78 effective words) took 0.0s, 14363 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 4 : training on 1141 raw words (68 effective words) took 0.0s, 10844 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 5 : training on 1141 raw words (53 effective words) took 0.0s, 10073 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 6 : training on 1141 raw words (75 effective words) took 0.0s, 17844 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 7 : training on 1141 raw words (70 effective words) took 0.0s, 12699 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 8 : training on 1141 raw words (73 effective words) took 0.0s, 17460 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 9 : training on 1141 raw words (77 effective words) took 0.0s, 15969 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 10 : training on 1141 raw words (69 effective words) took 0.0s, 10113 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 11 : training on 1141 raw words (80 effective words) took 0.0s, 13530 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 12 : training on 1141 raw words (76 effective words) took 0.0s, 11735 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 13 : training on 1141 raw words (75 effective words) took 0.0s, 14024 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 14 : training on 1141 raw words (75 effective words) took 0.0s, 10841 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 15 : training on 1141 raw words (73 effective words) took 0.0s, 16139 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 16 : training on 1141 raw words (67 effective words) took 0.0s, 11823 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 17 : training on 1141 raw words (74 effective words) took 0.0s, 9902 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 18 : training on 1141 raw words (80 effective words) took 0.0s, 12746 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 19 : training on 1141 raw words (74 effective words) took 0.0s, 11882 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 20 : training on 1141 raw words (90 effective words) took 0.0s, 13757 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 21 : training on 1141 raw words (83 effective words) took 0.0s, 15695 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 22 : training on 1141 raw words (90 effective words) took 0.0s, 19835 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 23 : training on 1141 raw words (69 effective words) took 0.0s, 10076 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 24 : training on 1141 raw words (78 effective words) took 0.0s, 15207 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 25 : training on 1141 raw words (64 effective words) took 0.0s, 9439 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 26 : training on 1141 raw words (69 effective words) took 0.0s, 13404 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 27 : training on 1141 raw words (66 effective words) took 0.0s, 13525 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 28 : training on 1141 raw words (68 effective words) took 0.0s, 12372 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 29 : training on 1141 raw words (80 effective words) took 0.0s, 9506 effective words/s\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:09:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:09:27: EPOCH - 30 : training on 1141 raw words (76 effective words) took 0.0s, 15865 effective words/s\n",
      "INFO - 22:09:27: training on a 34230 raw words (2228 effective words) took 0.5s, 4744 effective words/s\n",
      "WARNING - 22:09:27: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "INFO - 22:09:27: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRnoAWW3WJox",
    "outputId": "9c05cbed-c3a8-4e11-9bfd-1743ee577dea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:11:47: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 22:11:47: not storing attribute vectors_norm\n",
      "INFO - 22:11:47: not storing attribute cum_table\n",
      "INFO - 22:11:47: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# store the word2vec model\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6B-V2jBiWdbm",
    "outputId": "2f1bff77-611e-4560-d864-d0bbd5c87310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:11:48: loading Word2Vec object from word2vec.model\n",
      "INFO - 22:11:48: loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO - 22:11:48: setting ignored attribute vectors_norm to None\n",
      "INFO - 22:11:48: loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 22:11:48: loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "INFO - 22:11:48: setting ignored attribute cum_table to None\n",
      "INFO - 22:11:48: loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'text', 'label', 'tweet_type', 'tweet_topic', 'clean_tweet'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "IhHPF6ocgorY"
   },
   "outputs": [],
   "source": [
    "file_export = file_model.copy()\n",
    "file_export['old_tweets'] = file_export.clean_tweet\n",
    "file_export.old_tweets = file_export.old_tweets.str.join(' ')\n",
    "file_export.clean_tweet = file_export.clean_tweet.apply(lambda x: ' '.join(bigram[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_tweets</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ</td>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá</td>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc</td>\n",
       "      <td>ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫</td>\n",
       "      <td>ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà</td>\n",
       "      <td>ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å</td>\n",
       "      <td>ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä</td>\n",
       "      <td>ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™</td>\n",
       "      <td>ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>ÿ≥Ÿàÿß ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥</td>\n",
       "      <td>ÿ≥Ÿàÿß_ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±</td>\n",
       "      <td>ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     old_tweets  \\\n",
       "0    ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ   \n",
       "1                                                                          ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá   \n",
       "2                                                      ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc   \n",
       "3                                                                                                  ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫   \n",
       "4                                      ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà   \n",
       "..                                                                                                          ...   \n",
       "128                                                                           ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å   \n",
       "129                                                          ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä   \n",
       "130                                                              ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™   \n",
       "131                                                                                                ÿ≥Ÿàÿß ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥   \n",
       "132                                                                              ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±   \n",
       "\n",
       "                                                                                                    clean_tweet  \n",
       "0    ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ  \n",
       "1                                                                          ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá  \n",
       "2                                                      ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc  \n",
       "3                                                                                                  ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫  \n",
       "4                                      ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà  \n",
       "..                                                                                                          ...  \n",
       "128                                                                           ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å  \n",
       "129                                                          ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä  \n",
       "130                                                              ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™  \n",
       "131                                                                                                ÿ≥Ÿàÿß_ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥  \n",
       "132                                                                              ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±  \n",
       "\n",
       "[125 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_export[['old_tweets','clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "pKSd_Sn6-g7p"
   },
   "outputs": [],
   "source": [
    "file_export.to_csv('cleaned_dataset for training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "bfYrtmM-sX71"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "4g3pLeGFsCpn",
    "outputId": "fb4482ee-ac15-45d5-9121-b09763be40f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcŸàÿ¥</th>\n",
       "      <th>...</th>\n",
       "      <th>Ÿ°ŸßŸ†</th>\n",
       "      <th>Ÿ°Ÿ®</th>\n",
       "      <th>Ÿ¢Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ†Ÿ£Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ§</th>\n",
       "      <th>Ÿ¢Ÿ©</th>\n",
       "      <th>Ÿ£ÿÆÿ∑Ÿàÿ∑</th>\n",
       "      <th>Ÿ£Ÿ†Ÿ°Ÿ¢</th>\n",
       "      <th>Ÿ§ÿ≥ÿßÿπÿßÿ™</th>\n",
       "      <th>Ÿ®ÿ≥ŸÜŸàÿßÿ™</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10   13   5g   90  iphone   my  mystc  stc  stcpay  stcŸàÿ¥  ...  Ÿ°ŸßŸ†   Ÿ°Ÿ®  \\\n",
       "0  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "\n",
       "    Ÿ¢Ÿ†  Ÿ¢Ÿ†Ÿ£Ÿ†   Ÿ¢Ÿ§   Ÿ¢Ÿ©  Ÿ£ÿÆÿ∑Ÿàÿ∑  Ÿ£Ÿ†Ÿ°Ÿ¢  Ÿ§ÿ≥ÿßÿπÿßÿ™  Ÿ®ÿ≥ŸÜŸàÿßÿ™  \n",
       "0  0.0   1.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "1  0.0   0.0  0.0  0.0    1.0   0.0     0.0     0.0  \n",
       "2  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "3  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "4  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 824 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =10000)\n",
    "\n",
    "unigramdataGet = word_vectorizer.fit(file_export['clean_tweet']).transform(file_export['clean_tweet'])\n",
    "# unigramdataGet= word_vectorizer.transform(unigram)\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_file = 'vectorizer.pickle'\n",
    "pickle.dump(word_vectorizer, open(vec_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectorizer.pickle'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "jnctNBe8scne"
   },
   "outputs": [],
   "source": [
    "pro= preprocessing.LabelEncoder()\n",
    "encpro=pro.fit_transform(file_export['label'])\n",
    "file_export['label'] = encpro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Z3p4RTPSsc0d"
   },
   "outputs": [],
   "source": [
    "y=file_export['label']\n",
    "X=unigramdata_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "iYfurQbTsC8g"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "DsnssSJbbLur",
    "outputId": "e9ae2e90-aa8c-43fc-a384-dc406edc09b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcŸàÿ¥</th>\n",
       "      <th>...</th>\n",
       "      <th>Ÿ°ŸßŸ†</th>\n",
       "      <th>Ÿ°Ÿ®</th>\n",
       "      <th>Ÿ¢Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ†Ÿ£Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ§</th>\n",
       "      <th>Ÿ¢Ÿ©</th>\n",
       "      <th>Ÿ£ÿÆÿ∑Ÿàÿ∑</th>\n",
       "      <th>Ÿ£Ÿ†Ÿ°Ÿ¢</th>\n",
       "      <th>Ÿ§ÿ≥ÿßÿπÿßÿ™</th>\n",
       "      <th>Ÿ®ÿ≥ŸÜŸàÿßÿ™</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10   13   5g   90  iphone   my  mystc  stc  stcpay  stcŸàÿ¥  ...  Ÿ°ŸßŸ†   Ÿ°Ÿ®  \\\n",
       "5   0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "78  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "48  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "98  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "77  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "\n",
       "     Ÿ¢Ÿ†  Ÿ¢Ÿ†Ÿ£Ÿ†   Ÿ¢Ÿ§   Ÿ¢Ÿ©  Ÿ£ÿÆÿ∑Ÿàÿ∑  Ÿ£Ÿ†Ÿ°Ÿ¢  Ÿ§ÿ≥ÿßÿπÿßÿ™  Ÿ®ÿ≥ŸÜŸàÿßÿ™  \n",
       "5   0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "78  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "48  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "98  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "77  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 824 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nY4TOoL_sC_Z",
    "outputId": "20fcb077-b68c-4268-cd93-baa27938ab9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=GaussianNB()\n",
    "nb= nb.fit(X_train , y_train)\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwU29NoltQ_B",
    "outputId": "71edd4ad-d880-41e1-abe1-332d9b9cf924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.520\n",
      "Accuracy= 0.970\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.predict(X_test)\n",
    "nb_1=nb.score(X_test, y_test)\n",
    "print('Accuracy= {:.3f}'.format(nb.score(X_test, y_test)))\n",
    "print('Accuracy= {:.3f}'.format(nb.score(X_train , y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.50      0.62        18\n",
      "           1       0.57      0.67      0.62         6\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.52        25\n",
      "   macro avg       0.46      0.39      0.41        25\n",
      "weighted avg       0.73      0.52      0.59        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_file = 'classification.model'\n",
    "pickle.dump(nb, open(mod_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Replies.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-eb4452789411>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreplies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Replies.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1072\u001b[0m                     \u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(path, content, storage_options)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mcontent_or_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     ) as handle:\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Replies.xlsx'"
     ]
    }
   ],
   "source": [
    "replies = pd.read_excel('Replies.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327093, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['KSA', 'created_at', 'Date', 'Day', 'Hour', 'hashtags', 'media', 'urls',\n",
       "       'favorite_count', 'id', 'in_reply_to_screen_name',\n",
       "       'in_reply_to_status_id', 'in_reply_to_user_id', 'lang', 'place',\n",
       "       'possibly_sensitive', 'retweet_count', 'retweet_id',\n",
       "       'retweet_screen_name', 'source', 'text', 'tweet_url', 'user_created_at',\n",
       "       'user_screen_name', 'user_default_profile_image', 'user_description',\n",
       "       'user_favourites_count', 'user_followers_count', 'user_friends_count',\n",
       "       'user_listed_count', 'user_location', 'user_name', 'user_screen_name_1',\n",
       "       'user_statuses_count', 'user_time_zone', 'user_urls', 'user_verified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['text']=replies['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['clean_tweet'] = replies['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8ec7be3d016d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m                     \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m                 )\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-8ec7be3d016d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bigram' is not defined"
     ]
    }
   ],
   "source": [
    "replies.clean_tweet  = replies.clean_tweet.apply(lambda x: ' '.join(bigram[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['clean_tweet'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('classification.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=10000, strip_accents='unicode', sublinear_tf=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-d2027f6adc2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ŸÉŸÑŸÖŸÜŸä ÿßŸÜÿ≥ÿ™ÿß'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1249\u001b[1;33m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m                 \"string object received.\")\n\u001b[0;32m   1251\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "loaded_vectorizer.transform('ŸÉŸÑŸÖŸÜŸä ÿßŸÜÿ≥ÿ™ÿß')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(replies['clean_tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-1fcc36c21198>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "replies['label'] = loaded_model.predict(loaded_vectorizer.transform(replies['clean_tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-0175a11ee006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "loaded_vectorizer.transform(replies['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " loaded_model.predict(loaded_vectorizer(replies['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-837eb03c8063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreplies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassify_utterance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-cd86921007c6>\u001b[0m in \u001b[0;36mclassify_utterance\u001b[1;34m(utt)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classification.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1866\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \"\"\"\n\u001b[1;32m-> 1868\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "for i in replies:\n",
    "    replies['label']=replies.clean_tweet.apply(classify_utterance([i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer.fit_transform(file_export['clean_tweet'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-4bca21a165b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreplies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassify_utterance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-cd86921007c6>\u001b[0m in \u001b[0;36mclassify_utterance\u001b[1;34m(utt)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classification.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1866\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \"\"\"\n\u001b[1;32m-> 1868\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "# for i in replies:\n",
    "#     replies['label']=replies.clean_tweet.apply(classify_utterance([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-81692e922dee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     max_features =10000)\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0munigramdataGet\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mword_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0munigramdataGet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigramdataGet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1866\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \"\"\"\n\u001b[1;32m-> 1868\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =10000)\n",
    "\n",
    "unigramdataGet= word_vectorizer.transform(replies['clean_tweet'].to_list())\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['label']=replies['clean_tweet'].apply(nb.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNMQzz-QtROu",
    "outputId": "d8a338aa-9c72-4050-8169-a43dfdf32b49"
   },
   "outputs": [],
   "source": [
    "RC= RidgeClassifier()\n",
    "RC= RC.fit(X_train , y_train)\n",
    "RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bbv78dPntXqJ",
    "outputId": "ef1468c4-909a-4e24-848c-6c5da4e43f4d"
   },
   "outputs": [],
   "source": [
    "y_pred = RC.predict(X_test)\n",
    "rc_1=RC.score(X_test, y_test)\n",
    "print('Accuracy= {:.3f}'.format(RC.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOUMSBSzsDCo",
    "outputId": "3e72fa9e-351d-4422-9a44-5c97d78eb880"
   },
   "outputs": [],
   "source": [
    "PC= PassiveAggressiveClassifier()\n",
    "PC= PC.fit(X_train , y_train)\n",
    "PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIRVhqAWtc9F",
    "outputId": "1f325aa1-e958-4835-9c25-59d5e0a25bc2"
   },
   "outputs": [],
   "source": [
    "y_pred = PC.predict(X_test)\n",
    "pc_1=PC.score(X_test, y_test)\n",
    "pc_2=PC.score(X_train , y_train)\n",
    "print('Accuracy= {:.3f}'.format(PC.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w04BQCz0tdKW",
    "outputId": "06613c41-e821-436d-c3ad-8cbbebe15361"
   },
   "outputs": [],
   "source": [
    "\n",
    "LR= LogisticRegression(penalty = 'l2', C = 1)\n",
    "LR= LR.fit(X_train , y_train)\n",
    "LR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7pz1ceZtdOh",
    "outputId": "0484dd75-276b-4693-c054-a452c576db50"
   },
   "outputs": [],
   "source": [
    "y_pred = LR.predict(X_test)\n",
    "lr_1=LR.score(X_test, y_test)\n",
    "print('Accuracy= {:.3f}'.format(LR.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwWpY7TRt4k4"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAiZhWSNtlZ3",
    "outputId": "1138f829-884f-4af1-b89e-fa0562c0e0b3"
   },
   "outputs": [],
   "source": [
    "x = PrettyTable()\n",
    "print('\\n')\n",
    "print(\"Comparison of all algorithms on F1 score\")\n",
    "x.field_names = [\"Model\", \"Accuracy\"]\n",
    "\n",
    "\n",
    "x.add_row([\"Naive Bayes Algorithm\", round(nb_1,2)])\n",
    "x.add_row([\"Ridge Classifier Algorithm\",  round(rc_1,2)])\n",
    "x.add_row([\"Passive Aggressive Classifier Algorithm\", round(pc_1,2)])\n",
    "x.add_row([\"Logistic Regression Algorithm\", round(lr_1,2)])\n",
    "\n",
    "print(x)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Yc-2tUmhT8u"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "_dDnkYIFhVQ6",
    "outputId": "abb3c541-a738-4001-8588-ea9fa04f85aa"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qf9OVWPPrWD"
   },
   "outputs": [],
   "source": [
    "stc=[\"ÿßŸÑÿÆÿØŸÖÿ© ÿ¨ŸäÿØÿ©\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRJGXB86PxCf"
   },
   "outputs": [],
   "source": [
    "x = word_vectorizer.transform(stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOLtI6_jSiPG"
   },
   "outputs": [],
   "source": [
    "x = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSVKLhY7PpmW",
    "outputId": "b499f4ab-f0b3-4dc8-88a8-cc92345c838a"
   },
   "outputs": [],
   "source": [
    "pred=nb.predict(x)\n",
    "pred=pro.inverse_transform(pred)\n",
    "prediction=pd.DataFrame(pred, columns=['Prediction']) \n",
    "print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdozQrRXTDVL"
   },
   "outputs": [],
   "source": [
    "tweets = file_export.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRLEevnOblWi",
    "outputId": "b2007b98-4022-4415-91e2-80db9e258573"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmBQrUKHbfrq",
    "outputId": "6dc78c51-f03e-4742-936c-2d1c474b90ab"
   },
   "outputs": [],
   "source": [
    "tweets.clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKQOKpsPUdHY",
    "outputId": "82736079-1aae-4aab-a228-8482270060b4"
   },
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haTLjZO1c2hx"
   },
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer.fit_transform(file_export['clean_tweet'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcdxufrxdOWR",
    "outputId": "20a87305-0b10-4b36-eee4-4ca887ecb8d8"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6wmsgplaskR",
    "outputId": "7911659b-b554-4779-f287-cc0ae73c024c"
   },
   "outputs": [],
   "source": [
    "nb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "gRWtO9B3UTzi",
    "outputId": "d12eb0c1-3d4a-4b06-b11a-719721cdc563"
   },
   "outputs": [],
   "source": [
    "tweets[\"predict\"] = nb.predict(tweets[\"clean_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKzscSNqVJzd"
   },
   "outputs": [],
   "source": [
    "prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwr5WR8OTDgh"
   },
   "outputs": [],
   "source": [
    "for i in tweets:\n",
    "  X = word_vectorizer.transform(i)\n",
    "  pred=pro.inverse_transform(pred)\n",
    "  prediction=pd.DataFrame(pred, columns=['Prediction']) \n",
    "  print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AVXxFUrRKrr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tNu4GkZzRK-K",
    "outputId": "298fa9a4-b0e9-4949-dafd-00a00429c184"
   },
   "outputs": [],
   "source": [
    "file_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "cZRREeipAoRc",
    "outputId": "b71a8f05-a3e7-4e23-a087-d94a37eb30e1"
   },
   "outputs": [],
   "source": [
    "file_export = reset.index()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Tweets Clustering using Word2Vec and K-means",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
