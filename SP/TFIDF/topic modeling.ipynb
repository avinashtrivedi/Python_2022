{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zGiXUGe1Ud6Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import re   # regex\n",
    "import nltk # text handling\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from textblob import TextBlob # WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZNbFsfa_00Od"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import seaborn as sns \n",
    "from sklearn import preprocessing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "import re\n",
    "import string\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkQG38Mns_Sm",
    "outputId": "7444369a-7b29-4f86-ae19-a8158004592c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avitr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wdfNHINEnE0k"
   },
   "outputs": [],
   "source": [
    "# To see the whole tweet text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zlt2wFw-AdrY"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pld5-njokxdI"
   },
   "outputs": [],
   "source": [
    "# load specific columns from the csv file\n",
    "#col_list = [\"created_at\", \"id\", \"in_reply_to_status_id\", \n",
    " #           \"in_reply_to_user_id\", \"text\", \"user_screen_name\"]\n",
    "\n",
    "tweets = pd.read_excel('Sentiment Analysis Training.xlsx')\n",
    "#\n",
    " #                 , usecols = col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-t6yBIZQmXwc",
    "outputId": "24c906de-365d-42c1-b457-ee8cd8607995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4r9XpVoiTLQD",
    "outputId": "2b62c030-d1f8-4fd4-8e22-ff878e93333a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9qVVKShBZ5Z"
   },
   "source": [
    "# Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oFXQ99ueoC1F"
   },
   "outputs": [],
   "source": [
    "mentions = re.compile('(@[a-zA-Z0-9_]{1,50})') # mentions regex @...\n",
    "urls = re.compile('http\\S+')         # links regex\n",
    "diacritics = re.compile('Ÿ∞ Ÿë Ÿé Ÿã Ÿè Ÿå Ÿê Ÿç ŸíŸÄ')       # Tashkeel regex\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "arabic_punctuations = '''`√∑√óÿõ<>()*&^%][ŸÄÿå/:\"ÿü.,'{}~¬¶+|!‚Äù‚Ä¶‚Äú‚ÄìŸÄ¬´¬ª'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(mentions, '', text)\n",
    "    return text\n",
    "\n",
    "# removing URLs\n",
    "def remove_urls(text):\n",
    "    # twitter converts all links to thier own domain t.co\n",
    "    text = re.sub(urls, '', text)\n",
    "    return text\n",
    "\n",
    "# removing tashkeel\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(r'[\\u064b-\\u065f]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # https://www.linkedin.com/pulse/extracting-twitter-data-pre-processing-sentiment-using-jayasekara\n",
    "    return re.sub(emoji_pattern, '', text)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # lemmatizes text\n",
    "    lemmatized_text = []\n",
    "\n",
    "    # set up arabic lemmatizer Farasa\n",
    "    url = 'https://farasa.qcri.org/webapi/lemmatization/'\n",
    "    api_key = \"lErIOPgmHZtflLMgIf\"\n",
    "\n",
    "    # set up english lemmatizer\n",
    "    eng_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    for word in text:\n",
    "        # Detect language to use proper lemmatizer\n",
    "        if TextBlob(word).detect_language() == 'en':\n",
    "            lemmatized_text.append(eng_lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            payload = {'text': word, 'api_key': api_key}\n",
    "            data = requests.post(url, data=payload)\n",
    "            lemmatized_text.append(json.loads(data.text))\n",
    "    return lemmatized_text\n",
    "\n",
    "def text_stemming(text):\n",
    "    return ISRIStemmer().suf32(text)\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    translator = str.maketrans(' ', ' ', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    # from https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "    text = re.sub(\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n",
    "    text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
    "    text = re.sub(\"ÿ§\", \"ÿ°\", text)\n",
    "    text = re.sub(\"ÿ¶\", \"ÿ°\", text)\n",
    "    text = re.sub(\"ÿ©\", \"Ÿá\", text)\n",
    "    text = re.sub(\"⁄Ø\", \"ŸÉ\", text)\n",
    "    return text\n",
    "    \n",
    "# remove hashtags marks but keep the words itself\n",
    "def normalize_hashtags(text):\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"_\", \" \", text)\n",
    "    text = re.sub(\"_\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1jefUK1oOrW",
    "outputId": "d31497b0-a9c9-447c-c543-bb4ed446505c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÿßÿ∂', 'ÿ®ŸÖÿß', 'ÿ±ÿßÿ≠', 'ÿßŸÖÿßŸÖ', 'ÿ™ÿ±ŸÉ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_stopwords = map(normalize_arabic, stopwords)\n",
    "normalized_stopwords = list(normalized_stopwords)\n",
    "normalized_stopwords = list(set(normalized_stopwords))\n",
    "normalized_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iXZawvM4sqfD"
   },
   "outputs": [],
   "source": [
    "# clean text\n",
    "def text_preprocessing(text):\n",
    "    text = remove_diacritics(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = normalize_hashtags(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = ' '.join(word for word in text.split() if word not in normalized_stopwords)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-v1YRoLqNAdT"
   },
   "outputs": [],
   "source": [
    "tweets['text']=tweets['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q12YEnowtE4c"
   },
   "outputs": [],
   "source": [
    "tweets['clean_tweet'] = tweets['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VaNcuYsLOJd"
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj6-qVOpdDbq"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "isdDOMjELyZa"
   },
   "outputs": [],
   "source": [
    "file_model = tweets.copy()\n",
    "file_model = file_model[file_model.clean_tweet.str.len()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÿßÿ®ÿ¥ÿ±ŸÉ',\n",
       " 'ÿ®ÿ™ÿØÿÆŸÑ',\n",
       " 'ÿßŸÑŸäŸàŸÖ',\n",
       " 'ÿßŸÑÿπÿßÿ¥ÿ±',\n",
       " 'ÿπÿ¥ÿßŸÜ',\n",
       " 'Ÿäÿ±ÿØŸàÿß',\n",
       " 'ŸäÿÆÿØŸÖŸàŸÜŸÉ',\n",
       " 'ŸäÿßÿÆŸä',\n",
       " 'ÿ™ÿ≠ÿ≥',\n",
       " 'stc',\n",
       " 'ŸÖÿßŸäÿØÿ±ŸàŸÜ',\n",
       " 'ÿ±ÿ°ŸäŸá',\n",
       " 'Ÿ¢Ÿ†Ÿ£Ÿ†',\n",
       " 'ÿßŸÑÿ≠ŸäŸÜ',\n",
       " 'ŸàŸÜŸÅÿ≥',\n",
       " 'ÿßŸÑÿßÿÆÿ∑ÿßÿ°',\n",
       " 'ÿπÿ¥ÿ±ÿßÿ™',\n",
       " 'ÿßŸÑÿ≥ŸÜŸäŸÜ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_model['clean_tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6Gzyyoew7yI3"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from time import time \n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mAnNWDIiG0l2"
   },
   "outputs": [],
   "source": [
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:37:57: collecting all words and their counts\n",
      "INFO - 05:37:57: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 05:37:57: collected 1839 word types from a corpus of 1239 words (unigram + bigrams) and 125 sentences\n",
      "INFO - 05:37:57: using 1839 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 05:37:57: source_vocab length 1839\n",
      "INFO - 05:37:57: Phraser built with 55 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ÿßÿ®ÿ¥ÿ±ŸÉ', 'Ÿ£ÿÆÿ∑Ÿàÿ∑', 'ŸÜŸÇŸÑÿ™Ÿáÿß', 'Ÿàÿ™Ÿàÿ®Ÿá', 'ŸàŸÖÿßÿπÿßÿØ', 'ÿ±ÿ¨ÿπŸá']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine common expressions from 2 words that are repeated in the data\n",
    "sent = [row for row in file_model.clean_tweet]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1] # print the first tweet after the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [ÿßÿ®ÿ¥ÿ±ŸÉ, ÿ®ÿ™ÿØÿÆŸÑ, ÿßŸÑŸäŸàŸÖ, ÿßŸÑÿπÿßÿ¥ÿ±, ÿπÿ¥ÿßŸÜ, Ÿäÿ±ÿØŸàÿß, ŸäÿÆÿØŸÖŸàŸÜŸÉ, ŸäÿßÿÆŸä, ÿ™ÿ≠ÿ≥, stc, ŸÖÿßŸäÿØÿ±ŸàŸÜ, ÿ±ÿ°ŸäŸá, Ÿ¢Ÿ†Ÿ£Ÿ†, ÿßŸÑÿ≠ŸäŸÜ, ŸàŸÜŸÅÿ≥, ÿßŸÑÿßÿÆÿ∑ÿßÿ°, ÿπÿ¥ÿ±ÿßÿ™, ÿßŸÑÿ≥ŸÜŸäŸÜ]\n",
       "1                                                                                        [ÿßÿ®ÿ¥ÿ±ŸÉ, Ÿ£ÿÆÿ∑Ÿàÿ∑, ŸÜŸÇŸÑÿ™Ÿáÿß, Ÿàÿ™Ÿàÿ®Ÿá, ŸàŸÖÿßÿπÿßÿØ, ÿ±ÿ¨ÿπŸá]\n",
       "2                                                                [ÿßÿÆÿ∞ÿ™Ÿá, ÿ±ŸÇŸÖŸä, ÿßŸÑÿ´ÿßŸÜŸä, ŸÅÿ±ÿπ, ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ, ŸÖŸàŸÑ, ÿßŸÑŸÖŸàÿ∂Ÿàÿπ, ÿØŸÇÿßŸäŸÇ, ÿ¥ŸÉÿ±ÿß, stc]\n",
       "3                                                                                                                   [ŸäÿπŸàÿ∂ŸÜŸä, stc, ü•∫]\n",
       "4                                            [ÿ™ÿ±ÿß, ŸÖÿßŸäŸÜŸÅÿπ, ŸÖÿπŸáŸÖ, ÿßŸÜŸÉ, ÿ™ÿ±Ÿàÿ≠, ŸÑÿßŸÇÿ±ÿ®, ŸÅÿ±ÿπ, Ÿàÿ™ŸÅÿßŸáŸÖ, ŸÖÿπŸáŸÖ, ŸàŸÇŸÑŸáŸÖ, ÿ®ÿ≠ŸàŸÑ, ÿπŸÜŸÉŸÖ, ÿ¥ŸàŸÅ, Ÿäÿ±ÿ¨ÿπŸà]\n",
       "                                                                   ...                                                              \n",
       "128                                                                                         [ŸäÿßÿÆŸä, ŸÑŸäŸá, ŸÖÿßÿ™ÿ±ÿØŸàŸÜ, Ÿàÿ™ÿÆŸÑÿµŸàŸÜ, ŸÖÿ¥ŸÉŸÑÿ™Ÿä, ü§å]\n",
       "129                                                                      [ÿÆŸÑÿßÿµ, ŸÅÿπŸÑÿ™Ÿá, ÿ®ÿ∑ÿ±ŸäŸÇŸá, ÿ´ÿßŸÜŸäŸá, mystc, Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá, ÿ®ÿßŸÑÿ±ŸÖÿ≤, ÿßŸÑŸäÿØŸàŸä]\n",
       "130                                                                           [ÿßŸÑÿÆŸäÿ±, ÿßŸÑÿ∫Ÿäÿ™, ÿ±ŸÇŸÖŸä, ÿßŸÑŸÖŸÅŸàÿ™ÿ±, ŸàŸÜÿ≤ŸÑÿ™, ŸÖÿØŸäŸàÿßŸÜŸäŸá, ÿ≥ŸÑÿßŸÖÿßÿ™]\n",
       "131                                                                                                                 [ÿ≥Ÿàÿß, ÿ®Ÿàÿ≥ÿ™, ÿ®ŸÑÿ≥]\n",
       "132                                                                                            [ÿ™ŸÅÿπŸäŸÑ, ŸÖŸÉÿßŸÑŸÖÿßÿ™, ÿ≥Ÿàÿß, ŸÖÿ≠ÿØŸàÿØ, ÿ≠ŸÇ, ÿ¥Ÿáÿ±]\n",
       "Name: clean_tweet, Length: 125, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_model['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcŸàÿ¥</th>\n",
       "      <th>...</th>\n",
       "      <th>Ÿ°Ÿ®</th>\n",
       "      <th>Ÿ¢Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ†Ÿ£Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ§</th>\n",
       "      <th>Ÿ¢Ÿ©</th>\n",
       "      <th>Ÿ£ÿÆÿ∑Ÿàÿ∑</th>\n",
       "      <th>Ÿ£Ÿ†Ÿ°Ÿ¢</th>\n",
       "      <th>Ÿ§ÿ≥ÿßÿπÿßÿ™</th>\n",
       "      <th>Ÿ•Ÿ†</th>\n",
       "      <th>Ÿ®ÿ≥ŸÜŸàÿßÿ™</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10   13   5g   90  iphone   my  mystc  stc  stcpay  stcŸàÿ¥  ...   Ÿ°Ÿ®   Ÿ¢Ÿ†  \\\n",
       "0  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "\n",
       "   Ÿ¢Ÿ†Ÿ£Ÿ†   Ÿ¢Ÿ§   Ÿ¢Ÿ©  Ÿ£ÿÆÿ∑Ÿàÿ∑  Ÿ£Ÿ†Ÿ°Ÿ¢  Ÿ§ÿ≥ÿßÿπÿßÿ™   Ÿ•Ÿ†  Ÿ®ÿ≥ŸÜŸàÿßÿ™  \n",
       "0   1.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "1   0.0  0.0  0.0    1.0   0.0     0.0  0.0     0.0  \n",
       "2   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "3   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "4   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "\n",
       "[5 rows x 810 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =10000)\n",
    "\n",
    "cleaned_tweets = file_model['clean_tweet'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# unigram = word_vectorizer.fit(file_model['clean_tweet'])\n",
    "unigramdataGet= word_vectorizer.fit_transform(cleaned_tweets)\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emTSCfqhGYvh",
    "outputId": "ee46ebfb-9eb9-4bea-ec0b-e547ab318f35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:37:57: collecting all words and their counts\n",
      "INFO - 05:37:57: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 05:37:57: collected 833 word types from a corpus of 1141 raw words and 125 sentences\n",
      "INFO - 05:37:57: Loading a fresh vocabulary\n",
      "INFO - 05:37:57: effective_min_count=3 retains 61 unique words (7% of original 833, drops 772)\n",
      "INFO - 05:37:57: effective_min_count=3 leaves 244 word corpus (21% of original 1141, drops 897)\n",
      "INFO - 05:37:57: deleting the raw counts dictionary of 833 items\n",
      "INFO - 05:37:57: sample=0.001 downsamples 61 most-common words\n",
      "INFO - 05:37:57: downsampling leaves estimated 74 word corpus (30.4% of prior 244)\n",
      "INFO - 05:37:57: estimated required memory for 61 words and 100 dimensions: 79300 bytes\n",
      "INFO - 05:37:57: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=3,\n",
    "                     min_alpha=0.0007, \n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MirvAxX3V_Qz",
    "outputId": "112f7d81-6d98-4fb8-db42-f76369d575bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:37:57: training model with 7 workers on 61 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 1 : training on 1141 raw words (77 effective words) took 0.0s, 9111 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 2 : training on 1141 raw words (81 effective words) took 0.0s, 6227 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 3 : training on 1141 raw words (78 effective words) took 0.0s, 9501 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 4 : training on 1141 raw words (68 effective words) took 0.0s, 5910 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 5 : training on 1141 raw words (53 effective words) took 0.0s, 3367 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 6 : training on 1141 raw words (75 effective words) took 0.0s, 7039 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 7 : training on 1141 raw words (70 effective words) took 0.0s, 6896 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 8 : training on 1141 raw words (73 effective words) took 0.0s, 6070 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 9 : training on 1141 raw words (77 effective words) took 0.0s, 7468 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:57: EPOCH - 10 : training on 1141 raw words (69 effective words) took 0.0s, 6125 effective words/s\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:57: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 11 : training on 1141 raw words (80 effective words) took 0.0s, 8350 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 12 : training on 1141 raw words (76 effective words) took 0.0s, 7934 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 13 : training on 1141 raw words (75 effective words) took 0.0s, 9103 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 14 : training on 1141 raw words (75 effective words) took 0.0s, 9633 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 15 : training on 1141 raw words (73 effective words) took 0.0s, 6585 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 16 : training on 1141 raw words (67 effective words) took 0.0s, 5389 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 17 : training on 1141 raw words (74 effective words) took 0.0s, 4430 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 18 : training on 1141 raw words (80 effective words) took 0.0s, 7428 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 19 : training on 1141 raw words (74 effective words) took 0.0s, 9566 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 20 : training on 1141 raw words (90 effective words) took 0.0s, 5971 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 21 : training on 1141 raw words (83 effective words) took 0.0s, 7346 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 22 : training on 1141 raw words (90 effective words) took 0.0s, 6836 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 23 : training on 1141 raw words (69 effective words) took 0.0s, 4622 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 24 : training on 1141 raw words (78 effective words) took 0.0s, 6326 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 25 : training on 1141 raw words (64 effective words) took 0.0s, 4908 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 26 : training on 1141 raw words (69 effective words) took 0.0s, 5461 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 27 : training on 1141 raw words (66 effective words) took 0.0s, 4737 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 28 : training on 1141 raw words (68 effective words) took 0.0s, 4537 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 29 : training on 1141 raw words (80 effective words) took 0.0s, 4339 effective words/s\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 05:37:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 05:37:58: EPOCH - 30 : training on 1141 raw words (76 effective words) took 0.0s, 6343 effective words/s\n",
      "INFO - 05:37:58: training on a 34230 raw words (2228 effective words) took 0.9s, 2510 effective words/s\n",
      "WARNING - 05:37:58: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "INFO - 05:37:58: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRnoAWW3WJox",
    "outputId": "9c05cbed-c3a8-4e11-9bfd-1743ee577dea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:38:31: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 05:38:31: not storing attribute vectors_norm\n",
      "INFO - 05:38:31: not storing attribute cum_table\n",
      "INFO - 05:38:31: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# store the word2vec model\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6B-V2jBiWdbm",
    "outputId": "2f1bff77-611e-4560-d864-d0bbd5c87310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 05:38:33: loading Word2Vec object from word2vec.model\n",
      "INFO - 05:38:33: loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO - 05:38:33: setting ignored attribute vectors_norm to None\n",
      "INFO - 05:38:33: loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 05:38:33: loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "INFO - 05:38:33: setting ignored attribute cum_table to None\n",
      "INFO - 05:38:33: loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'text', 'label', 'tweet_type', 'tweet_topic', 'clean_tweet'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IhHPF6ocgorY"
   },
   "outputs": [],
   "source": [
    "file_export = file_model.copy()\n",
    "file_export['old_tweets'] = file_export.clean_tweet\n",
    "file_export.old_tweets = file_export.old_tweets.str.join(' ')\n",
    "file_export.clean_tweet = file_export.clean_tweet.apply(lambda x: ' '.join(bigram[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_tweets</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ</td>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá</td>\n",
       "      <td>ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc</td>\n",
       "      <td>ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫</td>\n",
       "      <td>ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà</td>\n",
       "      <td>ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å</td>\n",
       "      <td>ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä</td>\n",
       "      <td>ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™</td>\n",
       "      <td>ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>ÿ≥Ÿàÿß ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥</td>\n",
       "      <td>ÿ≥Ÿàÿß_ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±</td>\n",
       "      <td>ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     old_tweets  \\\n",
       "0    ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ   \n",
       "1                                                                          ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá   \n",
       "2                                                      ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc   \n",
       "3                                                                                                  ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫   \n",
       "4                                      ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà   \n",
       "..                                                                                                          ...   \n",
       "128                                                                           ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å   \n",
       "129                                                          ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä   \n",
       "130                                                              ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™   \n",
       "131                                                                                                ÿ≥Ÿàÿß ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥   \n",
       "132                                                                              ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±   \n",
       "\n",
       "                                                                                                    clean_tweet  \n",
       "0    ÿßÿ®ÿ¥ÿ±ŸÉ ÿ®ÿ™ÿØÿÆŸÑ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿπÿßÿ¥ÿ± ÿπÿ¥ÿßŸÜ Ÿäÿ±ÿØŸàÿß ŸäÿÆÿØŸÖŸàŸÜŸÉ ŸäÿßÿÆŸä ÿ™ÿ≠ÿ≥ stc ŸÖÿßŸäÿØÿ±ŸàŸÜ ÿ±ÿ°ŸäŸá Ÿ¢Ÿ†Ÿ£Ÿ† ÿßŸÑÿ≠ŸäŸÜ ŸàŸÜŸÅÿ≥ ÿßŸÑÿßÿÆÿ∑ÿßÿ° ÿπÿ¥ÿ±ÿßÿ™ ÿßŸÑÿ≥ŸÜŸäŸÜ  \n",
       "1                                                                          ÿßÿ®ÿ¥ÿ±ŸÉ Ÿ£ÿÆÿ∑Ÿàÿ∑ ŸÜŸÇŸÑÿ™Ÿáÿß Ÿàÿ™Ÿàÿ®Ÿá ŸàŸÖÿßÿπÿßÿØ ÿ±ÿ¨ÿπŸá  \n",
       "2                                                      ÿßÿÆÿ∞ÿ™Ÿá ÿ±ŸÇŸÖŸä ÿßŸÑÿ´ÿßŸÜŸä ŸÅÿ±ÿπ ÿßŸÑÿ∏Ÿáÿ±ÿßŸÜ ŸÖŸàŸÑ ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿØŸÇÿßŸäŸÇ ÿ¥ŸÉÿ±ÿß stc  \n",
       "3                                                                                                  ŸäÿπŸàÿ∂ŸÜŸä stc ü•∫  \n",
       "4                                      ÿ™ÿ±ÿß ŸÖÿßŸäŸÜŸÅÿπ ŸÖÿπŸáŸÖ ÿßŸÜŸÉ ÿ™ÿ±Ÿàÿ≠ ŸÑÿßŸÇÿ±ÿ® ŸÅÿ±ÿπ Ÿàÿ™ŸÅÿßŸáŸÖ ŸÖÿπŸáŸÖ ŸàŸÇŸÑŸáŸÖ ÿ®ÿ≠ŸàŸÑ ÿπŸÜŸÉŸÖ ÿ¥ŸàŸÅ Ÿäÿ±ÿ¨ÿπŸà  \n",
       "..                                                                                                          ...  \n",
       "128                                                                           ŸäÿßÿÆŸä ŸÑŸäŸá ŸÖÿßÿ™ÿ±ÿØŸàŸÜ Ÿàÿ™ÿÆŸÑÿµŸàŸÜ ŸÖÿ¥ŸÉŸÑÿ™Ÿä ü§å  \n",
       "129                                                          ÿÆŸÑÿßÿµ ŸÅÿπŸÑÿ™Ÿá ÿ®ÿ∑ÿ±ŸäŸÇŸá ÿ´ÿßŸÜŸäŸá mystc Ÿàÿ≠ÿ∑Ÿäÿ™Ÿá ÿ®ÿßŸÑÿ±ŸÖÿ≤ ÿßŸÑŸäÿØŸàŸä  \n",
       "130                                                              ÿßŸÑÿÆŸäÿ± ÿßŸÑÿ∫Ÿäÿ™ ÿ±ŸÇŸÖŸä ÿßŸÑŸÖŸÅŸàÿ™ÿ± ŸàŸÜÿ≤ŸÑÿ™ ŸÖÿØŸäŸàÿßŸÜŸäŸá ÿ≥ŸÑÿßŸÖÿßÿ™  \n",
       "131                                                                                                ÿ≥Ÿàÿß_ÿ®Ÿàÿ≥ÿ™ ÿ®ŸÑÿ≥  \n",
       "132                                                                              ÿ™ŸÅÿπŸäŸÑ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿ≥Ÿàÿß ŸÖÿ≠ÿØŸàÿØ ÿ≠ŸÇ ÿ¥Ÿáÿ±  \n",
       "\n",
       "[125 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_export[['old_tweets','clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pKSd_Sn6-g7p"
   },
   "outputs": [],
   "source": [
    "file_export.to_csv('cleaned_dataset for training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bfYrtmM-sX71"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "4g3pLeGFsCpn",
    "outputId": "fb4482ee-ac15-45d5-9121-b09763be40f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcŸàÿ¥</th>\n",
       "      <th>...</th>\n",
       "      <th>Ÿ°ŸßŸ†</th>\n",
       "      <th>Ÿ°Ÿ®</th>\n",
       "      <th>Ÿ¢Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ†Ÿ£Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ§</th>\n",
       "      <th>Ÿ¢Ÿ©</th>\n",
       "      <th>Ÿ£ÿÆÿ∑Ÿàÿ∑</th>\n",
       "      <th>Ÿ£Ÿ†Ÿ°Ÿ¢</th>\n",
       "      <th>Ÿ§ÿ≥ÿßÿπÿßÿ™</th>\n",
       "      <th>Ÿ®ÿ≥ŸÜŸàÿßÿ™</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10   13   5g   90  iphone   my  mystc  stc  stcpay  stcŸàÿ¥  ...  Ÿ°ŸßŸ†   Ÿ°Ÿ®  \\\n",
       "0  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0     0.0  0.0    0.0  1.0     0.0    0.0  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "\n",
       "    Ÿ¢Ÿ†  Ÿ¢Ÿ†Ÿ£Ÿ†   Ÿ¢Ÿ§   Ÿ¢Ÿ©  Ÿ£ÿÆÿ∑Ÿàÿ∑  Ÿ£Ÿ†Ÿ°Ÿ¢  Ÿ§ÿ≥ÿßÿπÿßÿ™  Ÿ®ÿ≥ŸÜŸàÿßÿ™  \n",
       "0  0.0   1.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "1  0.0   0.0  0.0  0.0    1.0   0.0     0.0     0.0  \n",
       "2  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "3  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "4  0.0   0.0  0.0  0.0    0.0   0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 824 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =10000)\n",
    "\n",
    "unigramdataGet = word_vectorizer.fit(file_export['clean_tweet']).transform(file_export['clean_tweet'])\n",
    "# unigramdataGet= word_vectorizer.transform(unigram)\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_file = 'vectorizer.pickle'\n",
    "pickle.dump(word_vectorizer, open(vec_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectorizer.pickle'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jnctNBe8scne"
   },
   "outputs": [],
   "source": [
    "pro= preprocessing.LabelEncoder()\n",
    "encpro=pro.fit_transform(file_export['label'])\n",
    "file_export['label'] = encpro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Z3p4RTPSsc0d"
   },
   "outputs": [],
   "source": [
    "y=file_export['label']\n",
    "X=unigramdata_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "iYfurQbTsC8g"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "DsnssSJbbLur",
    "outputId": "e9ae2e90-aa8c-43fc-a384-dc406edc09b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>13</th>\n",
       "      <th>5g</th>\n",
       "      <th>90</th>\n",
       "      <th>iphone</th>\n",
       "      <th>my</th>\n",
       "      <th>mystc</th>\n",
       "      <th>stc</th>\n",
       "      <th>stcpay</th>\n",
       "      <th>stcŸàÿ¥</th>\n",
       "      <th>...</th>\n",
       "      <th>Ÿ°Ÿ®</th>\n",
       "      <th>Ÿ¢Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ†Ÿ£Ÿ†</th>\n",
       "      <th>Ÿ¢Ÿ§</th>\n",
       "      <th>Ÿ¢Ÿ©</th>\n",
       "      <th>Ÿ£ÿÆÿ∑Ÿàÿ∑</th>\n",
       "      <th>Ÿ£Ÿ†Ÿ°Ÿ¢</th>\n",
       "      <th>Ÿ§ÿ≥ÿßÿπÿßÿ™</th>\n",
       "      <th>Ÿ•Ÿ†</th>\n",
       "      <th>Ÿ®ÿ≥ŸÜŸàÿßÿ™</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10   13   5g   90  iphone   my  mystc  stc  stcpay  stcŸàÿ¥  ...   Ÿ°Ÿ®   Ÿ¢Ÿ†  \\\n",
       "5   0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "78  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "48  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "98  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "77  0.0  0.0  0.0  0.0     0.0  0.0    0.0  0.0     0.0    0.0  ...  0.0  0.0   \n",
       "\n",
       "    Ÿ¢Ÿ†Ÿ£Ÿ†   Ÿ¢Ÿ§   Ÿ¢Ÿ©  Ÿ£ÿÆÿ∑Ÿàÿ∑  Ÿ£Ÿ†Ÿ°Ÿ¢  Ÿ§ÿ≥ÿßÿπÿßÿ™   Ÿ•Ÿ†  Ÿ®ÿ≥ŸÜŸàÿßÿ™  \n",
       "5    0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "78   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "48   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "98   0.0  0.0  0.0    0.0   0.0     0.0  1.0     0.0  \n",
       "77   0.0  0.0  0.0    0.0   0.0     0.0  0.0     0.0  \n",
       "\n",
       "[5 rows x 810 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nY4TOoL_sC_Z",
    "outputId": "20fcb077-b68c-4268-cd93-baa27938ab9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=GaussianNB()\n",
    "nb= nb.fit(X_train , y_train)\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwU29NoltQ_B",
    "outputId": "71edd4ad-d880-41e1-abe1-332d9b9cf924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.640\n",
      "Accuracy= 0.960\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.predict(X_test)\n",
    "nb_1=nb.score(X_test, y_test)\n",
    "print('Accuracy= {:.3f}'.format(nb.score(X_test, y_test)))\n",
    "print('Accuracy= {:.3f}'.format(nb.score(X_train , y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75        18\n",
      "           1       0.57      0.67      0.62         6\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.48      0.44      0.46        25\n",
      "weighted avg       0.75      0.64      0.69        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_file = 'classification.model'\n",
    "pickle.dump(nb, open(mod_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = pd.read_excel('test_set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'text', 'label', 'tweet_type', 'tweet_topic'], dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['text']=replies['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['clean_tweet'] = replies['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies.clean_tweet  = replies.clean_tweet.apply(lambda x: ' '.join(bigram[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies['clean_tweet'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('classification.model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=10000, strip_accents='unicode', sublinear_tf=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-d2027f6adc2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ŸÉŸÑŸÖŸÜŸä ÿßŸÜÿ≥ÿ™ÿß'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2099\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2101\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2102\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \"\"\"\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1373\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1374\u001b[0m                 \u001b[1;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "loaded_vectorizer.transform('ŸÉŸÑŸÖŸÜŸä ÿßŸÜÿ≥ÿ™ÿß')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(replies['clean_tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-1fcc36c21198>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "replies['label'] = loaded_model.predict(loaded_vectorizer.transform(replies['clean_tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-0175a11ee006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "loaded_vectorizer.transform(replies['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " loaded_model.predict(loaded_vectorizer(replies['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-837eb03c8063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreplies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassify_utterance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-cd86921007c6>\u001b[0m in \u001b[0;36mclassify_utterance\u001b[1;34m(utt)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classification.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1866\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \"\"\"\n\u001b[1;32m-> 1868\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "for i in replies:\n",
    "    replies['label']=replies.clean_tweet.apply(classify_utterance([i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer.fit_transform(file_export['clean_tweet'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-4bca21a165b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreplies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassify_utterance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-cd86921007c6>\u001b[0m in \u001b[0;36mclassify_utterance\u001b[1;34m(utt)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classification.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1866\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \"\"\"\n\u001b[1;32m-> 1868\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "# for i in replies:\n",
    "#     replies['label']=replies.clean_tweet.apply(classify_utterance([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-81692e922dee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     max_features =10000)\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0munigramdataGet\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mword_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0munigramdataGet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigramdataGet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1866\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \"\"\"\n\u001b[1;32m-> 1868\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features =10000)\n",
    "\n",
    "unigramdataGet= word_vectorizer.transform(replies['clean_tweet'].to_list())\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1\n",
    "\n",
    "unigramdata_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies['label']=replies['clean_tweet'].apply(nb.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNMQzz-QtROu",
    "outputId": "d8a338aa-9c72-4050-8169-a43dfdf32b49"
   },
   "outputs": [],
   "source": [
    "RC= RidgeClassifier()\n",
    "RC= RC.fit(X_train , y_train)\n",
    "RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bbv78dPntXqJ",
    "outputId": "ef1468c4-909a-4e24-848c-6c5da4e43f4d"
   },
   "outputs": [],
   "source": [
    "y_pred = RC.predict(X_test)\n",
    "rc_1=RC.score(X_test, y_test)\n",
    "print('Accuracy= {:.3f}'.format(RC.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOUMSBSzsDCo",
    "outputId": "3e72fa9e-351d-4422-9a44-5c97d78eb880"
   },
   "outputs": [],
   "source": [
    "PC= PassiveAggressiveClassifier()\n",
    "PC= PC.fit(X_train , y_train)\n",
    "PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIRVhqAWtc9F",
    "outputId": "1f325aa1-e958-4835-9c25-59d5e0a25bc2"
   },
   "outputs": [],
   "source": [
    "y_pred = PC.predict(X_test)\n",
    "pc_1=PC.score(X_test, y_test)\n",
    "pc_2=PC.score(X_train , y_train)\n",
    "print('Accuracy= {:.3f}'.format(PC.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w04BQCz0tdKW",
    "outputId": "06613c41-e821-436d-c3ad-8cbbebe15361"
   },
   "outputs": [],
   "source": [
    "\n",
    "LR= LogisticRegression(penalty = 'l2', C = 1)\n",
    "LR= LR.fit(X_train , y_train)\n",
    "LR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7pz1ceZtdOh",
    "outputId": "0484dd75-276b-4693-c054-a452c576db50"
   },
   "outputs": [],
   "source": [
    "y_pred = LR.predict(X_test)\n",
    "lr_1=LR.score(X_test, y_test)\n",
    "print('Accuracy= {:.3f}'.format(LR.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwWpY7TRt4k4"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAiZhWSNtlZ3",
    "outputId": "1138f829-884f-4af1-b89e-fa0562c0e0b3"
   },
   "outputs": [],
   "source": [
    "x = PrettyTable()\n",
    "print('\\n')\n",
    "print(\"Comparison of all algorithms on F1 score\")\n",
    "x.field_names = [\"Model\", \"Accuracy\"]\n",
    "\n",
    "\n",
    "x.add_row([\"Naive Bayes Algorithm\", round(nb_1,2)])\n",
    "x.add_row([\"Ridge Classifier Algorithm\",  round(rc_1,2)])\n",
    "x.add_row([\"Passive Aggressive Classifier Algorithm\", round(pc_1,2)])\n",
    "x.add_row([\"Logistic Regression Algorithm\", round(lr_1,2)])\n",
    "\n",
    "print(x)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Yc-2tUmhT8u"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "_dDnkYIFhVQ6",
    "outputId": "abb3c541-a738-4001-8588-ea9fa04f85aa"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qf9OVWPPrWD"
   },
   "outputs": [],
   "source": [
    "stc=[\"ÿßŸÑÿÆÿØŸÖÿ© ÿ¨ŸäÿØÿ©\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRJGXB86PxCf"
   },
   "outputs": [],
   "source": [
    "x = word_vectorizer.transform(stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOLtI6_jSiPG"
   },
   "outputs": [],
   "source": [
    "x = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSVKLhY7PpmW",
    "outputId": "b499f4ab-f0b3-4dc8-88a8-cc92345c838a"
   },
   "outputs": [],
   "source": [
    "pred=nb.predict(x)\n",
    "pred=pro.inverse_transform(pred)\n",
    "prediction=pd.DataFrame(pred, columns=['Prediction']) \n",
    "print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdozQrRXTDVL"
   },
   "outputs": [],
   "source": [
    "tweets = file_export.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRLEevnOblWi",
    "outputId": "b2007b98-4022-4415-91e2-80db9e258573"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmBQrUKHbfrq",
    "outputId": "6dc78c51-f03e-4742-936c-2d1c474b90ab"
   },
   "outputs": [],
   "source": [
    "tweets.clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKQOKpsPUdHY",
    "outputId": "82736079-1aae-4aab-a228-8482270060b4"
   },
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haTLjZO1c2hx"
   },
   "outputs": [],
   "source": [
    "unigramdataGet= word_vectorizer.fit_transform(file_export['clean_tweet'].astype('str'))\n",
    "unigramdataGet = unigramdataGet.toarray()\n",
    "\n",
    "vocab = word_vectorizer.get_feature_names()\n",
    "unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
    "unigramdata_features[unigramdata_features>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcdxufrxdOWR",
    "outputId": "20a87305-0b10-4b36-eee4-4ca887ecb8d8"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6wmsgplaskR",
    "outputId": "7911659b-b554-4779-f287-cc0ae73c024c"
   },
   "outputs": [],
   "source": [
    "nb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "gRWtO9B3UTzi",
    "outputId": "d12eb0c1-3d4a-4b06-b11a-719721cdc563"
   },
   "outputs": [],
   "source": [
    "tweets[\"predict\"] = nb.predict(tweets[\"clean_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKzscSNqVJzd"
   },
   "outputs": [],
   "source": [
    "prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwr5WR8OTDgh"
   },
   "outputs": [],
   "source": [
    "for i in tweets:\n",
    "  X = word_vectorizer.transform(i)\n",
    "  pred=pro.inverse_transform(pred)\n",
    "  prediction=pd.DataFrame(pred, columns=['Prediction']) \n",
    "  print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AVXxFUrRKrr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tNu4GkZzRK-K",
    "outputId": "298fa9a4-b0e9-4949-dafd-00a00429c184"
   },
   "outputs": [],
   "source": [
    "file_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "cZRREeipAoRc",
    "outputId": "b71a8f05-a3e7-4e23-a087-d94a37eb30e1"
   },
   "outputs": [],
   "source": [
    "file_export = reset.index()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Tweets Clustering using Word2Vec and K-means",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
