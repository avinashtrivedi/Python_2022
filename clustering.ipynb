{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAJqz8yYE9DL"
   },
   "source": [
    "# Use of dimensionality reduction for clustering and automatic keywords extraction on document collections \n",
    "**Author**:  <br>\n",
    "**Id**:  <br>\n",
    "**Description**: Use of dimensionality reduction for clustering and automatic keywords extraction on document collections <br>\n",
    "**Goal**: the general goal is, given one document, to efficiently identify the documents that are closest to the document under consideration, in terms of cosine similarity computed on the tf-idf representations of the documents. <br>\n",
    "**Dataset**: as a test case, we will use the relatively large\n",
    "Amazon Books Reviews dataset, available at at https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgMCCmahCmc-"
   },
   "source": [
    "##Set-up instructions\n",
    "If you do not have the Kaggle JSON API file in the local drive, in order to use the Kaggle’s public API, you must first authenticate using an API token.<br>\n",
    "Follow these steps:\n",
    "\n",
    "1.   From the site header in Kaggle, click on your user profile picture;\n",
    "2.   Then on “My Account” from the dropdown menu;\n",
    "3.   This will take you to your account settings at https://www.kaggle.com/account. \n",
    "4.   Scroll down to the section of the page labelled API: to create a new token, click on the “Create New API Token” button.\n",
    "5.   This will download a fresh authentication token onto your machine as a file named \"kaggle.json\".\n",
    "6.   After this simply run the next cell and insert the \"kaggle.json\" file when requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "NbIAgszDcO2E",
    "outputId": "3fcdbcf4-108e-4140-bce0-de49bdd48b9e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import os.path\n",
    "os.path.isfile(\"kaggle.json\") \n",
    "\n",
    "if (os.path.isfile(\"kaggle.json\")  == False):\n",
    "    uploaded = files.upload()\n",
    "\n",
    "print(\"[!] Kaggle configuration file uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bk0OGyaGCsnN"
   },
   "source": [
    "Download the dataset from Kaggle, unzip it and remove the zipped file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iA8YJEZbcPWl",
    "outputId": "9f9d2478-0626-4587-ad49-0039008e4253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█████▋                          | 10 kB 30.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 20 kB 39.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 30 kB 46.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 40 kB 34.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 51 kB 37.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 58 kB 7.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73052 sha256=816b18aa2f0b9e616cfd6f2ca27a0e767c195d8d645f994936a5046b3367ef47\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n",
      "Downloading amazon-books-reviews.zip to /content\n",
      " 98% 1.04G/1.06G [00:06<00:00, 213MB/s]\n",
      "100% 1.06G/1.06G [00:06<00:00, 181MB/s]\n",
      "Archive:  /content/amazon-books-reviews.zip\n",
      "  inflating: Books_rating.csv        \n",
      "  inflating: books_data.csv          \n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n",
    "\n",
    "!unzip /content/amazon-books-reviews.zip\n",
    "!rm /content/amazon-books-reviews.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CY1MLTqC7FF"
   },
   "source": [
    "##Preprocessing\n",
    "Implement all necessary text preprocessing required to transform\n",
    "each review into a tf-idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "gq0wADrzcRYQ",
    "outputId": "9dffbf5e-a520-44e5-b575-e1ecbbad8b90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d61a1129-c276-4b89-a60b-4b1efea1f2b3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>User_id</th>\n",
       "      <th>profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1882931173</td>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>Jim of Oz \"jim-of-oz\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>940636800</td>\n",
       "      <td>Nice collection of Julie Strain images</td>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>Kevin Killian</td>\n",
       "      <td>10/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1095724800</td>\n",
       "      <td>Really Enjoyed It</td>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>John Granger</td>\n",
       "      <td>10/11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1078790400</td>\n",
       "      <td>Essential for every personal and Public Library</td>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>Roy E. Perry \"amateur philosopher\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1090713600</td>\n",
       "      <td>Phlip Nel gives silly Seuss a serious treatment</td>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A22X4XUPKF66MR</td>\n",
       "      <td>D. H. Richards \"ninthwavestore\"</td>\n",
       "      <td>3/3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1107993600</td>\n",
       "      <td>Good academic overview</td>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d61a1129-c276-4b89-a60b-4b1efea1f2b3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d61a1129-c276-4b89-a60b-4b1efea1f2b3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d61a1129-c276-4b89-a60b-4b1efea1f2b3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           Id                           Title  Price         User_id  \\\n",
       "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
       "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
       "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
       "3  0826414346        Dr. Seuss: American Icon    NaN  A2MVUWT453QH61   \n",
       "4  0826414346        Dr. Seuss: American Icon    NaN  A22X4XUPKF66MR   \n",
       "\n",
       "                          profileName review/helpfulness  review/score  \\\n",
       "0               Jim of Oz \"jim-of-oz\"                7/7           4.0   \n",
       "1                       Kevin Killian              10/10           5.0   \n",
       "2                        John Granger              10/11           5.0   \n",
       "3  Roy E. Perry \"amateur philosopher\"                7/7           4.0   \n",
       "4     D. H. Richards \"ninthwavestore\"                3/3           4.0   \n",
       "\n",
       "   review/time                                   review/summary  \\\n",
       "0    940636800           Nice collection of Julie Strain images   \n",
       "1   1095724800                                Really Enjoyed It   \n",
       "2   1078790400  Essential for every personal and Public Library   \n",
       "3   1090713600  Phlip Nel gives silly Seuss a serious treatment   \n",
       "4   1107993600                           Good academic overview   \n",
       "\n",
       "                                         review/text  \n",
       "0  This is only for Julie Strain fans. It's a col...  \n",
       "1  I don't care much for Dr. Seuss but after read...  \n",
       "2  If people become the books they read and if \"t...  \n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...  \n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/content/Books_rating.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAIG1CDjPxRe"
   },
   "source": [
    "Clean the dataframe dropping rows having missing values or infinite ones, since they could create problems in numeric methods. <br>\n",
    "Then print a statistical description of the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "uratqwAqPy_1",
    "outputId": "cde0e645-c5e6-4b0a-b109-115f4fabb8f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1b993f14-35c8-4f55-881c-eb8a92f2e703\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>414548.000000</td>\n",
       "      <td>414548.000000</td>\n",
       "      <td>4.145480e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21.814618</td>\n",
       "      <td>4.240382</td>\n",
       "      <td>1.174725e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.277351</td>\n",
       "      <td>1.187782</td>\n",
       "      <td>1.239848e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.482752e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.850000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.086566e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.950000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.170288e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.283990e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>995.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.362355e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b993f14-35c8-4f55-881c-eb8a92f2e703')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1b993f14-35c8-4f55-881c-eb8a92f2e703 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1b993f14-35c8-4f55-881c-eb8a92f2e703');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               Price   review/score   review/time\n",
       "count  414548.000000  414548.000000  4.145480e+05\n",
       "mean       21.814618       4.240382  1.174725e+09\n",
       "std        26.277351       1.187782  1.239848e+08\n",
       "min         1.000000       1.000000  8.482752e+08\n",
       "25%        10.850000       4.000000  1.086566e+09\n",
       "50%        14.950000       5.000000  1.170288e+09\n",
       "75%        24.000000       5.000000  1.283990e+09\n",
       "max       995.000000       5.000000  1.362355e+09"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = data.replace([np.inf, -np.inf], np.nan) \n",
    "df = df.dropna()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3QO-eb8P1JY"
   },
   "source": [
    "Then, in order to make practice with a more real preprocessing on data, let's consider only the reviews having a score greater or equal than 0 and put them into a list, printing the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eemq0yiRQD8v",
    "outputId": "4f46d416-f914-4c66-b21e-2ac087864850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414548\n"
     ]
    }
   ],
   "source": [
    "pos = df[df['review/score']>= 0]['review/text'].tolist()\n",
    "print(len(pos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NELjQrgDQF2O"
   },
   "source": [
    "Convert all the text to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iosfe3jxQH9-"
   },
   "outputs": [],
   "source": [
    "pos = [doc.lower() for doc in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fh_VbSSzQIYR"
   },
   "source": [
    "Removing contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0pzFUyhXQI7Z"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "contractions_dict = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'alls\": \"you alls\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
    "\n",
    "# Removing contractions\n",
    "pos = [multiple_replace(contractions_dict, doc) for doc in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsqNDQ3JQU5t"
   },
   "source": [
    "\n",
    "Remove punctuation and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VLEXFDMEQYC6"
   },
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "pos = [doc.translate(table) for doc in pos]\n",
    "\n",
    "# Removing numbers\n",
    "pos = [re.sub(r'\\d+', \" \", doc) for doc in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvaEAzGOQab5"
   },
   "source": [
    "Apply tokenization, lemmatization and removal of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQypEXaFQa_B",
    "outputId": "a4b0083b-3d0e-4822-b61f-5cf20f6c1eea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk # Natural Language Toolkit\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
    "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
    "\n",
    "# Tokenization and lemmatization\n",
    "def get_lemmatized(doc):\n",
    "  word_list = word_tokenize(doc)\n",
    "  lemmatized_doc = \"\"\n",
    "  for word in word_list:\n",
    "    lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n",
    "  return lemmatized_doc\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos = [get_lemmatized(doc) for doc in pos]\n",
    "\n",
    "# Remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english') # Returns a list\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "def rem_stop(doc):\n",
    "  word_list = word_tokenize(doc)\n",
    "  cleaned_doc = \"\"\n",
    "  for word in word_list:\n",
    "    if word not in stopwords:\n",
    "      cleaned_doc += \" \" + word\n",
    "  return cleaned_doc\n",
    "\n",
    "pos = [rem_stop(doc) for doc in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uSB7mrOQj7s"
   },
   "source": [
    "Convert the list of reviews to a final dataframe and print the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iNnotbLQnWb",
    "outputId": "4a6e0094-993a-4251-d2a4-8676caa9f1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414548\n"
     ]
    }
   ],
   "source": [
    "df= pd.Series(pos).astype(str) # Final dataframe\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZG4b6hxQpBx"
   },
   "source": [
    "##Vectorization\n",
    "Computing the tf-idf vectors for each document passing the preprocessing phase.\n",
    "More on tf-idf vectors here: [tf-idf vectorization](https://towardsdatascience.com/text-vectorization-term-frequency-inverse-document-frequency-tfidf-5a3f9604da6d).<br>\n",
    "A matrix containing them is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "5iVuBydUQt8m",
    "outputId": "d0f36a3d-5953-4ae9-9a6b-bdae4a0df707"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d59f4949998b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAmazonReviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextPreprocessor' from 'nltk.text' (/usr/local/lib/python3.8/dist-packages/nltk/text.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.text import TextPreprocessor\n",
    "\n",
    "class AmazonReviews:\n",
    "  def __init__(self, reviews_dataframe):\n",
    "    self.reviews = reviews_dataframe # as a list of reviews\n",
    "    self.vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        min_df=10,\n",
    "        stop_words='english')\n",
    "    self.preprocessor = TextPreprocessor()\n",
    "    \n",
    "  def get_tfidf_vectors(self):\n",
    "    # Preprocessing (more accurate)\n",
    "    processed_reviews = self.preprocessor.preprocess(self.reviews)\n",
    "    \n",
    "    # Transform reviews in vectors tf-idf\n",
    "    return self.vectorizer.fit_transform(processed_reviews)\n",
    "\n",
    "\n",
    "reviews = AmazonReviews(df)\n",
    "matrix = reviews.get_tfidf_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDaFbRNOQwYT"
   },
   "source": [
    "Each tf-idf vector has one entry for each word in the vocabulary of the tokenized and lemmatized documents. <br>\n",
    "Let's print these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nenJItOQ09I"
   },
   "outputs": [],
   "source": [
    "feature_names = reviews.vectorizer.get_feature_names_out();\n",
    "print(feature_names)\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pd35ydLMsAb"
   },
   "source": [
    "ATTENTION: RUN IT TO USE COUNT VECTORIZER INSTEAD OF TF-IDF. As you can see, this class is very similar to the original class I provided, except that it uses the CountVectorizer class instead of the TfidfVectorizer class. To use this class, you would first load your Amazon review dataset into a list and then pass it to the class as a constructor argument, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gnbFqetMU0U"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk.text import TextPreprocessor\n",
    "\n",
    "# class AmazonReviews:\n",
    "#   def __init__(self, reviews):\n",
    "#     self.reviews = reviews\n",
    "#     self.vectorizer = CountVectorizer()\n",
    "#     self.preprocessor = TextPreprocessor()\n",
    "    \n",
    "#   def get_tfidf_vectors(self):\n",
    "#     # Esegui il preprocessing delle recensioni\n",
    "#     processed_reviews = self.preprocessor.preprocess(self.reviews)\n",
    "    \n",
    "#     # Trasforma le recensioni in vettori tf-idf\n",
    "#     return self.vectorizer.fit_transform(processed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw-tynr0MiPX"
   },
   "source": [
    "## Clustering help classes\n",
    "We will need to do Clustering and we will use a single class for doing these 2 things needed before doing clustering: \n",
    "\n",
    "To perform clustering, you should decide the value of k, the number of clusters. In this case, you do not know how many topics you are supposed to find, so in part (but only in part) you should proceed by trial-and-error. Principled ways to find a reasonable tentative value for k are the following: \n",
    "\n",
    "i) if you are using k-means, you can plot inertia of the clustering you compute for increasing values of k and apply the elbow method seen in class. Inertia of a clustering is maintained in the attribute inertia of the sklearn.cluster.KMeans object; <br>\n",
    "\n",
    "ii) if you are using SVD, you can use the explained variance or explained variance ratio attribute of the sklearn.decomposition.TruncatedSVD object to access the explained variance values of the k components (singular vectors) you kept. In this second case, you can keep the k largest components that account for 80-90% of the total variance. In normal datasets, this corresponds to a relatively small number of components. If you think this is still too large a number of components, you can set k to some predefined value (e.g., 100 to begin with) and then plot explained variance against the number n of components for n ranging from 1 to the predefined value of k you chose. If you already notice an elbow in this interval you are done. \n",
    "\n",
    "These are simple heuristics and it should be clear that it is up to you to proceed in a sensible and feasible way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wrbz5KYvNBKW"
   },
   "source": [
    "Here is an example of how you could create a class method that plots the inertia of a sklearn.cluster.KMeans object for incremental values of k. This method can help you apply the \"Elbow method\" to choose the right number of clusters, k, for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aafv8STpMk35"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class K-MEANS-K_FIND:\n",
    "\n",
    "    def __init__(self, max_k):\n",
    "        self.max_k = 1000 # maximum value of k to use in the elbow method\n",
    "\n",
    "    def fit_predict(self, data, k):\n",
    "        # create a KMeans instance with the specified number of clusters\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        # fit the model to the data\n",
    "        kmeans.fit(data)\n",
    "        # predict the cluster labels for the data\n",
    "        return kmeans.predict(data)\n",
    "\n",
    "    def plot_inertia(self):\n",
    "        ks = range(1, self.max_k + 1)\n",
    "        inertias = []\n",
    "        for k in ks:\n",
    "            # fit the model with k clusters and append the inertia to the list\n",
    "            inertias.append(self.fit_predict(data,k).inertia_)\n",
    "\n",
    "        # plot the inertia values against the number of clusters\n",
    "        plt.plot(ks, inertias, '-o')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.show()\n",
    "\n",
    "# This method will fit the clustering algorithm for different values of k and plot the resulting inertia values. \n",
    "# The elbow method can then be used to choose the optimal value of k by looking for the \"elbow\" in the plot,\n",
    "# where the inertia begins to decrease more slowly.\n",
    "\n",
    "# The KMeans class from scikit-learn is used to fit the model to the data and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZql8Op2QxpU"
   },
   "source": [
    "SVD : create another class: method  it uses the explained variance or explained variance ratio of sklearn.decomposition.truncatedSVD for all values for all kept singular vectors (so the components). In this case keep the k-largest components that account for 80-90% of total variance. If this number is too big then set k to a predefined value (e.g 100) and plot the explained variance for k from 1 to this value and apply the Elbow method for finding the best “k” parameter ( the return value of the class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1fs7n43S1uI"
   },
   "source": [
    "The TruncatedSVD class in the sklearn.decomposition module can be used to perform Singular Value Decomposition (SVD) on a matrix. This class can be used to reduce the dimensionality of a matrix by keeping only the largest singular values and corresponding singular vectors. The explained_variance_ or explained_variance_ratio_ attribute of the TruncatedSVD object can be used to determine the amount of variance explained by each of the singular vectors.\n",
    "\n",
    "To implement the class you described, you could do the following:\n",
    "\n",
    "Import the TruncatedSVD class from sklearn.decomposition.\n",
    "Define a class, SVD, that takes a matrix as an input in its constructor.\n",
    "Define a method in the SVD class, explained_variance, that uses the TruncatedSVD class to perform SVD on the input matrix and returns the explained variance or explained variance ratio for all of the singular vectors.\n",
    "Define another method in the SVD class, elbow_method, that plots the explained variance for each singular vector and applies the elbow method to find the optimal number of singular vectors to keep. This method should return the optimal number of singular vectors to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zReKtZxUQ1Y-"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SVD:\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = matrix\n",
    "    \n",
    "    def explained_variance(self):\n",
    "        svd = TruncatedSVD()\n",
    "        svd.fit(self.matrix)\n",
    "        return svd.explained_variance_ratio_\n",
    "    \n",
    "    def elbow_method(self):\n",
    "        # Perform SVD on the matrix and compute the explained variance ratio for each singular vector\n",
    "        explained_variance = self.explained_variance()\n",
    "        \n",
    "        # Plot the explained variance ratio for each singular vector\n",
    "        plt.plot(explained_variance)\n",
    "        plt.xlabel('Singular Vector Index')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.show()\n",
    "        \n",
    "        # Use the elbow method to find the optimal number of singular vectors to keep\n",
    "        # The optimal number of singular vectors is the point at which the explained variance\n",
    "        # ratio starts to decrease more slowly\n",
    "        num_singular_vectors = 0\n",
    "        for index, variance in enumerate(explained_variance):\n",
    "            if variance < explained_variance[index - 1] - 0.01:\n",
    "                num_singular_vectors = index\n",
    "                break\n",
    "        \n",
    "        # Return the optimal number of singular vectors to keep\n",
    "        return num_singular_vectors\n",
    "\n",
    "# Create an instance of the SVD class and perform SVD on a matrix\n",
    "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "svd = SVD(matrix)\n",
    "\n",
    "# Apply the elbow method to find the optimal number of singular vectors to keep\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ktj8NVMUbsu"
   },
   "source": [
    "## Test Unit Creation\n",
    "Create a class, named TestSample, that gives a random sample of the dataset Amazon books reviews setting the percentage as parameter is better.\n",
    "Here is one possible implementation of the TestSample class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7jEDn_8Ucnn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class TestSample:\n",
    "    def __init__(self, percentage):\n",
    "        self.percentage = percentage\n",
    "    \n",
    "    def get_sample(self, data):\n",
    "        sample_size = int(len(data) * self.percentage)\n",
    "        return random.sample(data, sample_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt_AFNoUUqUf"
   },
   "source": [
    "The TestSample class takes a percentage as a parameter in its constructor. The get_sample() method takes a dataset as input and returns a random sample of the data with the size equal to the given percentage of the original dataset.\n",
    "\n",
    "Here is an example of how you can use the TestSample class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnPpPx6OUrd_"
   },
   "outputs": [],
   "source": [
    "# Initialize the TestSample class with a percentage of 0.1\n",
    "test_sample = TestSample(0.1)\n",
    "\n",
    "# Get a random sample of the dataset\n",
    "sample = test_sample.get_sample(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      4\u001b[0m               [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m----> 5\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[0;32m      8\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mpredict([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1374\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1341\u001b[0m \n\u001b[0;32m   1342\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1366\u001b[0m     X,\n\u001b[0;32m   1367\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1371\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1372\u001b[0m )\n\u001b[1;32m-> 1374\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m   1376\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1305\u001b[0m, in \u001b[0;36mKMeans._check_params\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;66;03m# algorithm\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlloyd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melkan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:818\u001b[0m, in \u001b[0;36m_BaseKMeans._check_params\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;66;03m# n_init\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_init should be > 0, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_init\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_init\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
    "kmeans.labels_\n",
    "\n",
    "kmeans.predict([[0, 0], [12, 3]])\n",
    "\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
